{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Archaic to Modern Italian with Context Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Datases to work with Transformers by Hugging-Face\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "# Imports for Transformers\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer  # Datasets\n",
    "import pandas as pd\n",
    "from datasets.features import Value, Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Models\n",
    "* google/flan-t5-small - google/mt5-small (text2text model) ::NO_WORK\n",
    "* google/gemma-3-1b-it (LLM) ðŸš€\n",
    "* sapienzanlp/Minerva-1B-base-v1.0 ðŸ‡®ðŸ‡¹ (LMM)\n",
    "* Helsinki-NLP/opus-mt-itc-itc (Machine Translation) ðŸ† - use OpusPrompt \n",
    "* FacebookAI/xlm-roberta-base (fill-mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Promposed Models\n",
    "* google/flan-t5-small - google/mt5-small (text2text model) ::NO_WORK\n",
    "* google/gemma-3-1b-it (LLM) ðŸš€\n",
    "* sapienzanlp/Minerva-1B-base-v1.0 ðŸ‡®ðŸ‡¹ (LMM)\n",
    "* openai-community/gpt2 (LLM) ::NO IT\n",
    "* Helsinki-NLP/opus-mt-itc-itc (Machine Translation) ðŸ† - use OpusPrompt \n",
    "* facebook/nllb-200-3.3B (Translation)\n",
    "* FacebookAI/xlm-roberta-base (fill-mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"dataset.csv\"\n",
    "FEATURES = Features(\n",
    "    {\n",
    "        \"Author\": Value(dtype=\"string\"),\n",
    "        \"Date\": Value(dtype=\"string\"),\n",
    "        \"Region\": Value(dtype=\"string\"),\n",
    "        \"Sentence\": Value(dtype=\"string\")\n",
    "    }\n",
    ")\n",
    "NET = \"t5-base/checkpoint-2982\"\n",
    "BS = 8\n",
    "DEVICE = ('cuda' if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Styling of Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = None\n",
    "def ft5_std_map(examples):\n",
    "    return tokenizer(\n",
    "        [f\"Traduci dal volgare allâ€™italiano moderno: {example}\" for example in examples[\"Sentence\"]],  \n",
    "        padding=True, \n",
    "        max_length=128,\n",
    "        )\n",
    "\n",
    "def gemma_1b_map(examples):\n",
    "    chat = tokenizer.apply_chat_template([\n",
    "    [\n",
    "        {\"role\": \"system\",   \"content\": \"Sei un traduttore esperto di Italiano Antico \"},\n",
    "        {\"role\": \"user\",     \"content\": \"Traduci 'La corte era in gran fermento.' in Italiano Moderno\"},\n",
    "        {\"role\": \"assistant\",\"content\": \"Italiano Antico: 'La corte era in gran fermento.' Italiano Moderno: 'La corte era molto agitata.'\"},\n",
    "        {\"role\": \"user\",      \"content\": f\"Traduci '{example}' in Italiano Moderno\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"\"}\n",
    "    ] for example in examples[\"Sentence\"]], \n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_dict=True,\n",
    "        padding=True,         # adds zeros to make all sequences the same length\n",
    "        truncation=True,      # cuts sequences that are too long\n",
    "        max_length=250,       # (optional) max length of the sequences\n",
    "        return_tensors=\"pt\")\n",
    "\n",
    "    return chat\n",
    "\n",
    "def mask_std_map(examples):\n",
    "    return tokenizer(\n",
    "        [f\"Old Italian: {example} Modern Italian [MASK]\" for example in examples['Sentence']],  \n",
    "        padding=True, \n",
    "        max_length=128)\n",
    "\n",
    "def minerva_map(examples):\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    return tokenizer(\n",
    "        [f\"Traduci dal volgare allâ€™italiano moderno: {example}\" for example in examples[\"Sentence\"]],  \n",
    "        padding=True, \n",
    "        max_length=128,\n",
    "        )\n",
    "\n",
    "def style_map(examples):\n",
    "    return tokenizer(\n",
    "        [f\"The following sentence represents an example from the Dolce Stil Novo (sweet new style) literary movement, developed in the 13th and 14th century in Italy: {example} Translate it to modern Italian: \" for example in examples[\"Sentence\"]],\n",
    "        padding=True, \n",
    "        max_length=128)\n",
    "\n",
    "def period_region_map(examples):\n",
    "    return tokenizer(\n",
    "        [f\"This sentence {example['Sentence']} was written in {example['Date']}, in the {example['Region']} region. Translate it to Modern Italian\" for example in examples],\n",
    "        padding=True,\n",
    "        max_length=128)\n",
    "\n",
    "def author_map(examples):\n",
    "    return tokenizer(\n",
    "        [f\"This sentence: {example['Sentence']} was written by {example['Author']}. Translate it to Modern Italian\" for example in examples],\n",
    "        padding=True,\n",
    "        max_length=128)\n",
    "\n",
    "def question_map(examples):\n",
    "    return tokenizer(\n",
    "        [f\"Can you express this sentence: {example} in a more colloquial style?\" for example in examples['Sentence']],\n",
    "        padding=True,\n",
    "        max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to select the mapping function based on the prompt type\n",
    "PROMPT = \"ft5_std\"\n",
    "\n",
    "match PROMPT:\n",
    "    case \"ft5_std\":\n",
    "        map_func = ft5_std_map\n",
    "    case \"gemma-1b\":\n",
    "        map_func = gemma_1b_map\n",
    "    case \"mask_std\":\n",
    "        map_func = mask_std_map\n",
    "    case \"style\":\n",
    "        map_func = style_map\n",
    "    case \"period_region\":\n",
    "        map_func = period_region_map\n",
    "    case \"author\":\n",
    "        map_func = author_map\n",
    "    case \"question\":\n",
    "        map_func = question_map\n",
    "    case _:\n",
    "        raise ValueError(\"Unknown prompt type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Appropriate Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Rete t5-base/checkpoint-2982 non testabile",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mException\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 110\u001b[39m\n\u001b[32m     97\u001b[39m     params = {\n\u001b[32m     98\u001b[39m \n\u001b[32m     99\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmax_new_tokens\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m120\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    106\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mpad_token_id\u001b[39m\u001b[33m\"\u001b[39m:tokenizer.eos_token_id  \u001b[38;5;66;03m# evita warning se manca un token di padding\u001b[39;00m\n\u001b[32m    107\u001b[39m     }\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mcase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01m_\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRete \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNET\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m non testabile\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mException\u001b[39m: Rete t5-base/checkpoint-2982 non testabile"
     ]
    }
   ],
   "source": [
    "# Switch to select the network and load the appropriate model and tokenizer\n",
    "match NET:\n",
    "    \n",
    "    case \"google/flan-t5-small\" | \"google-t5/t5-small\" | \"google/mt5-small\":\n",
    "        from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "        tokenizer = T5Tokenizer.from_pretrained(NET)\n",
    "        model = T5ForConditionalGeneration.from_pretrained(NET, device_map=DEVICE, torch_dtype=torch.float16)\n",
    "        tr = ft5_std_map\n",
    "\n",
    "        params = {\n",
    "            \n",
    "            \"max_new_tokens\": 120, # max number of new tokens to generate\n",
    "            \"do_sample\":True,      # enables sampling for more diverse outputs\n",
    "            \"top_k\":50,            # diversity increase by controlling the candidate words\n",
    "            \"top_p\":0.90,          # nucleus sampling for further control over variety\n",
    "            \"temperature\":1.0,     # reduces randomness and increases coherence\n",
    "            \"repetition_penalty\":1.0,  # penalizza ripetizioni\n",
    "            \"num_return_sequences\":10,  # number of generated responses\n",
    "            \"pad_token_id\":tokenizer.eos_token_id  # avoids warning if padding token is missing\n",
    "        }\n",
    "        \n",
    "    case \"google/mt5-base\":\n",
    "        from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "        tokenizer = AutoTokenizer.from_pretrained(NET)\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(NET, device_map=DEVICE, torch_dtype=torch.float16)\n",
    "        tr = ft5_std_map\n",
    "\n",
    "        params = {\n",
    "            \n",
    "            \"max_new_tokens\": 120,\n",
    "            \"do_sample\":True,\n",
    "            \"top_k\":10,          \n",
    "            \"top_p\":0.90,          \n",
    "            \"temperature\":1.0,  \n",
    "            \"repetition_penalty\":1.0,\n",
    "            \"num_return_sequences\":10, \n",
    "            \"pad_token_id\":tokenizer.eos_token_id \n",
    "        }\n",
    "\n",
    "    case \"google/gemma-3-1b-it\":\n",
    "        from transformers import BitsAndBytesConfig, Gemma3ForCausalLM, AutoTokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(NET)\n",
    "        quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "        model = Gemma3ForCausalLM.from_pretrained(NET, device_map=DEVICE, quantization_config=quantization_config)\n",
    "        tr = gemma_1b_map\n",
    "        params = {\n",
    "            \n",
    "            \"max_new_tokens\": 120,\n",
    "            \"do_sample\":True,\n",
    "            \"top_k\":10,     \n",
    "            \"top_p\":0.90,  \n",
    "            \"temperature\":1.0,     \n",
    "            \"repetition_penalty\":1.0,\n",
    "            \"num_return_sequences\":10,  \n",
    "            \"pad_token_id\":tokenizer.eos_token_id \n",
    "        }\n",
    "    \n",
    "    case \"FacebookAI/xlm-roberta-base\":\n",
    "        \n",
    "        from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "        tokenizer = AutoTokenizer.from_pretrained(NET)\n",
    "        model = AutoModelForMaskedLM.from_pretrained(NET)\n",
    "        tr = mask_std_map\n",
    "        params = {\n",
    "            \n",
    "            \"max_new_tokens\": 120,\n",
    "            \"do_sample\":True,\n",
    "            \"top_k\":10,            \n",
    "            \"top_p\":0.90,         \n",
    "            \"temperature\":1.0,    \n",
    "            \"repetition_penalty\":1.0, \n",
    "            \"num_return_sequences\":10,  \n",
    "            \"pad_token_id\":tokenizer.eos_token_id \n",
    "        }\n",
    "    case \"sapienzanlp/Minerva-1B-base-v1.0\":\n",
    "        \n",
    "        from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "        tokenizer = AutoTokenizer.from_pretrained(NET)\n",
    "        model = AutoModelForCausalLM.from_pretrained(NET, device_map=DEVICE)\n",
    "        tr = minerva_map\n",
    "        params = {\n",
    "            \n",
    "            \"max_new_tokens\": 120,\n",
    "            \"do_sample\":True,\n",
    "            \"top_k\":10,            # aumento della diversitÃ  controllando le parole candidate\n",
    "            \"top_p\":0.90,          # campionamento nucleus per ulteriori controlli sulla varietÃ \n",
    "            \"temperature\":1.0,     # riduce la casualitÃ  e aumenta la coerenza\n",
    "            \"repetition_penalty\":1.0,  # penalizza ripetizioni\n",
    "            \"num_return_sequences\":10,  # numero di risposte generate\n",
    "            \"pad_token_id\":tokenizer.eos_token_id  # evita warning se manca un token di padding\n",
    "        }\n",
    "    case \"openai-community/gpt2\":\n",
    "        from transformers import GPT2Tokenizer, AutoModelForCausalLM\n",
    "        tokenizer = GPT2Tokenizer.from_pretrained(NET)\n",
    "        model = AutoModelForCausalLM.from_pretrained(NET, device_map=DEVICE)\n",
    "        tr = minerva_map\n",
    "        params = {\n",
    "            \n",
    "            \"max_new_tokens\": 120,\n",
    "            \"do_sample\":True,\n",
    "            \"top_k\":10,            # aumento della diversitÃ  controllando le parole candidate\n",
    "            \"top_p\":0.90,          # campionamento nucleus per ulteriori controlli sulla varietÃ \n",
    "            \"temperature\":1.0,     # riduce la casualitÃ  e aumenta la coerenza\n",
    "            \"repetition_penalty\":1.0,  # penalizza ripetizioni\n",
    "            \"num_return_sequences\":10,  # numero di risposte generate\n",
    "            \"pad_token_id\":tokenizer.eos_token_id  # evita warning se manca un token di padding\n",
    "        }\n",
    "\n",
    "    case _:\n",
    "        raise Exception(f\"Rete {NET} non testabile\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf = Dataset.from_csv(DATASET, features=FEATURES).shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m tokenized = hf.map(\u001b[43mtr\u001b[49m, batched=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name 'tr' is not defined"
     ]
    }
   ],
   "source": [
    "tokenized = hf.map(tr, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Author', 'Date', 'Region', 'Sentence', 'input_ids', 'attention_mask']\n",
      "sample nÂ°1: {'Author': 'Guido da Pisa', 'Date': '1337', 'Region': 'tosc.', 'Sentence': \"Ed ecco di subito tutta questa turba degli uccelli si levÃ² a volo dietro all'aquila\", 'input_ids': [3083, 4817, 23, 3, 26, 138, 5063, 1478, 15, 66, 22, 9538, 20028, 941, 32, 10, 4857, 3, 15, 75, 509, 1227, 769, 23, 235, 13829, 17, 9, 13118, 9, 3, 2905, 115, 9, 20, 4707, 3, 17431, 7999, 108, 3, 10912, 2, 3, 9, 5063, 32, 1227, 15252, 66, 31, 9, 1169, 521, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "Decode ids: Traduci dal volgare allâ€™italiano moderno: Ed ecco di subito tutta questa turba degli uccelli si lev a volo dietro all'aquila\n",
      "sample nÂ°2: {'Author': 'Bart. da San Concordio', 'Date': '1313', 'Region': 'tosc.', 'Sentence': \"la seconda suole talora per la grande provedenzia fare timoroso, e la prima per l'ardire rendere altrui matto.\", 'input_ids': [3083, 4817, 23, 3, 26, 138, 5063, 1478, 15, 66, 22, 9538, 20028, 941, 32, 10, 50, 511, 9, 2629, 32, 109, 3, 17, 9, 322, 9, 399, 50, 3749, 9193, 35, 702, 9, 623, 15, 3, 2998, 32, 1859, 32, 6, 3, 15, 50, 3778, 399, 3, 40, 31, 986, 2060, 16061, 15, 491, 2666, 23, 6928, 235, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "Decode ids: Traduci dal volgare allâ€™italiano moderno: la seconda suole talora per la grande provedenzia fare timoroso, e la prima per l'ardire rendere altrui matto.\n",
      "sample nÂ°3: {'Author': 'Prima catilinaria volg. (red. A)', 'Date': '1294', 'Region': 'fior.', 'Sentence': \"E dunque, da che queste cose son cosÃ¬, Catellina, e tu non puoi buonamente qui dimorare, dubiti tu d'andartene in alcuna terra ed usare questa vita fuggendo per li diserti\", 'input_ids': [3083, 4817, 23, 3, 26, 138, 5063, 1478, 15, 66, 22, 9538, 20028, 941, 32, 10, 262, 146, 29, 835, 6, 836, 3, 1033, 13118, 15, 576, 7, 15, 520, 576, 7, 2, 6, 1336, 1625, 8280, 6, 3, 15, 3, 17, 76, 529, 4353, 32, 23, 8524, 106, 16666, 285, 15688, 127, 355, 6, 146, 2360, 23, 3, 17, 76, 3, 26, 31, 232, 15422, 15, 16, 491, 75, 202, 9, 3, 12829, 3, 15, 26, 178, 355, 13118, 9, 3, 12411, 3, 14165, 729, 26, 32, 399, 3, 40, 23, 1028, 49, 17, 23, 1, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "Decode ids: Traduci dal volgare allâ€™italiano moderno: E dunque, da che queste cose son cos, Catellina, e tu non puoi buonamente qui dimorare, dubiti tu d'andartene in alcuna terra ed usare questa vita fuggendo per li diserti\n",
      "sample nÂ°4: {'Author': 'Valerio Massimo (red. V1', 'Date': '1336', 'Region': 'fior.', 'Sentence': \"A Milano fue ripressa la malvagitÃ  d' una donna in simile bugÃ¬a, nel tempo medesimo di questo signore della republica, in questo modo: \", 'input_ids': [3083, 4817, 23, 3, 26, 138, 5063, 1478, 15, 66, 22, 9538, 20028, 941, 32, 10, 71, 14556, 32, 7683, 15, 3, 5082, 9377, 9, 50, 1460, 208, 5356, 85, 3, 26, 31, 73, 9, 278, 29, 9, 16, 108, 8770, 8143, 2, 9, 6, 3, 29, 15, 40, 3, 13089, 140, 221, 28348, 1227, 13118, 32, 1320, 127, 15, 20, 195, 9, 20237, 9, 6, 16, 13118, 32, 1794, 32, 10, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "Decode ids: Traduci dal volgare allâ€™italiano moderno: A Milano fue ripressa la malvagitÃ  d' una donna in simile buga, nel tempo medesimo di questo signore della republica, in questo modo:\n",
      "sample nÂ°5: {'Author': 'Bono Giamboni', 'Date': '1292', 'Region': 'fior.', 'Sentence': \"uno luogo si mandano lancioni;  la quale cosa i cavalieri l' appellano capo di porco\", 'input_ids': [3083, 4817, 23, 3, 26, 138, 5063, 1478, 15, 66, 22, 9538, 20028, 941, 32, 10, 73, 32, 3, 40, 76, 32, 839, 108, 388, 3768, 32, 3, 1618, 20013, 23, 117, 50, 3, 11433, 15, 576, 7, 9, 3, 23, 212, 2165, 9626, 3, 40, 31, 8319, 1618, 32, 2468, 32, 1227, 22204, 32, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "Decode ids: Traduci dal volgare allâ€™italiano moderno: uno luogo si mandano lancioni; la quale cosa i cavalieri l' appellano capo di porco\n"
     ]
    }
   ],
   "source": [
    "print(tokenized.column_names)\n",
    "for idx, s in enumerate(tokenized.take(5), 1):\n",
    "    print(f\"sample nÂ°{idx}: {s}\")\n",
    "    print(f\"Decode ids: {tokenizer.decode(s[\"input_ids\"], attention_mask=s[\"attention_mask\"], skip_special_tokens=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:23<00:00,  1.79s/it]\n"
     ]
    }
   ],
   "source": [
    "output = []\n",
    "model= model.eval()\n",
    "with torch.no_grad():\n",
    "    tokenized.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
    "    loader = torch.utils.data.DataLoader(tokenized, batch_size=BS)\n",
    "    size = len(tokenized)\n",
    "    for batch in tqdm(loader, dynamic_ncols=True, leave=True):\n",
    "        \n",
    "        input_ids=batch[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask=batch[\"attention_mask\"].to(DEVICE)\n",
    "\n",
    "        pred = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        **params  \n",
    "        )\n",
    "        output.extend(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample nÂ°1\n",
      "User say:\n",
      " Prompt + Ed ecco di subito tutta questa turba degli uccelli si levÃ² a volo dietro all'aquila\n",
      "Model say:\n",
      " Thatâ€™s what youâ€™re saying, a ferocio dalmio dalata dala allâ€™s singing. In your words of love, the exact version of this phrase is fierce and insuasive, and eglia in the sky in its enunciated words, egligligli dalma, egliosa dâ€™ta egli dâ€™a glioglia dâ€™a eglio\n",
      "=======End Sentence nÂ°1=======\n",
      "Sample nÂ°2\n",
      "User say:\n",
      " Prompt + la seconda suole talora per la grande provedenzia fare timoroso, e la prima per l'ardire rendere altrui matto.\n",
      "Model say:\n",
      " That he's written on his own soul with the same expression, the same song, as he wrote it, for all of them to describe him to himâ€”the best description of each male subjectâ€”and the very words that contain the song.\n",
      "=======End Sentence nÂ°2=======\n",
      "Sample nÂ°3\n",
      "User say:\n",
      " Prompt + E dunque, da che queste cose son cosÃ¬, Catellina, e tu non puoi buonamente qui dimorare, dubiti tu d'andartene in alcuna terra ed usare questa vita fuggendo per li diserti\n",
      "Model say:\n",
      " Acciocchi, no. egligli, each man is angry for his glare, and tormental pain at that point, eglia dalable emorevole words.\n",
      "=======End Sentence nÂ°3=======\n",
      "Sample nÂ°4\n",
      "User say:\n",
      " Prompt + A Milano fue ripressa la malvagitÃ  d' una donna in simile bugÃ¬a, nel tempo medesimo di questo signore della republica, in questo modo: \n",
      "Model say:\n",
      " I'm calling them all on the whole world and they're describing each other as you're talking about.\n",
      "=======End Sentence nÂ°4=======\n",
      "Sample nÂ°5\n",
      "User say:\n",
      " Prompt + uno luogo si mandano lancioni;  la quale cosa i cavalieri l' appellano capo di porco\n",
      "Model say:\n",
      " Oh, it's incredible for you to have such an excellent voice. A beautiful one, and all the better than that , with each tormented on the skin of a man that, in his fine snorevole, and the blood of any male tormented,\n",
      "=======End Sentence nÂ°5=======\n",
      "Sample nÂ°6\n",
      "User say:\n",
      " Prompt + Quando li serpenti invellenava di giorno alcuno Romano, allora iera la maraviglia a vedere come li Psille si combattevano al veleno, chÃ© elli imolavano tutto inazzi della loro salive\n",
      "Model say:\n",
      " The kingâ€™s retaint has been questioned over and over again in modern times. The truth for the verse that says it, as they say, is so strong and onorevole, egli dâ€™ each womanâ€™s afflitto and torment dala, the whole of which you are the flimsiest one.\n",
      "=======End Sentence nÂ°6=======\n",
      "Sample nÂ°7\n",
      "User say:\n",
      " Prompt + pregollo che lo liberasse di quella obbligazione, in che egli l' aveva lasciato ubbligato. El gentile uomo assentÃ¬, e liberollo, e fecene carta.\n",
      "Model say:\n",
      " I wish I could a better song for you, it would please you and would you be able to bring your great love for it to you. I would retorte, or torment a male afflitto and torment a shitgun and torment the dalmatic slitting a finger on that very padrel.\n",
      "=======End Sentence nÂ°7=======\n",
      "Sample nÂ°8\n",
      "User say:\n",
      " Prompt + in vano si domanda chi questo Libro scrivesse, con ciÃ² sia cosa che fedelmente si debba credere che l'Auttore di quello fusse lo Spirito santo.\n",
      "Model say:\n",
      " I hope you're correct, that if all men look at me, eglia della inspiration over a single word.\n",
      "=======End Sentence nÂ°8=======\n",
      "Sample nÂ°9\n",
      "User say:\n",
      " Prompt + SicchÃ¨ dolore Ã¨ a udire, quando l' usare l' arme e la fatica ricusano, con grandissimo disonore come pecore essere\n",
      "Model say:\n",
      " It's as if all this king's expression is so strong and powerful that eglid each man in their original expressionâ€”egli egli dal eloquida della impieta luchta tormenting liarse a ferocio edificeâ€”tormentable male afflittoe all's intimo-sonic poetry.\n",
      "=======End Sentence nÂ°9=======\n",
      "Sample nÂ°10\n",
      "User say:\n",
      " Prompt + e quella cosa, la quale Ã¨ diricta et onesta, e con virtute, quella sola penso essere lo bene.\n",
      "Model say:\n",
      " Acciocchi, eglia dalata for the reason why, the most precise sentence that you can say is ferocity and onorevole, tormented in your excellent voice.\n",
      "=======End Sentence nÂ°10=======\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(columns=[\"Original\", \"Translation(Generated)\", \"Evaluation\"]) \n",
    "for idx, (y, y_pred) in enumerate(zip(hf, pred), 1):\n",
    "    print(f\"Sample nÂ°{idx}\")\n",
    "    print(f\"User said:\\n Prompt + {y[\"Sentence\"]}\")\n",
    "    response = tokenizer.decode(y_pred , skip_special_tokens=True)\n",
    "    print(f\"Model said:\\n {response}\")\n",
    "    print(f\"======= End Sentence nÂ°{idx} =======\")\n",
    "\n",
    "    df.loc[len(df), \"Original\"] = y[\"Sentence\"]\n",
    "    df.loc[len(df) -1, \"Translation(Generated)\"] = response\n",
    "\n",
    "df.to_csv(f\"./Translation model({NET.split('/')[0]}).tsv\", index=False, quotechar=\"\\'\", encoding='utf-8', sep=\"\\t\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MNLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
