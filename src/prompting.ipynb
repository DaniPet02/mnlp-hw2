{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Archaic to Modern Italian with Context Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Datases to work with Transformers by Hugging-Face\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "# Imports for Transformers\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer  # Datasets\n",
    "import pandas as pd\n",
    "from datasets.features import Value, Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Promposed Models\n",
    "* google/gemma-3-4b-it - google/gemma-3n-E4B-it-litert-preview (LLM) 🚀\n",
    "* sapienzanlp/Minerva-1B-base-v1.0 🇮🇹 (LMM)\n",
    "* Helsinki-NLP/opus-mt-itc-itc (Machine Translation) 🏆 - use OpusPrompt \n",
    "* facebook/nllb-200-3.3B (Translation)\n",
    "* meta-llama/Llama-3.2-3B\n",
    "* mistralai/Mistral-7B-Instruct-v0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"dataset.csv\"\n",
    "FEATURES = Features(\n",
    "    {\n",
    "        \"Author\": Value(dtype=\"string\"),\n",
    "        \"Date\": Value(dtype=\"string\"),\n",
    "        \"Region\": Value(dtype=\"string\"),\n",
    "        \"Sentence\": Value(dtype=\"string\")\n",
    "    }\n",
    ")\n",
    "NET = \"google/gemma-3-4b-it\"\n",
    "BS = 16\n",
    "PROMPT = \"chat\"\n",
    "DEVICE = ('cuda' if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Styling of Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = None\n",
    "def ft5_std_map(examples):\n",
    "    return tokenizer(\n",
    "        [f\"Traduci dal volgare all’italiano moderno: {example}\" for example in examples[\"Sentence\"]],  \n",
    "        padding=True, \n",
    "        max_length=128,\n",
    "        )\n",
    "\n",
    "\n",
    "def chat_map(examples):\n",
    "    chat = tokenizer.apply_chat_template([\n",
    "    [\n",
    "        {\"role\": \"system\",   \"content\": \"Sei un traduttore esperto di Italiano Antico. Devi tradurre in modo conciso i brani che ti vengono data dallo 'user'\"},\n",
    "        {\"role\": \"user\",     \"content\": \"Traduci 'La corte era in gran fermento.' in Italiano Moderno\"},\n",
    "        {\"role\": \"assistant\",\"content\": \"Italiano Antico: 'La corte era in gran fermento.' Italiano Moderno: 'La corte era molto agitata.'\"},\n",
    "        {\"role\": \"user\",      \"content\": f\"Traduci '{example}' in Italiano Moderno\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"\"}\n",
    "    ] for example in examples[\"Sentence\"]], \n",
    "        tokenize=True,\n",
    "        \n",
    "        return_dict=True,\n",
    "        padding=True,         # adds zeros to make all sequences the same length\n",
    "        truncation=True,      # cuts sequences that are too long\n",
    "        max_length=250,       # (optional) max length of the sequences\n",
    "        return_tensors=\"pt\")\n",
    "\n",
    "    return chat\n",
    "\n",
    "def gemma_1b_map(examples):\n",
    "    chat = tokenizer.apply_chat_template([\n",
    "    [\n",
    "        {\"role\": \"system\",   \"content\": \"Sei un traduttore esperto di Italiano Antico \"},\n",
    "        {\"role\": \"user\",     \"content\": \"Traduci 'La corte era in gran fermento.' in Italiano Moderno\"},\n",
    "        {\"role\": \"assistant\",\"content\": \"Italiano Antico: 'La corte era in gran fermento.' Italiano Moderno: 'La corte era molto agitata.'\"},\n",
    "        {\"role\": \"user\",      \"content\": f\"Traduci '{example}' in Italiano Moderno\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"\"}\n",
    "    ] for example in examples[\"Sentence\"]], \n",
    "        tokenize=True,\n",
    "        \n",
    "        return_dict=True,\n",
    "        padding=True,         # adds zeros to make all sequences the same length\n",
    "        truncation=True,      # cuts sequences that are too long\n",
    "        max_length=250,       # (optional) max length of the sequences\n",
    "        return_tensors=\"pt\")\n",
    "\n",
    "    return chat\n",
    "\n",
    "def mask_std_map(examples):\n",
    "    return tokenizer(\n",
    "        [f\"Old Italian: {example} Modern Italian [MASK]\" for example in examples['Sentence']],  \n",
    "        padding=True, \n",
    "        max_length=128)\n",
    "\n",
    "def minerva_map(examples):\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    return tokenizer(\n",
    "        [f\"Traduci dal volgare all’italiano moderno: {example}\" for example in examples[\"Sentence\"]],  \n",
    "        padding=True, \n",
    "        max_length=128,\n",
    "        )\n",
    "\n",
    "def style_map(examples):\n",
    "    return tokenizer(\n",
    "        [f\"The following sentence represents an example from the Dolce Stil Novo (sweet new style) literary movement, developed in the 13th and 14th century in Italy: {example} Translate it to modern Italian: \" for example in examples[\"Sentence\"]],\n",
    "        padding=True, \n",
    "        max_length=128)\n",
    "\n",
    "def period_region_map(examples):\n",
    "    return tokenizer(\n",
    "        [f\"This sentence {example['Sentence']} was written in {example['Date']}, in the {example['Region']} region. Translate it to Modern Italian\" for example in examples],\n",
    "        padding=True,\n",
    "        max_length=128)\n",
    "\n",
    "def author_map(examples):\n",
    "    return tokenizer(\n",
    "        [f\"This sentence: {example['Sentence']} was written by {example['Author']}. Translate it to Modern Italian\" for example in examples],\n",
    "        padding=True,\n",
    "        max_length=128)\n",
    "\n",
    "def question_map(examples):\n",
    "    return tokenizer(\n",
    "        [f\"Puoi riscrivere questa frase: {example} in uno stile più colloquiale?\" for example in examples['Sentence']],\n",
    "        padding=True,\n",
    "        max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to select the mapping function based on the prompt type\n",
    "\n",
    "\n",
    "match PROMPT:\n",
    "    case \"ft5_std\":\n",
    "        tr = ft5_std_map\n",
    "    case \"gemma-1b\":\n",
    "        tr = gemma_1b_map\n",
    "    case \"mask_std\":\n",
    "        tr = mask_std_map\n",
    "    case \"style\":\n",
    "        tr = style_map\n",
    "    case \"period_region\":\n",
    "        tr = period_region_map\n",
    "    case \"author\":\n",
    "        tr = author_map\n",
    "    case \"question\":\n",
    "        tr = question_map\n",
    "    case \"minerva\":\n",
    "        tr = minerva_map\n",
    "    case \"llma-1b\":\n",
    "        tr = llma_1b_map\n",
    "    case \"chat\":\n",
    "        tr = chat_map\n",
    "        \n",
    "    case _:\n",
    "        raise ValueError(\"Unknown prompt type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Appropriate Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Rete facebook/nllb-200-3.3B non testabile",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mException\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 135\u001b[39m\n\u001b[32m    122\u001b[39m     params = {\n\u001b[32m    123\u001b[39m \n\u001b[32m    124\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmax_new_tokens\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m512\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    131\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mpad_token_id\u001b[39m\u001b[33m\"\u001b[39m:tokenizer.eos_token_id  \u001b[38;5;66;03m# evita warning se manca un token di padding\u001b[39;00m\n\u001b[32m    132\u001b[39m     }\n\u001b[32m    134\u001b[39m \u001b[38;5;28;01mcase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01m_\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRete \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNET\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m non testabile\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mException\u001b[39m: Rete facebook/nllb-200-3.3B non testabile"
     ]
    }
   ],
   "source": [
    "# Switch to select the network and load the appropriate model and tokenizer\n",
    "match NET:\n",
    "    \n",
    "    case \"google/flan-t5-small\" | \"google-t5/t5-small\" | \"google/mt5-small\" | \"google/flan-t5-large\":\n",
    "        from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "        tokenizer = T5Tokenizer.from_pretrained(NET)\n",
    "        model = T5ForConditionalGeneration.from_pretrained(NET, device_map=DEVICE, torch_dtype=torch.float16)\n",
    "     \n",
    "\n",
    "        params = {\n",
    "            \n",
    "            \"max_new_tokens\": 120, # max number of new tokens to generate\n",
    "            \"do_sample\":True,      # enables sampling for more diverse outputs\n",
    "            \"top_k\":50,            # diversity increase by controlling the candidate words\n",
    "            \"top_p\":0.90,          # nucleus sampling for further control over variety\n",
    "            \"temperature\":1.0,     # reduces randomness and increases coherence\n",
    "            \"repetition_penalty\":1.0,  # penalizza ripetizioni\n",
    "            \"num_return_sequences\":10,  # number of generated responses\n",
    "            \"pad_token_id\":tokenizer.eos_token_id  # avoids warning if padding token is missing\n",
    "        }\n",
    "        \n",
    "    case \"google/mt5-base\":\n",
    "        from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "        tokenizer = AutoTokenizer.from_pretrained(NET)\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(NET, device_map=DEVICE, torch_dtype=torch.float16)\n",
    " \n",
    "\n",
    "        params = {\n",
    "            \n",
    "            \"max_new_tokens\": 120,\n",
    "            \"do_sample\":True,\n",
    "            \"top_k\":10,          \n",
    "            \"top_p\":0.90,          \n",
    "            \"temperature\":1.0,  \n",
    "            \"repetition_penalty\":1.0,\n",
    "            \"num_return_sequences\":10, \n",
    "            \"pad_token_id\":tokenizer.eos_token_id \n",
    "        }\n",
    "\n",
    "    case \"google/gemma-3-1b-it\" | \"google/gemma-3-4b-it\" | \"google/gemma-3n-E4B-it-litert-preview\":\n",
    "        from transformers import BitsAndBytesConfig, Gemma3ForCausalLM, AutoTokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(NET)\n",
    "        quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "        model = Gemma3ForCausalLM.from_pretrained(NET, device_map=DEVICE, quantization_config=quantization_config)\n",
    "    \n",
    "        params = {\n",
    "            \n",
    "            \"max_new_tokens\": 120,\n",
    "            \"do_sample\":True,\n",
    "            \"top_k\":10,     \n",
    "            \"top_p\":0.90,  \n",
    "            \"temperature\":1.0,     \n",
    "            \"repetition_penalty\":1.0,\n",
    "            \"num_return_sequences\":10,  \n",
    "            \"pad_token_id\":tokenizer.eos_token_id \n",
    "        }\n",
    "    \n",
    "    case \"FacebookAI/xlm-roberta-base\":\n",
    "        \n",
    "        from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "        tokenizer = AutoTokenizer.from_pretrained(NET)\n",
    "        model = AutoModelForMaskedLM.from_pretrained(NET)\n",
    "\n",
    "        params = {\n",
    "            \n",
    "            \"max_new_tokens\": 120,\n",
    "            \"do_sample\":True,\n",
    "            \"top_k\":10,            \n",
    "            \"top_p\":0.90,         \n",
    "            \"temperature\":1.0,    \n",
    "            \"repetition_penalty\":1.0, \n",
    "            \"num_return_sequences\":10,  \n",
    "            \"pad_token_id\":tokenizer.eos_token_id \n",
    "        }\n",
    "    case \"sapienzanlp/Minerva-1B-base-v1.0\":\n",
    "        \n",
    "        from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "        tokenizer = AutoTokenizer.from_pretrained(NET)\n",
    "        model = AutoModelForCausalLM.from_pretrained(NET, device_map=DEVICE)\n",
    "  \n",
    "        params = {\n",
    "            \n",
    "            \"max_new_tokens\": 120,\n",
    "            \"do_sample\":True,\n",
    "            \"top_k\":10,            # aumento della diversità controllando le parole candidate\n",
    "            \"top_p\":0.90,          # campionamento nucleus per ulteriori controlli sulla varietà\n",
    "            \"temperature\":1.0,     # riduce la casualità e aumenta la coerenza\n",
    "            \"repetition_penalty\":1.0,  # penalizza ripetizioni\n",
    "            \"num_return_sequences\":10,  # numero di risposte generate\n",
    "            \"pad_token_id\":tokenizer.eos_token_id  # evita warning se manca un token di padding\n",
    "        }\n",
    "    case \"meta-llama/Llama-3.2-3B-Instruct-QLORA_INT4_EO8\":\n",
    "        from transformers import LlamaForCausalLM, AutoTokenizer\n",
    "        from transformers import BitsAndBytesConfig\n",
    "        quantization = BitsAndBytesConfig(load_in_8bit=True)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(NET, padding_side='left', use_fast=False)\n",
    "        \n",
    "        model = LlamaForCausalLM.from_pretrained(NET, device_map=DEVICE, quantization_config=quantization)\n",
    "        params = {\n",
    "            \n",
    "            \"max_new_tokens\": 120,\n",
    "            \"do_sample\":True,\n",
    "            \"top_k\":10,            # aumento della diversità controllando le parole candidate\n",
    "            \"top_p\":0.90,          # campionamento nucleus per ulteriori controlli sulla varietà\n",
    "            \"temperature\":1.0,     # riduce la casualità e aumenta la coerenza\n",
    "            \"repetition_penalty\":1.0,  # penalizza ripetizioni\n",
    "            \"num_return_sequences\":10,  # numero di risposte generate\n",
    "            \"pad_token_id\":tokenizer.eos_token_id  # evita warning se manca un token di padding\n",
    "        }\n",
    "    \n",
    "    case \"mistralai/Mistral-7B-Instruct-v0.2\":\n",
    "        from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "        from transformers import BitsAndBytesConfig\n",
    "        quantization = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "        )\n",
    "        tokenizer = AutoTokenizer.from_pretrained(NET, padding_side='left')\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model = AutoModelForCausalLM.from_pretrained(NET, device_map=DEVICE, quantization_config=quantization, torch_dtype=torch.bfloat16, attn_implementation=\"sdpa\")\n",
    "        params = {\n",
    "            \n",
    "            \"max_new_tokens\": 512,\n",
    "            \"do_sample\":True,\n",
    "            \"top_k\":10,            # aumento della diversità controllando le parole candidate\n",
    "            \"top_p\":0.90,          # campionamento nucleus per ulteriori controlli sulla varietà\n",
    "            \"temperature\":0.85,     # riduce la casualità e aumenta la coerenza\n",
    "            \"repetition_penalty\":1.0,  # penalizza ripetizioni\n",
    "            #\"num_return_sequences\":10,  # numero di risposte generate\n",
    "            \"pad_token_id\":tokenizer.eos_token_id  # evita warning se manca un token di padding\n",
    "        }\n",
    "\n",
    "    case _:\n",
    "        raise Exception(f\"Rete {NET} non testabile\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def evaluate_and_save(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    tokenized_dataset,\n",
    "    original_dataset,\n",
    "    output_prefix: str,\n",
    "    device: str = \"cuda\",\n",
    "    batch_size: int = 32,\n",
    "    generate_params: dict = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate translations for each example in `tokenized_dataset`, compare against\n",
    "    `original_dataset`, and save a TSV with columns [\"Original\", \"Translation(Generated)\", \"Evaluation\"].\n",
    "\n",
    "    Args:\n",
    "        model           : a HuggingFace seq2seq model\n",
    "        tokenizer       : corresponding tokenizer\n",
    "        tokenized_dataset : a Dataset with fields \"input_ids\" & \"attention_mask\"\n",
    "        original_dataset  : the original (un-tokenized) dataset with field \"Sentence\"\n",
    "        output_prefix   : prefix for the output file; final name will be\n",
    "                          f\"{output_prefix}({model.__class__.__name__}).tsv\"\n",
    "        device          : device to run on, e.g. \"cuda\" or \"cpu\"\n",
    "        batch_size      : generation batch size\n",
    "        generate_params : additional kwargs for `model.generate()`\n",
    "    Returns:\n",
    "        pandas.DataFrame with columns [\"Original\", \"Translation(Generated)\", \"Evaluation\"]\n",
    "    \"\"\"\n",
    "    # Prep\n",
    "    model = model.eval()\n",
    "    generate_params = generate_params or {}\n",
    "    tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "    loader = torch.utils.data.DataLoader(tokenized_dataset, batch_size=batch_size)\n",
    "\n",
    "    # Results container\n",
    "    df = pd.DataFrame(columns=[\"Original\", \"Translation(Generated)\", \"Evaluation\"])\n",
    "\n",
    "    # Generation loop\n",
    "    example_idx = 0\n",
    "    for batch in tqdm(loader, dynamic_ncols=True, leave=True):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            preds = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                **generate_params\n",
    "            )\n",
    "\n",
    "        # Decode & append\n",
    "        for pred in preds:\n",
    "            src_sentence = original_dataset[example_idx][\"Sentence\"]\n",
    "            gen_translation = tokenizer.decode(pred, skip_special_tokens=True)\n",
    "\n",
    "            df.loc[len(df)] = [src_sentence, gen_translation, \"\"]\n",
    "            example_idx += 1\n",
    "\n",
    "    # Save to TSV\n",
    "    filename = f\"{output_prefix}({model.__class__.__name__}).tsv\"\n",
    "    df.to_csv(filename, index=False, quotechar=\"'\", encoding=\"utf-8\", sep=\"\\t\")\n",
    "\n",
    "    print(f\"Saved translations to {filename}\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf = Dataset.from_csv(DATASET, features=FEATURES).shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = hf.map(tr, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Author', 'Date', 'Region', 'Sentence', 'input_ids', 'attention_mask']\n",
      "sample n°1: {'Author': 'Guido da Pisa', 'Date': '1337', 'Region': 'tosc.', 'Sentence': \"Ed ecco di subito tutta questa turba degli uccelli si levò a volo dietro all'aquila\", 'input_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 105, 2364, 107, 1869, 236747, 723, 3031, 196848, 15112, 1071, 1001, 168248, 5307, 2486, 236761, 68131, 3031, 57648, 528, 18573, 3300, 11208, 858, 1615, 3236, 1273, 3163, 67404, 1262, 136658, 756, 2364, 236789, 108, 2035, 25593, 1287, 756, 4967, 50508, 6933, 528, 9085, 28196, 236748, 7085, 528, 168248, 13806, 236748, 106, 107, 105, 4368, 107, 64835, 236748, 5307, 2486, 236787, 756, 4967, 50508, 6933, 528, 9085, 28196, 236748, 7085, 168248, 13806, 236748, 236787, 756, 4967, 50508, 6933, 25965, 93160, 805, 7085, 106, 107, 105, 2364, 107, 2035, 25593, 1287, 756, 4675, 199304, 1001, 79727, 67169, 24903, 7909, 3604, 21751, 98474, 11981, 2083, 16866, 237493, 496, 228821, 157595, 784, 236789, 42709, 5657, 236789, 528, 168248, 13806, 236748, 106, 107, 105, 4368, 107, 106, 107], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "Decode ids: user\n",
      "Sei un traduttore esperto di Italiano Antico. Devi tradurre in modo conciso i brani che ti vengono data dallo 'user'\n",
      "\n",
      "Traduci 'La corte era in gran fermento.' in Italiano Moderno\n",
      "model\n",
      "Italiano Antico: 'La corte era in gran fermento.' Italiano Moderno: 'La corte era molto agitata.'\n",
      "user\n",
      "Traduci 'Ed ecco di subito tutta questa turba degli uccelli si levò a volo dietro all'aquila' in Italiano Moderno\n",
      "model\n",
      "\n",
      "\n",
      "sample n°2: {'Author': 'Bart. da San Concordio', 'Date': '1313', 'Region': 'tosc.', 'Sentence': \"la seconda suole talora per la grande provedenzia fare timoroso, e la prima per l'ardire rendere altrui matto.\", 'input_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 105, 2364, 107, 1869, 236747, 723, 3031, 196848, 15112, 1071, 1001, 168248, 5307, 2486, 236761, 68131, 3031, 57648, 528, 18573, 3300, 11208, 858, 1615, 3236, 1273, 3163, 67404, 1262, 136658, 756, 2364, 236789, 108, 2035, 25593, 1287, 756, 4967, 50508, 6933, 528, 9085, 28196, 236748, 7085, 528, 168248, 13806, 236748, 106, 107, 105, 4368, 107, 64835, 236748, 5307, 2486, 236787, 756, 4967, 50508, 6933, 528, 9085, 28196, 236748, 7085, 168248, 13806, 236748, 236787, 756, 4967, 50508, 6933, 25965, 93160, 805, 7085, 106, 107, 105, 2364, 107, 2035, 25593, 1287, 756, 2149, 65981, 664, 1777, 5883, 3509, 810, 759, 11754, 12183, 7652, 722, 18989, 4648, 504, 13565, 236764, 545, 759, 16790, 810, 537, 236789, 714, 750, 160799, 119239, 236747, 1756, 1071, 7085, 528, 168248, 13806, 236748, 106, 107, 105, 4368, 107, 106, 107], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "Decode ids: user\n",
      "Sei un traduttore esperto di Italiano Antico. Devi tradurre in modo conciso i brani che ti vengono data dallo 'user'\n",
      "\n",
      "Traduci 'La corte era in gran fermento.' in Italiano Moderno\n",
      "model\n",
      "Italiano Antico: 'La corte era in gran fermento.' Italiano Moderno: 'La corte era molto agitata.'\n",
      "user\n",
      "Traduci 'la seconda suole talora per la grande provedenzia fare timoroso, e la prima per l'ardire rendere altrui matto.' in Italiano Moderno\n",
      "model\n",
      "\n",
      "\n",
      "sample n°3: {'Author': 'Prima catilinaria volg. (red. A)', 'Date': '1294', 'Region': 'fior.', 'Sentence': \"E dunque, da che queste cose son così, Catellina, e tu non puoi buonamente qui dimorare, dubiti tu d'andartene in alcuna terra ed usare questa vita fuggendo per li diserti\", 'input_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 105, 2364, 107, 1869, 236747, 723, 3031, 196848, 15112, 1071, 1001, 168248, 5307, 2486, 236761, 68131, 3031, 57648, 528, 18573, 3300, 11208, 858, 1615, 3236, 1273, 3163, 67404, 1262, 136658, 756, 2364, 236789, 108, 2035, 25593, 1287, 756, 4967, 50508, 6933, 528, 9085, 28196, 236748, 7085, 528, 168248, 13806, 236748, 106, 107, 105, 4368, 107, 64835, 236748, 5307, 2486, 236787, 756, 4967, 50508, 6933, 528, 9085, 28196, 236748, 7085, 168248, 13806, 236748, 236787, 756, 4967, 50508, 6933, 25965, 93160, 805, 7085, 106, 107, 105, 2364, 107, 2035, 25593, 1287, 756, 236788, 96953, 236764, 1776, 1273, 59967, 46764, 2369, 33133, 236764, 13953, 713, 1630, 236764, 545, 4379, 1908, 97384, 111966, 3644, 2947, 4045, 504, 733, 236764, 21958, 4037, 4379, 513, 236789, 624, 661, 1633, 528, 206584, 37290, 1511, 119815, 24903, 26627, 517, 28142, 4362, 810, 4510, 864, 84740, 236789, 528, 168248, 13806, 236748, 106, 107, 105, 4368, 107, 106, 107], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "Decode ids: user\n",
      "Sei un traduttore esperto di Italiano Antico. Devi tradurre in modo conciso i brani che ti vengono data dallo 'user'\n",
      "\n",
      "Traduci 'La corte era in gran fermento.' in Italiano Moderno\n",
      "model\n",
      "Italiano Antico: 'La corte era in gran fermento.' Italiano Moderno: 'La corte era molto agitata.'\n",
      "user\n",
      "Traduci 'E dunque, da che queste cose son così, Catellina, e tu non puoi buonamente qui dimorare, dubiti tu d'andartene in alcuna terra ed usare questa vita fuggendo per li diserti' in Italiano Moderno\n",
      "model\n",
      "\n",
      "\n",
      "sample n°4: {'Author': 'Valerio Massimo (red. V1', 'Date': '1336', 'Region': 'fior.', 'Sentence': \"A Milano fue ripressa la malvagità d' una donna in simile bugìa, nel tempo medesimo di questo signore della republica, in questo modo: \", 'input_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 105, 2364, 107, 1869, 236747, 723, 3031, 196848, 15112, 1071, 1001, 168248, 5307, 2486, 236761, 68131, 3031, 57648, 528, 18573, 3300, 11208, 858, 1615, 3236, 1273, 3163, 67404, 1262, 136658, 756, 2364, 236789, 108, 2035, 25593, 1287, 756, 4967, 50508, 6933, 528, 9085, 28196, 236748, 7085, 528, 168248, 13806, 236748, 106, 107, 105, 4368, 107, 64835, 236748, 5307, 2486, 236787, 756, 4967, 50508, 6933, 528, 9085, 28196, 236748, 7085, 168248, 13806, 236748, 236787, 756, 4967, 50508, 6933, 25965, 93160, 805, 7085, 106, 107, 105, 2364, 107, 2035, 25593, 1287, 756, 236776, 59658, 9759, 3382, 6473, 236746, 759, 4405, 109841, 8957, 513, 236789, 1985, 61502, 528, 129685, 13582, 237241, 236746, 236764, 9781, 15295, 1470, 184383, 1001, 16196, 1519, 663, 5955, 34295, 236746, 236764, 528, 16196, 18573, 236787, 756, 528, 168248, 13806, 236748, 106, 107, 105, 4368, 107, 106, 107], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "Decode ids: user\n",
      "Sei un traduttore esperto di Italiano Antico. Devi tradurre in modo conciso i brani che ti vengono data dallo 'user'\n",
      "\n",
      "Traduci 'La corte era in gran fermento.' in Italiano Moderno\n",
      "model\n",
      "Italiano Antico: 'La corte era in gran fermento.' Italiano Moderno: 'La corte era molto agitata.'\n",
      "user\n",
      "Traduci 'A Milano fue ripressa la malvagità d' una donna in simile bugìa, nel tempo medesimo di questo signore della republica, in questo modo: ' in Italiano Moderno\n",
      "model\n",
      "\n",
      "\n",
      "sample n°5: {'Author': 'Bono Giamboni', 'Date': '1292', 'Region': 'fior.', 'Sentence': \"uno luogo si mandano lancioni;  la quale cosa i cavalieri l' appellano capo di porco\", 'input_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 105, 2364, 107, 1869, 236747, 723, 3031, 196848, 15112, 1071, 1001, 168248, 5307, 2486, 236761, 68131, 3031, 57648, 528, 18573, 3300, 11208, 858, 1615, 3236, 1273, 3163, 67404, 1262, 136658, 756, 2364, 236789, 108, 2035, 25593, 1287, 756, 4967, 50508, 6933, 528, 9085, 28196, 236748, 7085, 528, 168248, 13806, 236748, 106, 107, 105, 4368, 107, 64835, 236748, 5307, 2486, 236787, 756, 4967, 50508, 6933, 528, 9085, 28196, 236748, 7085, 168248, 13806, 236748, 236787, 756, 4967, 50508, 6933, 25965, 93160, 805, 7085, 106, 107, 105, 2364, 107, 2035, 25593, 1287, 756, 12257, 74843, 2083, 9324, 3173, 44443, 7891, 236793, 138, 2149, 42169, 26617, 858, 63396, 32736, 537, 236789, 24942, 3173, 131103, 1001, 1839, 1364, 236789, 528, 168248, 13806, 236748, 106, 107, 105, 4368, 107, 106, 107], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "Decode ids: user\n",
      "Sei un traduttore esperto di Italiano Antico. Devi tradurre in modo conciso i brani che ti vengono data dallo 'user'\n",
      "\n",
      "Traduci 'La corte era in gran fermento.' in Italiano Moderno\n",
      "model\n",
      "Italiano Antico: 'La corte era in gran fermento.' Italiano Moderno: 'La corte era molto agitata.'\n",
      "user\n",
      "Traduci 'uno luogo si mandano lancioni;  la quale cosa i cavalieri l' appellano capo di porco' in Italiano Moderno\n",
      "model\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tokenized.column_names)\n",
    "for idx, s in enumerate(tokenized.take(5), 1):\n",
    "    print(f\"sample n°{idx}: {s}\")\n",
    "    print(f\"Decode ids: {tokenizer.decode(s[\"input_ids\"], attention_mask=s[\"attention_mask\"], skip_special_tokens=True)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd57730c21e04a6eb65c9a1f10630f4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "Invalid key: 97 is out of bounds for size 97",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df = \u001b[43mevaluate_and_save\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenized_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenized\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43moriginal_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_prefix\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./Translation model\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgenerate_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 56\u001b[39m, in \u001b[36mevaluate_and_save\u001b[39m\u001b[34m(model, tokenizer, tokenized_dataset, original_dataset, output_prefix, device, batch_size, generate_params)\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# Decode & append\u001b[39;00m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m pred \u001b[38;5;129;01min\u001b[39;00m preds:\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m     src_sentence = \u001b[43moriginal_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mexample_idx\u001b[49m\u001b[43m]\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mSentence\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     57\u001b[39m     gen_translation = tokenizer.decode(pred, skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     59\u001b[39m     df.loc[\u001b[38;5;28mlen\u001b[39m(df)] = [src_sentence, gen_translation, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/MNLP/lib/python3.13/site-packages/datasets/arrow_dataset.py:2777\u001b[39m, in \u001b[36mDataset.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   2775\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[32m   2776\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2777\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/MNLP/lib/python3.13/site-packages/datasets/arrow_dataset.py:2761\u001b[39m, in \u001b[36mDataset._getitem\u001b[39m\u001b[34m(self, key, **kwargs)\u001b[39m\n\u001b[32m   2759\u001b[39m format_kwargs = format_kwargs \u001b[38;5;28;01mif\u001b[39;00m format_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[32m   2760\u001b[39m formatter = get_formatter(format_type, features=\u001b[38;5;28mself\u001b[39m._info.features, **format_kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m2761\u001b[39m pa_subtable = \u001b[43mquery_table\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_indices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2762\u001b[39m formatted_output = format_table(\n\u001b[32m   2763\u001b[39m     pa_subtable, key, formatter=formatter, format_columns=format_columns, output_all_columns=output_all_columns\n\u001b[32m   2764\u001b[39m )\n\u001b[32m   2765\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/MNLP/lib/python3.13/site-packages/datasets/formatting/formatting.py:607\u001b[39m, in \u001b[36mquery_table\u001b[39m\u001b[34m(table, key, indices)\u001b[39m\n\u001b[32m    605\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    606\u001b[39m     size = indices.num_rows \u001b[38;5;28;01mif\u001b[39;00m indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m table.num_rows\n\u001b[32m--> \u001b[39m\u001b[32m607\u001b[39m     \u001b[43m_check_valid_index_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    608\u001b[39m \u001b[38;5;66;03m# Query the main table\u001b[39;00m\n\u001b[32m    609\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/MNLP/lib/python3.13/site-packages/datasets/formatting/formatting.py:547\u001b[39m, in \u001b[36m_check_valid_index_key\u001b[39m\u001b[34m(key, size)\u001b[39m\n\u001b[32m    545\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mint\u001b[39m):\n\u001b[32m    546\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (key < \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m key + size < \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (key >= size):\n\u001b[32m--> \u001b[39m\u001b[32m547\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid key: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is out of bounds for size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    548\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    549\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mslice\u001b[39m):\n",
      "\u001b[31mIndexError\u001b[39m: Invalid key: 97 is out of bounds for size 97"
     ]
    }
   ],
   "source": [
    "df = evaluate_and_save(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    tokenized_dataset=tokenized,\n",
    "    original_dataset=hf,\n",
    "    output_prefix=\"./Translation model\",\n",
    "    device=DEVICE,\n",
    "    batch_size=BS,\n",
    "    generate_params=params\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MNLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
