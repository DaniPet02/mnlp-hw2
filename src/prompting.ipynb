{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Archaic to Modern Italian with Context Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrea/miniconda3/envs/MNLP/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import Datases to work with Transformers by Hugging-Face\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "# Imports for Transformers\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer  # Datasets\n",
    "import pandas as pd\n",
    "from datasets.features import Value, Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Globals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Promposed Models\n",
    "* google/flan-t5-small - google/mt5-small (text2text model) ::NO_WORK\n",
    "* google/gemma-3-1b-it (LLM) üöÄ\n",
    "* sapienzanlp/Minerva-1B-base-v1.0 üáÆüáπ (LMM)\n",
    "* openai-community/gpt2 (LLM) ::NO IT\n",
    "* Helsinki-NLP/opus-mt-itc-itc (Machine Translation) üèÜ - use OpusPrompt \n",
    "* facebook/nllb-200-3.3B (Translation)\n",
    "* FacebookAI/xlm-roberta-base (fill-mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"dataset.csv\"\n",
    "FEATURES = Features(\n",
    "    {\n",
    "        \"Author\": Value(dtype=\"string\"),\n",
    "        \"Date\": Value(dtype=\"string\"),\n",
    "        \"Region\": Value(dtype=\"string\"),\n",
    "        \"Sentence\": Value(dtype=\"string\")\n",
    "    }\n",
    ")\n",
    "NET = \"t5-base/checkpoint-2982\"\n",
    "BS = 8\n",
    "DEVICE = ('cuda' if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Network Pipline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = None\n",
    "def ft5_std_map(examples):\n",
    "    return tokenizer(\n",
    "        [f\"Traduci dal volgare all‚Äôitaliano moderno: {example}\" for example in examples[\"Sentence\"]],  \n",
    "        padding=True, \n",
    "        max_length=128,\n",
    "        )\n",
    "\n",
    "def gemma_1b_map(examples):\n",
    "    chat = tokenizer.apply_chat_template([\n",
    "    [\n",
    "        {\"role\": \"system\",   \"content\": \"Sei un traduttore esperto di Italiano Antico \"},\n",
    "        {\"role\": \"user\",     \"content\": \"Traduci 'La corte era in gran fermento.' in Italiano Moderno\"},\n",
    "        {\"role\": \"assistant\",\"content\": \"Italiano Antico: 'La corte era in gran fermento.' Italiano Moderno: 'La corte era molto agitata.'\"},\n",
    "        {\"role\": \"user\",      \"content\": f\"Traduci '{example}' in Italiano Moderno\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"\"}\n",
    "    ] for example in examples[\"Sentence\"] ], \n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_dict=True,\n",
    "        padding=True,         # <== aggiunge zeri per rendere le sequenze uguali\n",
    "        truncation=True,      # <== taglia sequenze troppo lunghe\n",
    "        max_length=250,       # (opzionale) puoi specificare una lunghezza massima\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    return chat\n",
    "\n",
    "def mask_std_map(examples):\n",
    "    return tokenizer(\n",
    "        [f\"Old Italian: {example} Modern Italian [MASK]\" for example in examples[\"Sentence\"]],  \n",
    "        padding=True, \n",
    "        max_length=128)\n",
    "\n",
    "def minerva_map(examples):\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    return tokenizer(\n",
    "        [f\"Traduci dal volgare all‚Äôitaliano moderno: {example}\" for example in examples[\"Sentence\"]],  \n",
    "        padding=True, \n",
    "        max_length=128,\n",
    "        )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "match NET:\n",
    "    \n",
    "    case \"google-t5/t5-small\" |  \"google/mt5-small\" | \"t5-base/best\" | \"t5-base/checkpoint-2982\":\n",
    "        from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "        tokenizer = T5Tokenizer.from_pretrained(NET)\n",
    "        model = T5ForConditionalGeneration.from_pretrained(NET, device_map=DEVICE, torch_dtype=torch.float16)\n",
    "        tr = ft5_std_map\n",
    "\n",
    "        params = {\n",
    "            \n",
    "            \"max_new_tokens\": 120,\n",
    "            \"do_sample\":True,\n",
    "            \"top_k\":50,            # aumento della diversit√† controllando le parole candidate\n",
    "            \"top_p\":0.90,          # campionamento nucleus per ulteriori controlli sulla variet√†\n",
    "            \"temperature\":1.0,     # riduce la casualit√† e aumenta la coerenza\n",
    "            \"repetition_penalty\":1.0,  # penalizza ripetizioni\n",
    "            \"num_return_sequences\":10,  # numero di risposte generate\n",
    "            \"pad_token_id\":tokenizer.eos_token_id  # evita warning se manca un token di padding\n",
    "        }\n",
    "    case \"google/mt5-base\":\n",
    "        from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "        tokenizer = AutoTokenizer.from_pretrained(NET)\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(NET, device_map=DEVICE, torch_dtype=torch.float16)\n",
    "        tr = ft5_std_map\n",
    "\n",
    "        params = {\n",
    "            \n",
    "            \"max_new_tokens\": 120,\n",
    "            \"do_sample\":True,\n",
    "            \"top_k\":10,            # aumento della diversit√† controllando le parole candidate\n",
    "            \"top_p\":0.90,          # campionamento nucleus per ulteriori controlli sulla variet√†\n",
    "            \"temperature\":1.0,     # riduce la casualit√† e aumenta la coerenza\n",
    "            \"repetition_penalty\":1.0,  # penalizza ripetizioni\n",
    "            \"num_return_sequences\":10,  # numero di risposte generate\n",
    "            \"pad_token_id\":tokenizer.eos_token_id  # evita warning se manca un token di padding\n",
    "        }\n",
    "    case \"google/gemma-3-1b-it\":\n",
    "        from transformers import BitsAndBytesConfig, Gemma3ForCausalLM, AutoTokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(NET)\n",
    "        quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "        model = Gemma3ForCausalLM.from_pretrained(NET, device_map=DEVICE, quantization_config=quantization_config)\n",
    "        tr = gemma_1b_map\n",
    "        params = {\n",
    "            \n",
    "            \"max_new_tokens\": 120,\n",
    "            \"do_sample\":True,\n",
    "            \"top_k\":10,            # aumento della diversit√† controllando le parole candidate\n",
    "            \"top_p\":0.90,          # campionamento nucleus per ulteriori controlli sulla variet√†\n",
    "            \"temperature\":1.0,     # riduce la casualit√† e aumenta la coerenza\n",
    "            \"repetition_penalty\":1.0,  # penalizza ripetizioni\n",
    "            \"num_return_sequences\":10,  # numero di risposte generate\n",
    "            \"pad_token_id\":tokenizer.eos_token_id  # evita warning se manca un token di padding\n",
    "        }\n",
    "    \n",
    "    case \"FacebookAI/xlm-roberta-base\":\n",
    "        \n",
    "        from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "        tokenizer = AutoTokenizer.from_pretrained(NET)\n",
    "        model = AutoModelForMaskedLM.from_pretrained(NET)\n",
    "        tr = mask_std_map\n",
    "        params = {\n",
    "            \n",
    "            \"max_new_tokens\": 120,\n",
    "            \"do_sample\":True,\n",
    "            \"top_k\":10,            # aumento della diversit√† controllando le parole candidate\n",
    "            \"top_p\":0.90,          # campionamento nucleus per ulteriori controlli sulla variet√†\n",
    "            \"temperature\":1.0,     # riduce la casualit√† e aumenta la coerenza\n",
    "            \"repetition_penalty\":1.0,  # penalizza ripetizioni\n",
    "            \"num_return_sequences\":10,  # numero di risposte generate\n",
    "            \"pad_token_id\":tokenizer.eos_token_id  # evita warning se manca un token di padding\n",
    "        }\n",
    "    case \"sapienzanlp/Minerva-1B-base-v1.0\":\n",
    "        \n",
    "        from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "        tokenizer = AutoTokenizer.from_pretrained(NET)\n",
    "        model = AutoModelForCausalLM.from_pretrained(NET, device_map=DEVICE)\n",
    "        tr = minerva_map\n",
    "        params = {\n",
    "            \n",
    "            \"max_new_tokens\": 120,\n",
    "            \"do_sample\":True,\n",
    "            \"top_k\":10,            # aumento della diversit√† controllando le parole candidate\n",
    "            \"top_p\":0.90,          # campionamento nucleus per ulteriori controlli sulla variet√†\n",
    "            \"temperature\":1.0,     # riduce la casualit√† e aumenta la coerenza\n",
    "            \"repetition_penalty\":1.0,  # penalizza ripetizioni\n",
    "            \"num_return_sequences\":10,  # numero di risposte generate\n",
    "            \"pad_token_id\":tokenizer.eos_token_id  # evita warning se manca un token di padding\n",
    "        }\n",
    "    case \"openai-community/gpt2\":\n",
    "        from transformers import GPT2Tokenizer, AutoModelForCausalLM\n",
    "        tokenizer = GPT2Tokenizer.from_pretrained(NET)\n",
    "        model = AutoModelForCausalLM.from_pretrained(NET, device_map=DEVICE)\n",
    "        tr = minerva_map\n",
    "        params = {\n",
    "            \n",
    "            \"max_new_tokens\": 120,\n",
    "            \"do_sample\":True,\n",
    "            \"top_k\":10,            # aumento della diversit√† controllando le parole candidate\n",
    "            \"top_p\":0.90,          # campionamento nucleus per ulteriori controlli sulla variet√†\n",
    "            \"temperature\":1.0,     # riduce la casualit√† e aumenta la coerenza\n",
    "            \"repetition_penalty\":1.0,  # penalizza ripetizioni\n",
    "            \"num_return_sequences\":10,  # numero di risposte generate\n",
    "            \"pad_token_id\":tokenizer.eos_token_id  # evita warning se manca un token di padding\n",
    "        }\n",
    "\n",
    "    case _:\n",
    "        raise Exception(f\"Rete {NET} non testabile\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf = Dataset.from_csv(DATASET, features=FEATURES).shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized =hf.map(tr, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Author', 'Date', 'Region', 'Sentence', 'input_ids', 'attention_mask']\n",
      "sample n¬∞1: {'Author': 'Guido da Pisa', 'Date': '1337', 'Region': 'tosc.', 'Sentence': \"Ed ecco di subito tutta questa turba degli uccelli si lev√≤ a volo dietro all'aquila\", 'input_ids': [3083, 4817, 23, 3, 26, 138, 5063, 1478, 15, 66, 22, 9538, 20028, 941, 32, 10, 4857, 3, 15, 75, 509, 1227, 769, 23, 235, 13829, 17, 9, 13118, 9, 3, 2905, 115, 9, 20, 4707, 3, 17431, 7999, 108, 3, 10912, 2, 3, 9, 5063, 32, 1227, 15252, 66, 31, 9, 1169, 521, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "Decode ids: Traduci dal volgare all‚Äôitaliano moderno: Ed ecco di subito tutta questa turba degli uccelli si lev a volo dietro all'aquila\n",
      "sample n¬∞2: {'Author': 'Bart. da San Concordio', 'Date': '1313', 'Region': 'tosc.', 'Sentence': \"la seconda suole talora per la grande provedenzia fare timoroso, e la prima per l'ardire rendere altrui matto.\", 'input_ids': [3083, 4817, 23, 3, 26, 138, 5063, 1478, 15, 66, 22, 9538, 20028, 941, 32, 10, 50, 511, 9, 2629, 32, 109, 3, 17, 9, 322, 9, 399, 50, 3749, 9193, 35, 702, 9, 623, 15, 3, 2998, 32, 1859, 32, 6, 3, 15, 50, 3778, 399, 3, 40, 31, 986, 2060, 16061, 15, 491, 2666, 23, 6928, 235, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "Decode ids: Traduci dal volgare all‚Äôitaliano moderno: la seconda suole talora per la grande provedenzia fare timoroso, e la prima per l'ardire rendere altrui matto.\n",
      "sample n¬∞3: {'Author': 'Prima catilinaria volg. (red. A)', 'Date': '1294', 'Region': 'fior.', 'Sentence': \"E dunque, da che queste cose son cos√¨, Catellina, e tu non puoi buonamente qui dimorare, dubiti tu d'andartene in alcuna terra ed usare questa vita fuggendo per li diserti\", 'input_ids': [3083, 4817, 23, 3, 26, 138, 5063, 1478, 15, 66, 22, 9538, 20028, 941, 32, 10, 262, 146, 29, 835, 6, 836, 3, 1033, 13118, 15, 576, 7, 15, 520, 576, 7, 2, 6, 1336, 1625, 8280, 6, 3, 15, 3, 17, 76, 529, 4353, 32, 23, 8524, 106, 16666, 285, 15688, 127, 355, 6, 146, 2360, 23, 3, 17, 76, 3, 26, 31, 232, 15422, 15, 16, 491, 75, 202, 9, 3, 12829, 3, 15, 26, 178, 355, 13118, 9, 3, 12411, 3, 14165, 729, 26, 32, 399, 3, 40, 23, 1028, 49, 17, 23, 1, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "Decode ids: Traduci dal volgare all‚Äôitaliano moderno: E dunque, da che queste cose son cos, Catellina, e tu non puoi buonamente qui dimorare, dubiti tu d'andartene in alcuna terra ed usare questa vita fuggendo per li diserti\n",
      "sample n¬∞4: {'Author': 'Valerio Massimo (red. V1', 'Date': '1336', 'Region': 'fior.', 'Sentence': \"A Milano fue ripressa la malvagit√† d' una donna in simile bug√¨a, nel tempo medesimo di questo signore della republica, in questo modo: \", 'input_ids': [3083, 4817, 23, 3, 26, 138, 5063, 1478, 15, 66, 22, 9538, 20028, 941, 32, 10, 71, 14556, 32, 7683, 15, 3, 5082, 9377, 9, 50, 1460, 208, 5356, 85, 3, 26, 31, 73, 9, 278, 29, 9, 16, 108, 8770, 8143, 2, 9, 6, 3, 29, 15, 40, 3, 13089, 140, 221, 28348, 1227, 13118, 32, 1320, 127, 15, 20, 195, 9, 20237, 9, 6, 16, 13118, 32, 1794, 32, 10, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "Decode ids: Traduci dal volgare all‚Äôitaliano moderno: A Milano fue ripressa la malvagit√† d' una donna in simile buga, nel tempo medesimo di questo signore della republica, in questo modo:\n",
      "sample n¬∞5: {'Author': 'Bono Giamboni', 'Date': '1292', 'Region': 'fior.', 'Sentence': \"uno luogo si mandano lancioni;  la quale cosa i cavalieri l' appellano capo di porco\", 'input_ids': [3083, 4817, 23, 3, 26, 138, 5063, 1478, 15, 66, 22, 9538, 20028, 941, 32, 10, 73, 32, 3, 40, 76, 32, 839, 108, 388, 3768, 32, 3, 1618, 20013, 23, 117, 50, 3, 11433, 15, 576, 7, 9, 3, 23, 212, 2165, 9626, 3, 40, 31, 8319, 1618, 32, 2468, 32, 1227, 22204, 32, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "Decode ids: Traduci dal volgare all‚Äôitaliano moderno: uno luogo si mandano lancioni; la quale cosa i cavalieri l' appellano capo di porco\n"
     ]
    }
   ],
   "source": [
    "print(tokenized.column_names)\n",
    "for idx, s in enumerate(tokenized.take(5), 1):\n",
    "    print(f\"sample n¬∞{idx}: {s}\")\n",
    "    print(f\"Decode ids: {tokenizer.decode(s[\"input_ids\"], attention_mask=s[\"attention_mask\"], skip_special_tokens=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [00:23<00:00,  1.79s/it]\n"
     ]
    }
   ],
   "source": [
    "output = []\n",
    "model= model.eval()\n",
    "with torch.no_grad():\n",
    "    tokenized.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
    "    loader = torch.utils.data.DataLoader(tokenized, batch_size=BS)\n",
    "    size = len(tokenized)\n",
    "    for batch in tqdm(loader, dynamic_ncols=True, leave=True):\n",
    "        \n",
    "        input_ids=batch[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask=batch[\"attention_mask\"].to(DEVICE)\n",
    "\n",
    "        pred = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        **params  \n",
    "        )\n",
    "        output.extend(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample n¬∞1\n",
      "User say:\n",
      " Prompt + Ed ecco di subito tutta questa turba degli uccelli si lev√≤ a volo dietro all'aquila\n",
      "Model say:\n",
      " That‚Äôs what you‚Äôre saying, a ferocio dalmio dalata dala all‚Äôs singing. In your words of love, the exact version of this phrase is fierce and insuasive, and eglia in the sky in its enunciated words, egligligli dalma, egliosa d‚Äôta egli d‚Äôa glioglia d‚Äôa eglio\n",
      "=======End Sentence n¬∞1=======\n",
      "Sample n¬∞2\n",
      "User say:\n",
      " Prompt + la seconda suole talora per la grande provedenzia fare timoroso, e la prima per l'ardire rendere altrui matto.\n",
      "Model say:\n",
      " That he's written on his own soul with the same expression, the same song, as he wrote it, for all of them to describe him to him‚Äîthe best description of each male subject‚Äîand the very words that contain the song.\n",
      "=======End Sentence n¬∞2=======\n",
      "Sample n¬∞3\n",
      "User say:\n",
      " Prompt + E dunque, da che queste cose son cos√¨, Catellina, e tu non puoi buonamente qui dimorare, dubiti tu d'andartene in alcuna terra ed usare questa vita fuggendo per li diserti\n",
      "Model say:\n",
      " Acciocchi, no. egligli, each man is angry for his glare, and tormental pain at that point, eglia dalable emorevole words.\n",
      "=======End Sentence n¬∞3=======\n",
      "Sample n¬∞4\n",
      "User say:\n",
      " Prompt + A Milano fue ripressa la malvagit√† d' una donna in simile bug√¨a, nel tempo medesimo di questo signore della republica, in questo modo: \n",
      "Model say:\n",
      " I'm calling them all on the whole world and they're describing each other as you're talking about.\n",
      "=======End Sentence n¬∞4=======\n",
      "Sample n¬∞5\n",
      "User say:\n",
      " Prompt + uno luogo si mandano lancioni;  la quale cosa i cavalieri l' appellano capo di porco\n",
      "Model say:\n",
      " Oh, it's incredible for you to have such an excellent voice. A beautiful one, and all the better than that , with each tormented on the skin of a man that, in his fine snorevole, and the blood of any male tormented,\n",
      "=======End Sentence n¬∞5=======\n",
      "Sample n¬∞6\n",
      "User say:\n",
      " Prompt + Quando li serpenti invellenava di giorno alcuno Romano, allora iera la maraviglia a vedere come li Psille si combattevano al veleno, ch√© elli imolavano tutto inazzi della loro salive\n",
      "Model say:\n",
      " The king‚Äôs retaint has been questioned over and over again in modern times. The truth for the verse that says it, as they say, is so strong and onorevole, egli d‚Äô each woman‚Äôs afflitto and torment dala, the whole of which you are the flimsiest one.\n",
      "=======End Sentence n¬∞6=======\n",
      "Sample n¬∞7\n",
      "User say:\n",
      " Prompt + pregollo che lo liberasse di quella obbligazione, in che egli l' aveva lasciato ubbligato. El gentile uomo assent√¨, e liberollo, e fecene carta.\n",
      "Model say:\n",
      " I wish I could a better song for you, it would please you and would you be able to bring your great love for it to you. I would retorte, or torment a male afflitto and torment a shitgun and torment the dalmatic slitting a finger on that very padrel.\n",
      "=======End Sentence n¬∞7=======\n",
      "Sample n¬∞8\n",
      "User say:\n",
      " Prompt + in vano si domanda chi questo Libro scrivesse, con ci√≤ sia cosa che fedelmente si debba credere che l'Auttore di quello fusse lo Spirito santo.\n",
      "Model say:\n",
      " I hope you're correct, that if all men look at me, eglia della inspiration over a single word.\n",
      "=======End Sentence n¬∞8=======\n",
      "Sample n¬∞9\n",
      "User say:\n",
      " Prompt + Sicch√® dolore √® a udire, quando l' usare l' arme e la fatica ricusano, con grandissimo disonore come pecore essere\n",
      "Model say:\n",
      " It's as if all this king's expression is so strong and powerful that eglid each man in their original expression‚Äîegli egli dal eloquida della impieta luchta tormenting liarse a ferocio edifice‚Äîtormentable male afflittoe all's intimo-sonic poetry.\n",
      "=======End Sentence n¬∞9=======\n",
      "Sample n¬∞10\n",
      "User say:\n",
      " Prompt + e quella cosa, la quale √® diricta et onesta, e con virtute, quella sola penso essere lo bene.\n",
      "Model say:\n",
      " Acciocchi, eglia dalata for the reason why, the most precise sentence that you can say is ferocity and onorevole, tormented in your excellent voice.\n",
      "=======End Sentence n¬∞10=======\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(columns=[\"Original\", \"Translation(Generated)\", \"Evaluation\"]) \n",
    "for idx, (y, y_pred) in enumerate(zip(hf, pred), 1):\n",
    "    print(f\"Sample n¬∞{idx}\")\n",
    "    print(f\"User say:\\n Prompt + {y[\"Sentence\"]}\")\n",
    "    response = tokenizer.decode(y_pred , skip_special_tokens=True)\n",
    "    print(f\"Model say:\\n {response}\")\n",
    "    print(f\"=======End Sentence n¬∞{idx}=======\")\n",
    "\n",
    "    df.loc[len(df), \"Original\"] = y[\"Sentence\"]\n",
    "    df.loc[len(df) -1, \"Translation(Generated)\"] = response\n",
    "\n",
    "df.to_csv(f\"./Translation model({NET.split('/')[0]}).tsv\", index=False, quotechar=\"\\'\", encoding='utf-8', sep=\"\\t\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MNLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
