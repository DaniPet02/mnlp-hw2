{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Archaic to Modern Italian with Context Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import Datases to work with Transformers by Hugging-Face\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "# Imports for Transformers\n",
    "from transformers import AutoTokenizer  # Datasets\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "import numpy as np  # Evaluation\n",
    "import evaluate\n",
    "from datasets import Dataset, load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from transformers import TrainerCallback\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType, PeftModelForSeq2SeqLM # Optimize traning for big models! (more than 1B parameters)\n",
    "import numpy as np\n",
    "import evaluate # Assicurati che evaluate sia importato\n",
    "from datasets.features import Value, Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"dataset.csv\"\n",
    "FEATURES = Features(\n",
    "    {\n",
    "        \"Author\": Value(dtype=\"string\"),\n",
    "        \"Date\": Value(dtype=\"string\"),\n",
    "        \"Region\": Value(dtype=\"string\"),\n",
    "        \"Sentence\": Value(dtype=\"string\")\n",
    "    }\n",
    ")\n",
    "network = \"google/mt5-base\"\n",
    "device = ('cuda' if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Dataset and Tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Author': ['Giovanni da San Miniato', 'Brunetto Latini', 'Filippo da Santa Croce', \"Fatti de' Romani (H+R)\", 'Bono Giamboni'], 'Date': ['1415', '1294', '1323', '1313', '1292'], 'Region': ['tosc.', 'fior.', 'tosc.', 'fior.', 'fior.'], 'Sentence': ['di coloro che cominciano a fare buone operazioni, cioè che colui che ancora non sa amare il prossimo come sé medesimo già cominci a temere i giudicii di Dio. E perché', 'molto di maggiore memoria saranno faccendole al re, perciò che nella nostra cittade sempre fue santo e glorioso il nome reale, e sse furono compagni fue il loro nome santissimo;', \"quello che sopra tutti gli altri perdonasse a' cittadini, e a cui più sicuramente possiate credere; poi ch'egli fu vostro comandatore.\", \"non contasterebe a sua virtude. Allora che il navilio si fue impinto e messo in alto mare per andare diritto per mezo questo trapasso periglioso, l'aira divenne nuvolosa e pioveginosa\", \"Tarentini, i quali erano nati di quegli di Lacedemonia et facta da lloro nobile cittade de' Greci.\"]}\n"
     ]
    }
   ],
   "source": [
    "hf = Dataset.from_csv(DATASET, features=FEATURES).shuffle(seed=42).train_test_split(test_size=0.05, seed=42)\n",
    "print(hf[\"train\"].take(5)[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_prompt(examples):\n",
    "    \"\"\"\n",
    "    standard_prompt function to create the input for the model\n",
    "    It takes a list of examples and returns a list of strings\n",
    "    where each string is a prompt for the model.\n",
    "    \"\"\"\n",
    "    inputs = [\"translate \\\"\" + example + \"\\\" to Italian: \" for example in examples[\"Sentence\"]]\n",
    "    model_inputs = tokenizer(inputs, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    return model_inputs\n",
    "\n",
    "def advanced_prompt(examples):\n",
    "    #TODO: implement advanced_prompt function\n",
    "    inputs = []\n",
    "    for i in range(len(examples[\"Sentence\"])):\n",
    "        inputs.append(\n",
    "            f\"Author: {examples['Author'][i]}\\nDate: {examples['Date'][i]}\\nRegion: {examples['Region'][i]}\\nSentence: {examples['Sentence'][i]}\"\n",
    "        )\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 94/94 [00:00<00:00, 3771.17 examples/s]\n",
      "Map: 100%|██████████| 5/5 [00:00<00:00, 1504.63 examples/s]\n"
     ]
    }
   ],
   "source": [
    "standard_token = hf.map(standard_prompt, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "translate \"molto di maggiore memoria saranno faccendole al re, perci che nella nostra cittade sempre fue santo e glorioso il nome reale, e sse furono compagni fue il loro nome santissimo;\" to Italian:\n"
     ]
    }
   ],
   "source": [
    "src = standard_token[\"train\"]['input_ids'][1]\n",
    "print(tokenizer.decode(src, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement the EvalModel class\n",
    "\n",
    "model = model.eval()\n",
    "text = standard_token['test'][2][\"Sentence\"]\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "ids = inputs.input_ids.to(device)\n",
    "attention = inputs.attention_mask.to(device)\n",
    "\n",
    "output = model.generate(input_ids=ids,  attention_mask=attention, max_new_tokens=120, do_sample=True, top_k=10, top_p=0.95)\n",
    "output = tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence:\n",
      " Signiore\", et nonn ebbe in fastido Cristo cotali parole d'udire.\n",
      "Translated Sentence:\n",
      " \" \"\" et nonn ebbe et nonn in fastido Cristo cotali parole d'udire.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original Sentence:\\n {text}\")\n",
    "print(f\"Translated Sentence:\\n {output}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
