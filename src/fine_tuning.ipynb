{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asJxkfJNJ6Ab"
      },
      "source": [
        "# Fine-tuning for Translation from Ancient to Modern Italian"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZXyfYF5KTyH"
      },
      "source": [
        "## System Setup 🖥️"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWshcyakjQV8"
      },
      "source": [
        "### Drive Interface 📁"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bn2BVRjjKKUZ",
        "outputId": "95a55d51-b204-47e2-85d1-008a0984ccf5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Local env dected\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "    %cp /content/drive/MyDrive/MNLP_HW_2/Many_Naps_Little_Progress/*.* .\n",
        "    %ls\n",
        "except ModuleNotFoundError:\n",
        "    print(\"Local env dected\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgbPZ6tqKLDD"
      },
      "source": [
        "### Additional Dependencies 🐍"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNhSDgXeKKBS",
        "outputId": "a0f02e11-0497-4e18-e78b-45ee5a4dd38f"
      },
      "outputs": [],
      "source": [
        "#!bash ../install_colab.sh >> /dev/null"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_q2wvHwFKt5F"
      },
      "source": [
        "### Hugging Face 🤗"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Ccb4L42zKuVB"
      },
      "outputs": [],
      "source": [
        "import huggingface_hub\n",
        "TOKEN = \"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\"\n",
        "huggingface_hub.login(token=TOKEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LN3PEzDWJ6Ai"
      },
      "outputs": [],
      "source": [
        "# Import Datases to work with Transformers by Hugging-Face\n",
        "import torch\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "# Imports for Transformers\n",
        "from transformers import AutoTokenizer  # Datasets\n",
        "from datasets import Dataset, DatasetDict\n",
        "from utils import Report\n",
        "import numpy as np\n",
        "import evaluate\n",
        "from transformers import DataCollatorForLanguageModeling, Trainer, TrainingArguments, AutoModelForCausalLM          # imports for causal Learning\n",
        "from transformers import DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer, AutoModelForSeq2SeqLM    # imports for Seq2Seq models\n",
        "from peft import LoraConfig, TaskType, LoftQConfig, PeftModelForSeq2SeqLM, PeftModelForCausalLM, get_peft_model     # imports for quantization methods (LoRA etc...)\n",
        "from transformers import EarlyStoppingCallback"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8kgPZC0J6Ak"
      },
      "source": [
        "## Fine-Tuned Models\n",
        "\n",
        "* google/mt5-base (Machine Translation)\n",
        "* sapienzanlp/Minerva-1B-base-v1.0 🇮🇹 (LLM)\n",
        "* sapienzanlp/Minerva-3B-base-v1.0 🇮🇹 (LLM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "QSAlloAIJ6Am"
      },
      "outputs": [],
      "source": [
        "device = ('cuda' if torch.cuda.is_available() else \"cpu\")\n",
        "DATASET_TRAIN = \"train_dataset_ann.csv\"\n",
        "DATASET_TEST = \"test_dataset_ann.csv\"\n",
        "SRC_L = \"Sentence\"\n",
        "TRG_L = \"Target\"\n",
        "network = \"sapienzanlp/Minerva-3B-base-v1.0\"\n",
        "tokenization_method = \"minerva_base\"\n",
        "OUT_DIR = network.split(\"/\")[-1]\n",
        "EPOCHS = 35\n",
        "BATCH_SIZE = 8\n",
        "max_length = 60"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8ivbkfnJ6Aq"
      },
      "source": [
        "## Model Selection\n",
        "\n",
        "Configure Pipeline to select models for fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "c802734e29674453b4553504f77407da"
          ]
        },
        "id": "zw802PcHJ6Ar",
        "outputId": "b7bf3d15-5c77-4c74-ada1-5a78fc0b0e3a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "87405d4185114e2cbbaadee8a6468ee3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 34,078,720 || all params: 2,928,314,880 || trainable%: 1.1638\n"
          ]
        }
      ],
      "source": [
        "# Switch to select the network and load the appropriate model and tokenizer\n",
        "match network:\n",
        "\n",
        "    case \"sapienzanlp/Minerva-3B-base-v1.0\":\n",
        "        from transformers import BitsAndBytesConfig\n",
        "        bnb_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_compute_dtype=torch.bfloat16,  # or torch.float16 if bfloat16 not supported\n",
        "        )\n",
        "\n",
        "        tokenizer = AutoTokenizer.from_pretrained(network)\n",
        "        if tokenizer.pad_token is None:\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "        model = AutoModelForCausalLM.from_pretrained(network, device_map=\"auto\", quantization_config=bnb_config)\n",
        "        data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        "\n",
        "        lora_config = LoraConfig(\n",
        "            task_type=TaskType.CAUSAL_LM,\n",
        "            inference_mode=False,\n",
        "            r=128,\n",
        "            lora_alpha=128,\n",
        "            lora_dropout=0.50\n",
        "            )\n",
        "\n",
        "        qlora_config = LoraConfig(\n",
        "            init_lora_weights=\"loftq\",\n",
        "            loftq_config=LoftQConfig(loftq_bits=4),\n",
        "            r=8,\n",
        "            lora_alpha=16,\n",
        "            target_modules=\"all-linear\",\n",
        "            lora_dropout=0.5,\n",
        "            bias=\"none\",\n",
        "            task_type=\"CAUSAL_LM\"\n",
        "        )\n",
        "\n",
        "        model = get_peft_model(model, lora_config)\n",
        "        model.print_trainable_parameters()\n",
        "\n",
        "        early_stopping_patience = 10\n",
        "        early_stopping_threshold = 0.01\n",
        "\n",
        "        early_callback = EarlyStoppingCallback(\n",
        "            early_stopping_patience=early_stopping_patience, # If validation loss does not improve for 3 consecutive epochs\n",
        "            early_stopping_threshold=early_stopping_threshold # Ignores improvements smaller than 0.001\n",
        "        )\n",
        "\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=OUT_DIR,\n",
        "            learning_rate=3e-5,\n",
        "            weight_decay=1e-2,\n",
        "            warmup_steps=500,\n",
        "            per_device_train_batch_size=BATCH_SIZE,\n",
        "            per_device_eval_batch_size=BATCH_SIZE,\n",
        "            num_train_epochs=EPOCHS,\n",
        "            eval_strategy=\"epoch\",\n",
        "            save_strategy=\"epoch\",\n",
        "            logging_strategy=\"epoch\",\n",
        "            load_best_model_at_end=True,\n",
        "            report_to=\"none\",\n",
        "            save_total_limit=3,\n",
        "            lr_scheduler_type=\"cosine\",\n",
        "            logging_dir=OUT_DIR,\n",
        "            logging_steps=10,\n",
        "            label_names=['labels'],\n",
        "            metric_for_best_model=\"eval_chrf++\",\n",
        "            greater_is_better=True,\n",
        "        )\n",
        "\n",
        "        params = {\n",
        "            #\"max_new_tokens\": max_length, # max number of new tokens to generate\n",
        "            #\"do_sample\":True,      # enables sampling for more diverse outputs\n",
        "            #\"top_k\":100,            # diversity increase by controlling the candidate words\n",
        "            #\"top_p\":0.95,          # nucleus sampling for further control over variety\n",
        "            #\"temperature\":1.0,     # reduces randomness and increases coherence\n",
        "            #\"repetition_penalty\":1.0,  # penalizes repetitions\n",
        "            #\"num_return_sequences\":10,  # number of generated responses\n",
        "            \"pad_token_id\":tokenizer.eos_token_id  # avoids warning if padding token is missing\n",
        "        }\n",
        "        \n",
        "    case \"google/mt5-base\" | \"google/mt5-large\":\n",
        "        tokenizer = AutoTokenizer.from_pretrained(network)\n",
        "        if tokenizer.pad_token is None:\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "        model = AutoModelForSeq2SeqLM.from_pretrained(network, device_map=device, torch_dtype=torch.float32)\n",
        "        data_collator = DataCollatorForSeq2Seq(tokenizer, network)\n",
        "\n",
        "        lora_config = LoraConfig(\n",
        "            task_type=TaskType.SEQ_2_SEQ_LM,\n",
        "            inference_mode=False,\n",
        "            r=64,\n",
        "            lora_alpha=64,\n",
        "            lora_dropout=0.3\n",
        "            )\n",
        "\n",
        "        qlora_config = LoraConfig(\n",
        "            init_lora_weights=\"loftq\",\n",
        "            loftq_config=LoftQConfig(loftq_bits=4),\n",
        "            r=128,\n",
        "            lora_alpha=128*2,\n",
        "            target_modules=\"all-linear\",\n",
        "            lora_dropout=0.3,\n",
        "            bias=\"none\",\n",
        "            task_type=TaskType.SEQ_2_SEQ_LM\n",
        "        )\n",
        "\n",
        "        model = get_peft_model(model, lora_config)\n",
        "        model.print_trainable_parameters()\n",
        "\n",
        "        early_stopping_patience = 3\n",
        "        early_stopping_threshold = 0.01\n",
        "\n",
        "        early_callback = EarlyStoppingCallback(\n",
        "            early_stopping_patience=early_stopping_patience, # If validation loss does not improve for 3 consecutive epochs\n",
        "            early_stopping_threshold=early_stopping_threshold # Ignores improvements smaller than 0.001\n",
        "        )\n",
        "\n",
        "        training_args = Seq2SeqTrainingArguments(\n",
        "            output_dir=OUT_DIR,\n",
        "            learning_rate=4e-4,\n",
        "            weight_decay=3e-4,\n",
        "            warmup_steps=500,\n",
        "            per_device_train_batch_size=BATCH_SIZE,\n",
        "            per_device_eval_batch_size=BATCH_SIZE,\n",
        "            num_train_epochs=EPOCHS,\n",
        "            eval_strategy=\"epoch\",\n",
        "            save_strategy=\"epoch\",\n",
        "            logging_strategy=\"epoch\",\n",
        "            load_best_model_at_end=True,\n",
        "            report_to=\"none\",\n",
        "            save_total_limit=3,\n",
        "            lr_scheduler_type=\"linear\",\n",
        "            logging_dir=OUT_DIR,\n",
        "            logging_steps=10,\n",
        "            label_names=['labels'],\n",
        "            metric_for_best_model=\"eval_chrf++\",\n",
        "            greater_is_better=True,\n",
        "        )\n",
        "\n",
        "        params = {\n",
        "            \"max_new_tokens\": max_length, # max number of new tokens to generate\n",
        "            \"do_sample\":True,      # enables sampling for more diverse outputs\n",
        "            #\"top_k\":100,            # diversity increase by controlling the candidate words\n",
        "            #\"top_p\":0.95,          # nucleus sampling for further control over variety\n",
        "            #\"temperature\":1.0,     # reduces randomness and increases coherence\n",
        "            #\"repetition_penalty\":1.0,  # penalizes repetitions\n",
        "            #\"num_return_sequences\":10,  # number of generated responses\n",
        "            \"pad_token_id\":tokenizer.eos_token_id  # avoids warning if padding token is missing\n",
        "        }\n",
        "\n",
        "    case _:\n",
        "        raise Exception(f\"Rete {network} non testabile\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "xFz9vXsgJ6As"
      },
      "outputs": [],
      "source": [
        "class MyTrainerSeq2Seq(Seq2SeqTrainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
        "        if 'num_items_in_batch' in inputs:\n",
        "            inputs = {k: v for k, v in inputs.items() if k != 'num_items_in_batch'}\n",
        "        return super().compute_loss(model, inputs, return_outputs=return_outputs)\n",
        "\n",
        "class MyTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
        "        if 'num_items_in_batch' in inputs:\n",
        "            inputs = {k: v for k, v in inputs.items() if k != 'num_items_in_batch'}\n",
        "        return super().compute_loss(model, inputs, return_outputs=return_outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "iPt0wW7VJ6At",
        "outputId": "714c0a63-d816-4373-dac9-bbb95acfb8b5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /home/andrea/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /home/andrea/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /home/andrea/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "sacrebleu_metric = evaluate.load(\"sacrebleu\")\n",
        "rouge_metric = evaluate.load(\"rouge\")\n",
        "meteor_metric = evaluate.load(\"meteor\")\n",
        "chrf_metric = evaluate.load(\"chrf\")\n",
        "ter_metric = evaluate.load(\"ter\")\n",
        "\n",
        "\n",
        "def postprocess_text(preds, labels):\n",
        "    preds = [pred.strip() for pred in preds]\n",
        "    labels = [[label.strip()] for label in labels] # Specific format for SacreBLEU\n",
        "    return preds, labels\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    preds_input, label_ids = eval_preds\n",
        "\n",
        "    # Dealing with logits or token IDs for predictions\n",
        "    # If preds_input are logits (es. direct output of training modello)\n",
        "    current_preds = preds_input\n",
        "    if isinstance(current_preds, tuple): # Common in HF Trainer, es. (logits, hidden_states)\n",
        "        current_preds = current_preds[0]\n",
        "\n",
        "    if hasattr(current_preds, \"ndim\") and current_preds.ndim == 3: # Array of logits (batch_size, seq_len, vocab_size)\n",
        "        current_preds_ids = np.argmax(current_preds, axis=-1)\n",
        "    else: # Otherwise, assumed to be token ID (batch_size, seq_len)\n",
        "        current_preds_ids = current_preds\n",
        "\n",
        "    # Decode predictions and labels\n",
        "    decoded_preds_raw = tokenizer.batch_decode(current_preds_ids, skip_special_tokens=True)\n",
        "\n",
        "    # Replace -100 in labels (common for token to be ignored) with pad_token_id for decoding\n",
        "    processed_label_ids = np.where(label_ids != -100, label_ids, tokenizer.pad_token_id)\n",
        "    decoded_labels_raw = tokenizer.batch_decode(processed_label_ids, skip_special_tokens=True)\n",
        "\n",
        "    processed_preds, processed_labels_for_sacrebleu = postprocess_text(decoded_preds_raw, decoded_labels_raw)\n",
        "\n",
        "    # For other metrics (ROUGE, METEOR, CHRF, TER), usually expects a flat list of reference strings\n",
        "    flat_references = [ref[0] for ref in processed_labels_for_sacrebleu]\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    # 1. SacreBLEU\n",
        "    sacrebleu_output = sacrebleu_metric.compute(predictions=processed_preds, references=processed_labels_for_sacrebleu)\n",
        "    if sacrebleu_output and \"score\" in sacrebleu_output:\n",
        "        results[\"bleu\"] = sacrebleu_output[\"score\"]\n",
        "    else:\n",
        "        results[\"bleu\"] = 0.0 # Fallback\n",
        "\n",
        "    # 2. ROUGE (rouge1, rouge2, rougeL, rougeLsum)\n",
        "    rouge_output = rouge_metric.compute(predictions=processed_preds, references=flat_references, use_stemmer=True)\n",
        "    if rouge_output:\n",
        "        results[\"rouge1\"] = rouge_output.get(\"rouge1\", 0.0)\n",
        "        results[\"rouge2\"] = rouge_output.get(\"rouge2\", 0.0)\n",
        "        results[\"rougeL\"] = rouge_output.get(\"rougeL\", 0.0)\n",
        "        results[\"rougeLsum\"] = rouge_output.get(\"rougeLsum\", 0.0) # Spesso più robusto per sommario\n",
        "    else:\n",
        "        results.update({\"rouge1\": 0.0, \"rouge2\": 0.0, \"rougeL\": 0.0, \"rougeLsum\": 0.0})\n",
        "\n",
        "    # 3. METEOR\n",
        "    meteor_output = meteor_metric.compute(predictions=processed_preds, references=flat_references)\n",
        "    if meteor_output and \"meteor\" in meteor_output:\n",
        "        results[\"meteor\"] = meteor_output[\"meteor\"]\n",
        "    else:\n",
        "        results[\"meteor\"] = 0.0\n",
        "\n",
        "    # 4. CHRF++ (CHRF with n-grams of words)\n",
        "    # For CHRF++, word_order (or word_n) is > 0. Default of evaluate.load('chrf') are word_order=0 (CHRF standard).\n",
        "    # Common parameters for CHRF++: word_order=2, beta=2 (beta=2 default)\n",
        "    chrf_output = chrf_metric.compute(predictions=processed_preds, references=flat_references, word_order=2, beta=2)\n",
        "    if chrf_output and \"score\" in chrf_output:\n",
        "        results[\"chrf++\"] = chrf_output[\"score\"] # CHRF++ score\n",
        "    else:\n",
        "        results[\"chrf++\"] = 0.0\n",
        "\n",
        "    # (Optional) CHRF standard (only characters)\n",
        "    # chrf_std_output = chrf_metric.compute(predictions=processed_preds, references=flat_references, word_order=0)\n",
        "    # if chrf_std_output and \"score\" in chrf_std_output:\n",
        "    #     results[\"chrf\"] = chrf_std_output[\"score\"]\n",
        "    # else:\n",
        "    #     results[\"chrf\"] = 0.0\n",
        "\n",
        "    # 5. TER (Translation Edit Rate) - the smaller, the better\n",
        "    ter_output = ter_metric.compute(predictions=processed_preds, references=flat_references)\n",
        "    if ter_output and \"score\" in ter_output:\n",
        "        results[\"ter\"] = ter_output[\"score\"]\n",
        "    else:\n",
        "        results[\"ter\"] = 1.0 # Fallback on worst score TER possible\n",
        "\n",
        "    # Mean length of generated predictions (excluding padding tokens)\n",
        "    # 'current_preds_ids' are ID token of the predictions\n",
        "    prediction_lengths = [np.count_nonzero(pid_seq != tokenizer.pad_token_id) for pid_seq in current_preds_ids]\n",
        "    results[\"gen_len\"] = np.mean(prediction_lengths) if prediction_lengths else 0.0\n",
        "\n",
        "    # Rounding of all numerical results\n",
        "    final_results = {k: round(v, 4) for k, v in results.items() if isinstance(v, (int, float))}\n",
        "\n",
        "    return final_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Ui4ZU02DJ6Au"
      },
      "outputs": [],
      "source": [
        "from datasets.features import Value, Features\n",
        "hf_train = Dataset.from_csv(DATASET_TRAIN, features=\n",
        "    Features({\n",
        "        SRC_L : Value(\"string\"),\n",
        "        TRG_L : Value(\"string\"),\n",
        "        \"Date\": Value(\"string\"),\n",
        "        \"Author\":Value(\"string\"),\n",
        "        \"Region\":Value(\"string\")\n",
        "    }),\n",
        "    split=\"train\"\n",
        "    ).shuffle(2025)\n",
        "\n",
        "hf_test = Dataset.from_csv(DATASET_TEST, features=\n",
        "    Features({\n",
        "        SRC_L : Value(\"string\"),\n",
        "        TRG_L : Value(\"string\"),\n",
        "        \"Date\": Value(\"string\"),\n",
        "        \"Author\":Value(\"string\"),\n",
        "        \"Region\":Value(\"string\")\n",
        "    }),\n",
        "    split=\"test\"\n",
        "    ).shuffle(2025)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCtZMnYMJ6Au"
      },
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "rtW9sFCaJ6Av",
        "outputId": "5856bc07-74c2-4176-adbc-861353c16240"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model max length: 60\n"
          ]
        }
      ],
      "source": [
        "print(f\"model max length: {max_length}\")\n",
        "\n",
        "def noprompt_it_it(examples):\n",
        "    inputs = [example for example in examples[SRC_L]]\n",
        "    targets = [example for example in examples[TRG_L]]\n",
        "\n",
        "    model_inputs = tokenizer(inputs, text_target=targets, max_length=max_length, truncation=True, padding=\"max_length\")\n",
        "    return model_inputs\n",
        "\n",
        "# I. Rewrite\n",
        "# II. Translate\n",
        "# III. Correct\n",
        "def minerva_base_prompt_it_it_train(examples):\n",
        "\n",
        "    prompts = [\n",
        "        f\"\"\"riscrivi la seguente frase '{src}' scritta in italiano arcaico in Italiano moderno: {dst}\"\"\"\n",
        "        for src, dst, dat, dia in zip(examples[SRC_L], examples[TRG_L], examples[\"Date\"], examples[\"Region\"])\n",
        "    ]\n",
        "\n",
        "    # Tokenizes input+target and creates label with same tokens\n",
        "    model_inputs = tokenizer(\n",
        "        prompts,\n",
        "        max_length=max_length,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\"\n",
        "    )\n",
        "\n",
        "    model_inputs[\"labels\"] = [\n",
        "        [(label if label != tokenizer.pad_token_id else -100) for label in input_ids]\n",
        "        for input_ids in model_inputs[\"input_ids\"]\n",
        "    ]\n",
        "    return model_inputs\n",
        "\n",
        "def minerva_base_prompt_it_it_eval(examples):\n",
        "    prompts = [\n",
        "\n",
        "        f\"\"\"riscrivi la seguente frase '{src}' scritta in italiano arcaico in Italiano moderno: \"\"\"\n",
        "        for src in examples[SRC_L]\n",
        "    ]\n",
        "\n",
        "    # Tokenizes input+target and creates label with same tokens\n",
        "    model_inputs = tokenizer(\n",
        "        prompts,\n",
        "        max_length=max_length,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\"\n",
        "    )\n",
        "\n",
        "    return model_inputs\n",
        "\n",
        "def base_prompt_en(examples):\n",
        "    inputs = [\"translate from Ancient Italian to Modern Italian: \" + example for example in examples[SRC_L]]\n",
        "\n",
        "    # Tokenizes only inputs\n",
        "    model_inputs = tokenizer(inputs, max_length=max_length, truncation=True, padding=\"max_length\")\n",
        "    targets = [example for example in examples[TRG_L]]\n",
        "\n",
        "    model_inputs = tokenizer(inputs, text_target=targets, max_length=max_length, truncation=True, padding=\"longest\")\n",
        "\n",
        "    return model_inputs\n",
        "\n",
        "def base_prompt_it_it(examples):\n",
        "    inputs = [\"Riscrivi dall'Italiano Antico a l'Italiano Moderno: \" + example for example in examples[SRC_L]]\n",
        "\n",
        "    # Tokenizes only inputs\n",
        "    model_inputs = tokenizer(inputs, max_length=max_length, truncation=True, padding=\"max_length\")\n",
        "    targets = [example for example in examples[TRG_L]]\n",
        "\n",
        "    model_inputs = tokenizer(inputs, text_target=targets, max_length=max_length, truncation=True, padding=\"longest\")\n",
        "\n",
        "    return model_inputs\n",
        "\n",
        "def parafrasi_prompt_it_it(examples):\n",
        "    inputs = [\"Scrivi la parafrasi di questo testo: \" + example for example in examples[SRC_L]]\n",
        "    targets = [example for example in examples[TRG_L]]\n",
        "\n",
        "    model_inputs = tokenizer(inputs, text_target=targets, max_length=max_length, truncation=True, padding=\"longest\")\n",
        "    return model_inputs\n",
        "\n",
        "def informative_prompt_it_it(examples):\n",
        "    inputs = [f\"Riscrivi in uno stile più moderno il testo del seguente Autore: '{author}', anno di scrittura: {date}, luogo: Italia, dialetto: '{region}', testo: '{text}'.\" for text, date, region, author in zip(examples[SRC_L], examples[\"Date\"], examples[\"Region\"], examples[\"Author\"]) ]\n",
        "    targets = [example for example in examples[TRG_L]]\n",
        "\n",
        "    model_inputs = tokenizer(inputs, text_target=targets, max_length=max_length, truncation=True, padding=\"longest\")\n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BFXkm0YJ6Av"
      },
      "source": [
        "## Tokenizer Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "e439f25244e041b8b00b62a4e79edd36",
            "86c58244034945728ce237f1d28e215d"
          ]
        },
        "id": "luEQkXHIJ6Aw",
        "outputId": "297d2697-d874-441c-85ef-bf3f45f1cfde"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c6205d62d5db478583a5e8f962ee7904",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/97 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b96f4e872ba049fdbfedf1e55a5e7251",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "match tokenization_method:\n",
        "    case \"minerva_base\":\n",
        "        map_callback_train = minerva_base_prompt_it_it_train\n",
        "        map_callback_eval   = minerva_base_prompt_it_it_eval\n",
        "\n",
        "        hf_tokenized = DatasetDict({\n",
        "            \"train\": hf_train.map(map_callback_train, batched=True),\n",
        "            \"test\":  hf_test.map(map_callback_eval, batched=True)\n",
        "        })\n",
        "\n",
        "        hf_tokenized.set_format(type=\"torch\", columns=['input_ids', 'attention_mask'])\n",
        "\n",
        "    case \"base_prompt_en\":\n",
        "        map_callback = base_prompt_en\n",
        "        hf_tokenized = DatasetDict({\n",
        "            \"train\": hf_train.map(base_prompt_en, batched=True),\n",
        "            \"test\":  hf_test.map(base_prompt_en, batched=True)\n",
        "        })\n",
        "\n",
        "    case \"base_prompt_it_it\":\n",
        "        map_callback = base_prompt_it_it\n",
        "        hf_tokenized = DatasetDict({\n",
        "            \"train\": hf_train.map(base_prompt_it_it, batched=True),\n",
        "            \"test\":  hf_test.map(base_prompt_it_it, batched=True)\n",
        "        })\n",
        "        \n",
        "    case _:\n",
        "        raise ValueError(\"Tokenization method not avaiable\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "X8_EGew_J6Aw",
        "outputId": "c5da6260-613d-4cb6-e04f-c3b475fa36b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'train': ['Sentence', 'Target', 'Date', 'Author', 'Region', 'input_ids', 'attention_mask', 'labels'], 'test': ['Sentence', 'Target', 'Date', 'Author', 'Region', 'input_ids', 'attention_mask']}\n",
            "{'train': (97, 8), 'test': (10, 7)}\n"
          ]
        }
      ],
      "source": [
        "print(hf_tokenized.column_names)\n",
        "print(hf_tokenized.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "eC3T3SlkJ6Aw",
        "outputId": "32aa2760-ca3b-4bb7-82ec-a67f56d5d680"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===:(sentence n°1):===\n",
            "Sentence:riscrivi la seguente frase 'et nonn ebbe in fastido Cristo cotali parole d'udire.' scritta in italiano arcaico in Italiano moderno: E Cristo non provò fastidio ad ascoltare parole di quel genere.\n",
            "===:(sentence n°2):===\n",
            "Sentence:riscrivi la seguente frase 'e, cosa che anche è più grave, cioè essere preso, o vero fuggire, e suo Comune lasciare vincere.' scritta in italiano arcaico in Italiano moderno: E, cosa ancor più grave che essere catturato, fuggire e lasciare che il proprio\n",
            "===:(sentence n°3):===\n",
            "Sentence:riscrivi la seguente frase 'Se questo piace a tutti e se 'l tempo hae bisogno d'avere Pompeio per cavaliere e non per compagno, non riterrò più i fati.' scritta in italiano arcaico in Italiano moderno: Se questo è quello che tutti\n",
            "===:(sentence n°4):===\n",
            "Sentence:riscrivi la seguente frase 'la moltitudine de' quali tu ài potuto vedere e riguardare lo studio e poco dinanzi udire le voci, e lle cui mani e lance apena posso ritenere.' scritta in italiano arcaico in Italiano moderno: La moltitudine\n",
            "===:(sentence n°5):===\n",
            "Sentence:riscrivi la seguente frase 'Ma no sapeano già le nomora di coloro dela congiurazione, ché la donna no nominava già li nomi.' scritta in italiano arcaico in Italiano moderno: Ma non conoscevano ancora i nomi di quelli della congiura, perché la\n"
          ]
        }
      ],
      "source": [
        "for idx, s in enumerate(hf_tokenized[\"train\"].take(5), 1):\n",
        "    print(f\"===:(sentence n°{idx}):===\")\n",
        "    print(f\"{SRC_L}:{tokenizer.decode(s['input_ids'], attention_mask=s['attention_mask'], skip_special_tokens=True)}\" )\n",
        "    #print(f\"{TRG_L}:{tokenizer.decode(s[\"labels\"], skip_special_tokens=True)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "WB1tROKuJ6Ax",
        "outputId": "82cde9f8-a3c9-454a-ed9b-a6e497b7c4dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===:(sentence n°1):===\n",
            "Sentence:riscrivi la seguente frase 'Nella quale battaglia, certo io ebbi sempre ardire di ragionare di pace e sempre mi dolfi che non solamente la pace era schifata,' scritta in italiano arcaico in Italiano moderno: \n",
            "===:(sentence n°2):===\n",
            "Sentence:riscrivi la seguente frase 'sono due già non in una carne, ma in uno spirito, cioè Iddio, e l' anima. Onde in altro luogo dice S. Paolo: Chi s' accosta a Dio è uno spirito' scritta in italiano arcaico in Italiano\n",
            "===:(sentence n°3):===\n",
            "Sentence:riscrivi la seguente frase 'Altressì uno amante chiamando merzé alla sua donna dice parole e ragioni molte, et ella si difende in suo dire.' scritta in italiano arcaico in Italiano moderno: \n",
            "===:(sentence n°4):===\n",
            "Sentence:riscrivi la seguente frase 'Pietro, essendogli mostrato in figura il populo Gentile, sì gli fu detto: ammazza, e mangia' scritta in italiano arcaico in Italiano moderno: \n",
            "===:(sentence n°5):===\n",
            "Sentence:riscrivi la seguente frase 'A Milano fue ripressa la malvagità d' una donna in simile bugìa, nel tempo medesimo di questo signore della republica, in questo modo: ' scritta in italiano arcaico in Italiano moderno: \n"
          ]
        }
      ],
      "source": [
        "for idx, s in enumerate(hf_tokenized[\"test\"].take(5), 1):\n",
        "    print(f\"===:(sentence n°{idx}):===\")\n",
        "    print(f\"{SRC_L}:{tokenizer.decode(s['input_ids'], attention_mask=s['attention_mask'], skip_special_tokens=True)}\" )\n",
        "    #print(f\"{TRG_L}:{tokenizer.decode(s[\"labels\"], skip_special_tokens=True)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLsr_a4RJ6Ax"
      },
      "source": [
        "## Models & Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aNXSOX0J6Ay"
      },
      "source": [
        "### PEFT Fine-Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "KGZ5c0DcJ6Ay",
        "outputId": "67462809-6c0c-40af-a623-f77612bdd67c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CAUSAL Generation\n"
          ]
        }
      ],
      "source": [
        "if isinstance(model, PeftModelForSeq2SeqLM):\n",
        "    print(\"[SEQ2SEQ Generation]\")\n",
        "    trainer = MyTrainerSeq2Seq(\n",
        "                model=model,\n",
        "                args=training_args,\n",
        "                train_dataset=hf_tokenized[\"train\"],\n",
        "                eval_dataset=hf_tokenized[\"test\"],\n",
        "                processing_class=tokenizer,\n",
        "                data_collator=data_collator,\n",
        "                compute_metrics=compute_metrics,\n",
        "                callbacks=[Report(OUT_DIR)]\n",
        "            )\n",
        "else:\n",
        "    print(\"CAUSAL Generation\")\n",
        "    trainer = MyTrainer(\n",
        "                model=model,\n",
        "                args=training_args,\n",
        "                train_dataset=hf_tokenized[\"train\"],\n",
        "                eval_dataset=hf_tokenized[\"test\"],\n",
        "                processing_class=tokenizer,\n",
        "                data_collator=data_collator,\n",
        "                compute_metrics=compute_metrics,\n",
        "                callbacks=[Report(OUT_DIR), early_callback]\n",
        "            )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "UojACT4JJ6Ay",
        "outputId": "bda716fb-9292-48bc-f5ee-d9464f454b6f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='11' max='455' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 11/455 00:05 < 04:35, 1.61 it/s, Epoch 0.77/35]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/MNLP/lib/python3.13/site-packages/transformers/trainer.py:2240\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2238\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2239\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2240\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2241\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2242\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2243\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2244\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2245\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/MNLP/lib/python3.13/site-packages/transformers/trainer.py:2555\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2548\u001b[39m context = (\n\u001b[32m   2549\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2550\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2551\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2552\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2553\u001b[39m )\n\u001b[32m   2554\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2555\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2557\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2558\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2559\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2560\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2561\u001b[39m ):\n\u001b[32m   2562\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2563\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/MNLP/lib/python3.13/site-packages/transformers/trainer.py:3791\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   3788\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type == DistributedType.DEEPSPEED:\n\u001b[32m   3789\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mscale_wrt_gas\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3791\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3793\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss.detach()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/MNLP/lib/python3.13/site-packages/accelerate/accelerator.py:2473\u001b[39m, in \u001b[36mAccelerator.backward\u001b[39m\u001b[34m(self, loss, **kwargs)\u001b[39m\n\u001b[32m   2471\u001b[39m     \u001b[38;5;28mself\u001b[39m.lomo_backward(loss, learning_rate)\n\u001b[32m   2472\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2473\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/MNLP/lib/python3.13/site-packages/torch/_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/MNLP/lib/python3.13/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/MNLP/lib/python3.13/site-packages/torch/autograd/graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "5ba49f79e3d2462baf2deaf3898bea07"
          ]
        },
        "id": "Mvm49C6vJ6Az",
        "outputId": "dbb38e13-993c-4b56-8056-d4e1557a9628"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start generation on cuda\n",
            "=============================\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "36e154ffdc164de4bab2cd2583b3da39",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===:0-(Model for Prompt)===\n",
            "riscrivi la seguente frase 'Nella quale battaglia, certo io ebbi sempre ardire di ragionare di pace e sempre mi dolfi che non solamente la pace era schifata,' scritta in italiano arcaico in Italiano moderno: \n",
            "=========================\n",
            "===:(model sapienzanlp/Minerva-3B-base-v1.0):===\n",
            "riscrivi la seguente frase 'Nella quale battaglia, certo io ebbi sempre ardire di ragionare di pace e sempre mi dolfi che non solamente la pace era schifata,' scritta in italiano arcaico in Italiano moderno: ​​Nella quale battaglia, certo io ebbi sempre il coraggio di discutere di pace e sempre mi dolsi che non solo la pace era schifata, ma anche la guerra. Il 2019 è stato un anno ricco di novità per il mondo del lavoro: la legge di Bilancio ha introdotto importanti novità in materia di lavoro, tra cui il contratto di\n",
            "=================================\n",
            "===:1-(Model for Prompt)===\n",
            "riscrivi la seguente frase 'sono due già non in una carne, ma in uno spirito, cioè Iddio, e l' anima. Onde in altro luogo dice S. Paolo: Chi s' accosta a Dio è uno spirito' scritta in italiano arcaico in Italiano moderno: \n",
            "=========================\n",
            "===:(model sapienzanlp/Minerva-3B-base-v1.0):===\n",
            "riscrivi la seguente frase 'sono due già non in una carne, ma in uno spirito, cioè Iddio, e l' anima. Onde in altro luogo dice S. Paolo: Chi s' accosta a Dio è uno spirito' scritta in italiano arcaico in Italiano moderno: 2 sono due già non in carne, ma in spirito, cioè in Dio e nell'anima. Quindi in altro luogo dice S. Paolo: Chi si avvicina a Dio è uno spirito.\n",
            "\n",
            "Il 2018 è stato un anno ricco di novità per il mondo della tecnologia e, in particolare, per il mondo degli smartphone. Il 2019, invece, sarà un\n",
            "=================================\n",
            "===:2-(Model for Prompt)===\n",
            "riscrivi la seguente frase 'Altressì uno amante chiamando merzé alla sua donna dice parole e ragioni molte, et ella si difende in suo dire.' scritta in italiano arcaico in Italiano moderno: \n",
            "=========================\n",
            "===:(model sapienzanlp/Minerva-3B-base-v1.0):===\n",
            "riscrivi la seguente frase 'Altressì uno amante chiamando merzé alla sua donna dice parole e ragioni molte, et ella si difende in suo dire.' scritta in italiano arcaico in Italiano moderno: … un amante chiama la sua donna “merzé” e le dice parole e ragioni molte, e lei si difende in suo dire. Il 2019 è stato un anno di grandi cambiamenti per il mondo della comunicazione. Il 2020 sarà l’anno della svolta.\n",
            "Il 2019 è stato un anno di grandi cambiamenti per il\n",
            "=================================\n",
            "===:3-(Model for Prompt)===\n",
            "riscrivi la seguente frase 'Pietro, essendogli mostrato in figura il populo Gentile, sì gli fu detto: ammazza, e mangia' scritta in italiano arcaico in Italiano moderno: \n",
            "=========================\n",
            "===:(model sapienzanlp/Minerva-3B-base-v1.0):===\n",
            "riscrivi la seguente frase 'Pietro, essendogli mostrato in figura il populo Gentile, sì gli fu detto: ammazza, e mangia' scritta in italiano arcaico in Italiano moderno: ​​Pietro, vedendo che il popolo di Roma gli era stato mostrato in figura, gli fu detto: mangia e uccidi. Il 2018 è stato un anno di grandi cambiamenti per il mondo della comunicazione. Il 2019 sarà l’anno della svolta.\n",
            "Il 2018 è stato un anno di grandi cambiamenti per\n",
            "=================================\n",
            "===:4-(Model for Prompt)===\n",
            "riscrivi la seguente frase 'A Milano fue ripressa la malvagità d' una donna in simile bugìa, nel tempo medesimo di questo signore della republica, in questo modo: ' scritta in italiano arcaico in Italiano moderno: \n",
            "=========================\n",
            "===:(model sapienzanlp/Minerva-3B-base-v1.0):===\n",
            "riscrivi la seguente frase 'A Milano fue ripressa la malvagità d' una donna in simile bugìa, nel tempo medesimo di questo signore della republica, in questo modo: ' scritta in italiano arcaico in Italiano moderno: ella fu ripresa dalla malvagità di una donna in modo simile. Il 2019 è stato un anno di grandi cambiamenti per il mondo della comunicazione. Il 2020 sarà l’anno della svolta.\n",
            "Il 2019 è stato un anno di grandi cambiamenti per il mondo della comunicazione. Il 2020 sarà l’anno della svolta.\n",
            "=================================\n",
            "===:5-(Model for Prompt)===\n",
            "riscrivi la seguente frase 'Il re entrò in uno giardino dietro al suo albergo, quasi come s'egli andasse pensando alla risposta.' scritta in italiano arcaico in Italiano moderno: \n",
            "=========================\n",
            "===:(model sapienzanlp/Minerva-3B-base-v1.0):===\n",
            "riscrivi la seguente frase 'Il re entrò in uno giardino dietro al suo albergo, quasi come s'egli andasse pensando alla risposta.' scritta in italiano arcaico in Italiano moderno: … il re entrò in un giardino dietro al suo albergo, quasi come se si stesse chiedendo la risposta. Il 2019 è stato un anno di grandi cambiamenti per il mondo del lavoro. Il 2020 sarà l’anno della svolta.\n",
            "Il 2019 è stato un anno di grandi cambiamenti per il mondo del lavoro. Il 2\n",
            "=================================\n",
            "===:6-(Model for Prompt)===\n",
            "riscrivi la seguente frase 'Gorgone, e ho questa proprietà che io volo per l'aire sì come uno ucello\".' scritta in italiano arcaico in Italiano moderno: \n",
            "=========================\n",
            "===:(model sapienzanlp/Minerva-3B-base-v1.0):===\n",
            "riscrivi la seguente frase 'Gorgone, e ho questa proprietà che io volo per l'aire sì come uno ucello\".' scritta in italiano arcaico in Italiano moderno: … e ho questa proprietà che io volo per l’aria come uno uccello. Il 2019 è stato un anno ricco di novità per il mondo del lavoro: la legge di Bilancio ha introdotto importanti novità in materia di lavoro, tra cui il contratto di espansione, che permette di anticipare l’uscita dei lavoratori più anziani.\n",
            "Il contratto di espansione è un contratto\n",
            "=================================\n",
            "===:7-(Model for Prompt)===\n",
            "riscrivi la seguente frase 'Ma l' occhio della intelligenza è più alto. Perciò che, passata la grandezza della universitade, quella medesima semplice forma vede nella sottil vista' scritta in italiano arcaico in Italiano moderno: \n",
            "=========================\n",
            "===:(model sapienzanlp/Minerva-3B-base-v1.0):===\n",
            "riscrivi la seguente frase 'Ma l' occhio della intelligenza è più alto. Perciò che, passata la grandezza della universitade, quella medesima semplice forma vede nella sottil vista' scritta in italiano arcaico in Italiano moderno: … ma l’occhio dell’intelligenza è più alto. Perciò che, passata la grandezza dell’università, quella stessa forma semplice vede nella sottil vista. Il 2019 è stato un anno ricco di novità per il mondo del lavoro: la legge di Bilancio ha introdotto importanti novità in materia di lavoro, come la riduzione del cuneo fiscale e contributivo, la riduzione\n",
            "=================================\n",
            "===:0-(Model for Prompt)===\n",
            "riscrivi la seguente frase 'e l' acconciamento a fare grandissime cose, cioè a ttenere pace et amare Idio e 'l proximo, a ffare cittadi, castella e magioni' scritta in italiano arcaico in Italiano moderno: \n",
            "=========================\n",
            "===:(model sapienzanlp/Minerva-3B-base-v1.0):===\n",
            "riscrivi la seguente frase 'e l' acconciamento a fare grandissime cose, cioè a ttenere pace et amare Idio e 'l proximo, a ffare cittadi, castella e magioni' scritta in italiano arcaico in Italiano moderno: ​e l'acconciatura per fare grandi cose, cioè per ottenere pace e amare Dio e il prossimo, per costruire città, castelli e fortezze. Il 2019 è stato un anno di grandi cambiamenti per il mondo della comunicazione. Il 2020 sarà l’anno della svolta.\n",
            "Il 2019 è stato un anno\n",
            "=================================\n",
            "===:1-(Model for Prompt)===\n",
            "riscrivi la seguente frase 'Gregorio. Non udii mai che egli avesse maestro; ma il dono dello Spirito Santo non si può stringere a legge.' scritta in italiano arcaico in Italiano moderno: \n",
            "=========================\n",
            "===:(model sapienzanlp/Minerva-3B-base-v1.0):===\n",
            "riscrivi la seguente frase 'Gregorio. Non udii mai che egli avesse maestro; ma il dono dello Spirito Santo non si può stringere a legge.' scritta in italiano arcaico in Italiano moderno: ​​Gregorio non udì mai che egli avesse un maestro; ma il dono dello Spirito Santo non può essere stretto a legge. Il 2019 è stato un anno di grandi cambiamenti per il mondo della comunicazione. Il 2020 sarà l’anno della svolta.\n",
            "Il 2019 è stato un anno di grandi cambiamenti per\n",
            "=================================\n",
            "\n",
            "Generation completed.\n"
          ]
        }
      ],
      "source": [
        "# Sets model in evaluation mode and moves it on device\n",
        "model = model.eval()\n",
        "model = model.to(device)\n",
        "\n",
        "# Creates DataLoader\n",
        "hf_tokenized.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
        "loader = torch.utils.data.DataLoader(hf_tokenized[\"test\"], batch_size=8)\n",
        "\n",
        "\n",
        "print(f\"Start generation on {device}\")\n",
        "print(\"=============================\")\n",
        "\n",
        "for batch in tqdm(loader):\n",
        "    # Move the entire batch to the device\n",
        "    # Note: DataLoader returns a batch as a dictionary of tensors\n",
        "\n",
        "    batch[\"input_ids\"] = batch[\"input_ids\"].to(device)\n",
        "    batch[\"attention_mask\"] = batch[\"attention_mask\"].to(device)\n",
        "\n",
        "    # Generate output for the batch\n",
        "    # Note: The model.generate method expects input_ids and attention_mask\n",
        "    result_ids = model.generate(\n",
        "        input_ids=batch[\"input_ids\"],\n",
        "        attention_mask=batch[\"attention_mask\"],\n",
        "        max_new_tokens=max_length,\n",
        "    )\n",
        "\n",
        "    # Decodes separately each prompt and each generated result\n",
        "    # Note: result_ids has shape (batch_size, seq_len_output)\n",
        "    # Iterate on batch to decode one by one\n",
        "    # batch[\"input_ids\"] has shape (batch_size, seq_len_input)\n",
        "\n",
        "    # Decode original prompts\n",
        "    decoded_prompts = tokenizer.batch_decode(batch[\"input_ids\"], skip_special_tokens=True)\n",
        "\n",
        "    # Decode gnerated results\n",
        "    decoded_results = tokenizer.batch_decode(result_ids, skip_special_tokens=True)\n",
        "\n",
        "    # Prints results for each batch element\n",
        "    with open(OUT_DIR + \"/output_chat.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "        for i in range(len(decoded_prompts)):\n",
        "            print(f\"===:{i}-(Model for Prompt)===\")\n",
        "            print(f\"{decoded_prompts[i]}\")\n",
        "            print(\"=========================\")\n",
        "\n",
        "            print(f\"===:(model {network}):===\")\n",
        "            print(decoded_results[i])\n",
        "            print(\"=================================\")\n",
        "\n",
        "            ############################################\n",
        "\n",
        "\n",
        "            f.write(\"f===:({i}Model for Prompt)===\")\n",
        "            f.write(f\"{decoded_prompts[i]}\")\n",
        "            f.write(\"=========================\")\n",
        "\n",
        "            f.write(f\"===:(model {network}):===\")\n",
        "            f.write(decoded_results[i])\n",
        "            f.write(\"==========================\")\n",
        "\n",
        "print(\"\\nGeneration completed.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "MNLP",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
