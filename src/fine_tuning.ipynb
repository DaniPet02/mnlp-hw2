{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asJxkfJNJ6Ab"
      },
      "source": [
        "# Fine-tuning for Translation from Ancient to Modern Italian"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZXyfYF5KTyH"
      },
      "source": [
        "## System Setup 🖥️"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWshcyakjQV8"
      },
      "source": [
        "### Google Drive Interface 📁"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bn2BVRjjKKUZ",
        "outputId": "95a55d51-b204-47e2-85d1-008a0984ccf5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Local env dected\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "    %cp /content/drive/MyDrive/MNLP_HW_2/Many_Naps_Little_Progress/*.* .\n",
        "    %ls\n",
        "except ModuleNotFoundError:\n",
        "    print(\"Local env dected\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgbPZ6tqKLDD"
      },
      "source": [
        "### Additional Dependencies 🐍"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNhSDgXeKKBS",
        "outputId": "a0f02e11-0497-4e18-e78b-45ee5a4dd38f"
      },
      "outputs": [],
      "source": [
        "#!bash ../install_colab.sh >> /dev/null"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_q2wvHwFKt5F"
      },
      "source": [
        "### Hugging Face 🤗"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Ccb4L42zKuVB"
      },
      "outputs": [],
      "source": [
        "import huggingface_hub\n",
        "TOKEN = \"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\"\n",
        "huggingface_hub.login(token=TOKEN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Necessary Imports 📚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LN3PEzDWJ6Ai"
      },
      "outputs": [],
      "source": [
        "# Import Datases to work with Transformers by Hugging-Face\n",
        "import torch\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "# Imports for Transformers\n",
        "from transformers import AutoTokenizer  # Datasets\n",
        "from datasets import Dataset, DatasetDict\n",
        "from utils import Report\n",
        "import numpy as np\n",
        "import evaluate\n",
        "from transformers import DataCollatorForLanguageModeling, Trainer, TrainingArguments, AutoModelForCausalLM          # imports for causal Learning\n",
        "from transformers import DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer, AutoModelForSeq2SeqLM    # imports for Seq2Seq models\n",
        "from peft import LoraConfig, TaskType, LoftQConfig, PeftModelForSeq2SeqLM, PeftModelForCausalLM, get_peft_model     # imports for quantization methods (LoRA etc...)\n",
        "from transformers import EarlyStoppingCallback\n",
        "from datasets.features import Value, Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8kgPZC0J6Ak"
      },
      "source": [
        "## Fine-Tuned Models\n",
        "\n",
        "* google/mt5-large (Machine Translation)\n",
        "* sapienzanlp/Minerva-3B-base-v1.0 🇮🇹 (LLM)\n",
        "* mistralai/Mistral-7B-v0.3 (LLM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "QSAlloAIJ6Am"
      },
      "outputs": [],
      "source": [
        "device = ('cuda' if torch.cuda.is_available() else \"cpu\")\n",
        "DATASET_TRAIN = \"train_dataset_ann.csv\"\n",
        "DATASET_TEST = \"test_dataset_ann.csv\"\n",
        "SRC_L = \"Sentence\"\n",
        "TRG_L = \"Target\"\n",
        "network = \"mistralai/Mistral-7B-v0.3\"\n",
        "tokenization_method = \"mistral_base\"\n",
        "OUT_DIR = network.split(\"/\")[-1]\n",
        "EPOCHS = 43\n",
        "BATCH_SIZE = 8\n",
        "max_length = 180"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8ivbkfnJ6Aq"
      },
      "source": [
        "## Model Selection\n",
        "\n",
        "Configure Pipeline to select models for fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "c802734e29674453b4553504f77407da"
          ]
        },
        "id": "zw802PcHJ6Ar",
        "outputId": "b7bf3d15-5c77-4c74-ada1-5a78fc0b0e3a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4f869a18ad494050b27f30120a868f42",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 3,407,872 || all params: 7,251,431,424 || trainable%: 0.0470\n"
          ]
        }
      ],
      "source": [
        "# Switch to select the network and load the appropriate model and tokenizer\n",
        "\n",
        "match network:\n",
        "    case \"mistralai/Mistral-7B-v0.3\":\n",
        "        EPOCHS = 8\n",
        "        BATCH_SIZE = 4\n",
        "        from transformers import BitsAndBytesConfig\n",
        "        bnb_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_compute_dtype=torch.bfloat16,  # or torch.float16 if bfloat16 not supported\n",
        "        )\n",
        "\n",
        "        tokenizer = AutoTokenizer.from_pretrained(network)\n",
        "        if tokenizer.pad_token is None:\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "        model = AutoModelForCausalLM.from_pretrained(network, device_map=\"auto\", quantization_config=bnb_config)\n",
        "        data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        "\n",
        "        lora_config = LoraConfig(\n",
        "            task_type=TaskType.CAUSAL_LM,\n",
        "            inference_mode=False,\n",
        "            r=8,\n",
        "            lora_alpha=8*4,\n",
        "            lora_dropout=0.30\n",
        "            )\n",
        "\n",
        "        qlora_config = LoraConfig(\n",
        "            init_lora_weights=\"loftq\",\n",
        "            loftq_config=LoftQConfig(loftq_bits=4),\n",
        "            r=8,\n",
        "            lora_alpha=16,\n",
        "            target_modules=\"all-linear\",\n",
        "            lora_dropout=0.5,\n",
        "            bias=\"none\",\n",
        "            task_type=\"CAUSAL_LM\"\n",
        "        )\n",
        "\n",
        "        model = get_peft_model(model, lora_config)\n",
        "        model.print_trainable_parameters()\n",
        "\n",
        "        early_stopping_patience = 3\n",
        "        early_stopping_threshold = 0.01\n",
        "\n",
        "        early_callback = EarlyStoppingCallback(\n",
        "            early_stopping_patience=early_stopping_patience, # If validation loss does not improve for 3 consecutive epochs\n",
        "            early_stopping_threshold=early_stopping_threshold # Ignores improvements smaller than 0.001\n",
        "        )\n",
        "\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=OUT_DIR,\n",
        "            learning_rate=5e-5,\n",
        "            weight_decay=1e-1,\n",
        "            warmup_steps=100,\n",
        "            per_device_train_batch_size=BATCH_SIZE,\n",
        "            per_device_eval_batch_size=BATCH_SIZE,\n",
        "            num_train_epochs=EPOCHS,\n",
        "            eval_strategy=\"epoch\",\n",
        "            save_strategy=\"epoch\",\n",
        "            logging_strategy=\"epoch\",\n",
        "            load_best_model_at_end=True,\n",
        "            report_to=\"none\",\n",
        "            save_total_limit=3,\n",
        "            lr_scheduler_type=\"cosine\",\n",
        "            logging_dir=OUT_DIR,\n",
        "            logging_steps=10,\n",
        "            label_names=['labels'],\n",
        "            metric_for_best_model=\"eval_loss\",\n",
        "            greater_is_better=False,\n",
        "        )\n",
        "\n",
        "        params = {\n",
        "            #\"max_new_tokens\": max_length, # max number of new tokens to generate\n",
        "            #\"do_sample\":True,      # enables sampling for more diverse outputs\n",
        "            #\"top_k\":100,            # diversity increase by controlling the candidate words\n",
        "            #\"top_p\":0.95,          # nucleus sampling for further control over variety\n",
        "            #\"temperature\":1.0,     # reduces randomness and increases coherence\n",
        "            #\"repetition_penalty\":1.0,  # penalizes repetitions\n",
        "            #\"num_return_sequences\":10,  # number of generated responses\n",
        "            \"pad_token_id\":tokenizer.eos_token_id  # avoids warning if padding token is missing\n",
        "        }\n",
        "\n",
        "    case \"sapienzanlp/Minerva-3B-base-v1.0\":\n",
        "        from transformers import BitsAndBytesConfig\n",
        "        bnb_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_compute_dtype=torch.bfloat16, \n",
        "        )\n",
        "\n",
        "        tokenizer = AutoTokenizer.from_pretrained(network)\n",
        "        if tokenizer.pad_token is None:\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "        model = AutoModelForCausalLM.from_pretrained(network, device_map=\"auto\", quantization_config=bnb_config)\n",
        "        data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        "\n",
        "        lora_config = LoraConfig(\n",
        "            task_type=TaskType.CAUSAL_LM,\n",
        "            inference_mode=False,\n",
        "            r=128,\n",
        "            lora_alpha=128,\n",
        "            lora_dropout=0.50\n",
        "            )\n",
        "\n",
        "        qlora_config = LoraConfig(\n",
        "            init_lora_weights=\"loftq\",\n",
        "            loftq_config=LoftQConfig(loftq_bits=4),\n",
        "            r=8,\n",
        "            lora_alpha=16,\n",
        "            target_modules=\"all-linear\",\n",
        "            lora_dropout=0.5,\n",
        "            bias=\"none\",\n",
        "            task_type=\"CAUSAL_LM\"\n",
        "        )\n",
        "\n",
        "        model = get_peft_model(model, lora_config)\n",
        "        model.print_trainable_parameters()\n",
        "\n",
        "        early_stopping_patience = 10\n",
        "        early_stopping_threshold = 0.01\n",
        "\n",
        "        early_callback = EarlyStoppingCallback(\n",
        "            early_stopping_patience=early_stopping_patience, \n",
        "            early_stopping_threshold=early_stopping_threshold \n",
        "        )\n",
        "\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=OUT_DIR,\n",
        "            learning_rate=3e-5,\n",
        "            weight_decay=1e-2,\n",
        "            warmup_steps=500,\n",
        "            per_device_train_batch_size=BATCH_SIZE,\n",
        "            per_device_eval_batch_size=BATCH_SIZE,\n",
        "            num_train_epochs=EPOCHS,\n",
        "            eval_strategy=\"epoch\",\n",
        "            save_strategy=\"epoch\",\n",
        "            logging_strategy=\"epoch\",\n",
        "            load_best_model_at_end=True,\n",
        "            report_to=\"none\",\n",
        "            save_total_limit=3,\n",
        "            lr_scheduler_type=\"cosine\",\n",
        "            logging_dir=OUT_DIR,\n",
        "            logging_steps=10,\n",
        "            label_names=['labels'],\n",
        "            metric_for_best_model=\"eval_chrf++\",\n",
        "            greater_is_better=True,\n",
        "        )\n",
        "\n",
        "        params = {\n",
        "            #\"max_new_tokens\": max_length, \n",
        "            #\"do_sample\":True,      \n",
        "            #\"top_k\":100,            \n",
        "            #\"top_p\":0.95,         \n",
        "            #\"temperature\":1.0,    \n",
        "            #\"repetition_penalty\":1.0, \n",
        "            #\"num_return_sequences\":10,  \n",
        "            \"pad_token_id\":tokenizer.eos_token_id  \n",
        "        }\n",
        "        \n",
        "    case \"google/mt5-base\" | \"google/mt5-large\":\n",
        "        tokenizer = AutoTokenizer.from_pretrained(network)\n",
        "        if tokenizer.pad_token is None:\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "        model = AutoModelForSeq2SeqLM.from_pretrained(network, device_map=device, torch_dtype=torch.float32)\n",
        "        data_collator = DataCollatorForSeq2Seq(tokenizer, network)\n",
        "\n",
        "        lora_config = LoraConfig(\n",
        "            task_type=TaskType.SEQ_2_SEQ_LM,\n",
        "            inference_mode=False,\n",
        "            r=64,\n",
        "            lora_alpha=64,\n",
        "            lora_dropout=0.3\n",
        "            )\n",
        "\n",
        "        qlora_config = LoraConfig(\n",
        "            init_lora_weights=\"loftq\",\n",
        "            loftq_config=LoftQConfig(loftq_bits=4),\n",
        "            r=128,\n",
        "            lora_alpha=128*2,\n",
        "            target_modules=\"all-linear\",\n",
        "            lora_dropout=0.3,\n",
        "            bias=\"none\",\n",
        "            task_type=TaskType.SEQ_2_SEQ_LM\n",
        "        )\n",
        "\n",
        "        model = get_peft_model(model, lora_config)\n",
        "        model.print_trainable_parameters()\n",
        "\n",
        "        early_stopping_patience = 3\n",
        "        early_stopping_threshold = 0.01\n",
        "\n",
        "        early_callback = EarlyStoppingCallback(\n",
        "            early_stopping_patience=early_stopping_patience, \n",
        "            early_stopping_threshold=early_stopping_threshold \n",
        "        )\n",
        "\n",
        "        training_args = Seq2SeqTrainingArguments(\n",
        "            output_dir=OUT_DIR,\n",
        "            learning_rate=4e-4,\n",
        "            weight_decay=3e-4,\n",
        "            warmup_steps=500,\n",
        "            per_device_train_batch_size=BATCH_SIZE,\n",
        "            per_device_eval_batch_size=BATCH_SIZE,\n",
        "            num_train_epochs=EPOCHS,\n",
        "            eval_strategy=\"epoch\",\n",
        "            save_strategy=\"epoch\",\n",
        "            logging_strategy=\"epoch\",\n",
        "            load_best_model_at_end=True,\n",
        "            report_to=\"none\",\n",
        "            save_total_limit=3,\n",
        "            lr_scheduler_type=\"linear\",\n",
        "            logging_dir=OUT_DIR,\n",
        "            logging_steps=10,\n",
        "            label_names=['labels'],\n",
        "            metric_for_best_model=\"eval_chrf++\",\n",
        "            greater_is_better=True,\n",
        "        )\n",
        "\n",
        "        params = {\n",
        "            \"max_new_tokens\": max_length, \n",
        "            \"do_sample\":True,     \n",
        "            #\"top_k\":100,            \n",
        "            #\"top_p\":0.95,          \n",
        "            #\"temperature\":1.0,   \n",
        "            #\"repetition_penalty\":1.0,\n",
        "            #\"num_return_sequences\":10, \n",
        "            \"pad_token_id\":tokenizer.eos_token_id  \n",
        "        }\n",
        "\n",
        "    case _:\n",
        "        raise Exception(f\"Rete {network} non testabile\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "xFz9vXsgJ6As"
      },
      "outputs": [],
      "source": [
        "class MyTrainerSeq2Seq(Seq2SeqTrainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
        "        if 'num_items_in_batch' in inputs:\n",
        "            inputs = {k: v for k, v in inputs.items() if k != 'num_items_in_batch'}\n",
        "        return super().compute_loss(model, inputs, return_outputs=return_outputs)\n",
        "\n",
        "class MyTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
        "        if 'num_items_in_batch' in inputs:\n",
        "            inputs = {k: v for k, v in inputs.items() if k != 'num_items_in_batch'}\n",
        "        return super().compute_loss(model, inputs, return_outputs=return_outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "iPt0wW7VJ6At",
        "outputId": "714c0a63-d816-4373-dac9-bbb95acfb8b5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /home/andrea/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /home/andrea/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /home/andrea/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "sacrebleu_metric = evaluate.load(\"sacrebleu\")\n",
        "rouge_metric = evaluate.load(\"rouge\")\n",
        "meteor_metric = evaluate.load(\"meteor\")\n",
        "chrf_metric = evaluate.load(\"chrf\")\n",
        "ter_metric = evaluate.load(\"ter\")\n",
        "\n",
        "\n",
        "def postprocess_text(preds, labels):\n",
        "    preds = [pred.strip() for pred in preds]\n",
        "    labels = [[label.strip()] for label in labels] # Specific format for SacreBLEU\n",
        "    return preds, labels\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    preds_input, label_ids = eval_preds\n",
        "\n",
        "    # Dealing with logits or token IDs for predictions\n",
        "    # If preds_input are logits (es. direct output of training modello)\n",
        "    current_preds = preds_input\n",
        "    if isinstance(current_preds, tuple): # Common in HF Trainer, es. (logits, hidden_states)\n",
        "        current_preds = current_preds[0]\n",
        "\n",
        "    if hasattr(current_preds, \"ndim\") and current_preds.ndim == 3: # Array of logits (batch_size, seq_len, vocab_size)\n",
        "        current_preds_ids = np.argmax(current_preds, axis=-1)\n",
        "    else: # Otherwise, assumed to be token ID (batch_size, seq_len)\n",
        "        current_preds_ids = current_preds\n",
        "\n",
        "    # Decode predictions and labels\n",
        "    decoded_preds_raw = tokenizer.batch_decode(current_preds_ids, skip_special_tokens=True)\n",
        "\n",
        "    # Replace -100 in labels (common for token to be ignored) with pad_token_id for decoding\n",
        "    processed_label_ids = np.where(label_ids != -100, label_ids, tokenizer.pad_token_id)\n",
        "    decoded_labels_raw = tokenizer.batch_decode(processed_label_ids, skip_special_tokens=True)\n",
        "\n",
        "    processed_preds, processed_labels_for_sacrebleu = postprocess_text(decoded_preds_raw, decoded_labels_raw)\n",
        "\n",
        "    # For other metrics (ROUGE, METEOR, CHRF, TER), usually expects a flat list of reference strings\n",
        "    flat_references = [ref[0] for ref in processed_labels_for_sacrebleu]\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    # 1. SacreBLEU\n",
        "    sacrebleu_output = sacrebleu_metric.compute(predictions=processed_preds, references=processed_labels_for_sacrebleu)\n",
        "    if sacrebleu_output and \"score\" in sacrebleu_output:\n",
        "        results[\"bleu\"] = sacrebleu_output[\"score\"]\n",
        "    else:\n",
        "        results[\"bleu\"] = 0.0 # Fallback\n",
        "\n",
        "    # 2. ROUGE (rouge1, rouge2, rougeL, rougeLsum)\n",
        "    rouge_output = rouge_metric.compute(predictions=processed_preds, references=flat_references, use_stemmer=True)\n",
        "    if rouge_output:\n",
        "        results[\"rouge1\"] = rouge_output.get(\"rouge1\", 0.0)\n",
        "        results[\"rouge2\"] = rouge_output.get(\"rouge2\", 0.0)\n",
        "        results[\"rougeL\"] = rouge_output.get(\"rougeL\", 0.0)\n",
        "        results[\"rougeLsum\"] = rouge_output.get(\"rougeLsum\", 0.0) # Spesso più robusto per sommario\n",
        "    else:\n",
        "        results.update({\"rouge1\": 0.0, \"rouge2\": 0.0, \"rougeL\": 0.0, \"rougeLsum\": 0.0})\n",
        "\n",
        "    # 3. METEOR\n",
        "    meteor_output = meteor_metric.compute(predictions=processed_preds, references=flat_references)\n",
        "    if meteor_output and \"meteor\" in meteor_output:\n",
        "        results[\"meteor\"] = meteor_output[\"meteor\"]\n",
        "    else:\n",
        "        results[\"meteor\"] = 0.0\n",
        "\n",
        "    # 4. CHRF++ (CHRF with n-grams of words)\n",
        "    # For CHRF++, word_order (or word_n) is > 0. Default of evaluate.load('chrf') are word_order=0 (CHRF standard).\n",
        "    # Common parameters for CHRF++: word_order=2, beta=2 (beta=2 default)\n",
        "    chrf_output = chrf_metric.compute(predictions=processed_preds, references=flat_references, word_order=2, beta=2)\n",
        "    if chrf_output and \"score\" in chrf_output:\n",
        "        results[\"chrf++\"] = chrf_output[\"score\"] # CHRF++ score\n",
        "    else:\n",
        "        results[\"chrf++\"] = 0.0\n",
        "\n",
        "    # (Optional) CHRF standard (only characters)\n",
        "    # chrf_std_output = chrf_metric.compute(predictions=processed_preds, references=flat_references, word_order=0)\n",
        "    # if chrf_std_output and \"score\" in chrf_std_output:\n",
        "    #     results[\"chrf\"] = chrf_std_output[\"score\"]\n",
        "    # else:\n",
        "    #     results[\"chrf\"] = 0.0\n",
        "\n",
        "    # 5. TER (Translation Edit Rate) - the smaller, the better\n",
        "    ter_output = ter_metric.compute(predictions=processed_preds, references=flat_references)\n",
        "    if ter_output and \"score\" in ter_output:\n",
        "        results[\"ter\"] = ter_output[\"score\"]\n",
        "    else:\n",
        "        results[\"ter\"] = 1.0 # Fallback on worst score TER possible\n",
        "\n",
        "    # Mean length of generated predictions (excluding padding tokens)\n",
        "    # 'current_preds_ids' are ID token of the predictions\n",
        "    prediction_lengths = [np.count_nonzero(pid_seq != tokenizer.pad_token_id) for pid_seq in current_preds_ids]\n",
        "    results[\"gen_len\"] = np.mean(prediction_lengths) if prediction_lengths else 0.0\n",
        "\n",
        "    # Rounding of all numerical results\n",
        "    final_results = {k: round(v, 4) for k, v in results.items() if isinstance(v, (int, float))}\n",
        "\n",
        "    return final_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Ui4ZU02DJ6Au"
      },
      "outputs": [],
      "source": [
        "hf_train = Dataset.from_csv(DATASET_TRAIN, features=\n",
        "    Features({\n",
        "        SRC_L : Value(\"string\"),\n",
        "        TRG_L : Value(\"string\"),\n",
        "        \"Date\": Value(\"string\"),\n",
        "        \"Author\":Value(\"string\"),\n",
        "        \"Region\":Value(\"string\")\n",
        "    }),\n",
        "    split=\"train\"\n",
        "    ).shuffle(2025)\n",
        "\n",
        "hf_test = Dataset.from_csv(DATASET_TEST, features=\n",
        "    Features({\n",
        "        SRC_L : Value(\"string\"),\n",
        "        TRG_L : Value(\"string\"),\n",
        "        \"Date\": Value(\"string\"),\n",
        "        \"Author\":Value(\"string\"),\n",
        "        \"Region\":Value(\"string\")\n",
        "    }),\n",
        "    split=\"test\"\n",
        "    ).shuffle(2025)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCtZMnYMJ6Au"
      },
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "rtW9sFCaJ6Av",
        "outputId": "5856bc07-74c2-4176-adbc-861353c16240"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model max length: 180\n"
          ]
        }
      ],
      "source": [
        "print(f\"model max length: {max_length}\")\n",
        "\n",
        "def noprompt_it_it(examples):\n",
        "    inputs = [example for example in examples[SRC_L]]\n",
        "    targets = [example for example in examples[TRG_L]]\n",
        "\n",
        "    model_inputs = tokenizer(inputs, text_target=targets, max_length=max_length, truncation=True, padding=\"max_length\")\n",
        "    return model_inputs\n",
        "\n",
        "# I. Rewrite\n",
        "# II. Translate\n",
        "# III. Correct\n",
        "\n",
        "def minerva_base_prompt_it_it_train(examples):\n",
        "\n",
        "    prompts = [\n",
        "        f\"\"\"riscrivi la seguente frase '{src}' scritta in italiano arcaico in Italiano moderno: {dst}\"\"\"\n",
        "        for src, dst, dat, dia in zip(examples[SRC_L], examples[TRG_L], examples[\"Date\"], examples[\"Region\"])\n",
        "    ]\n",
        "\n",
        "    # Tokenizes input+target and creates label with same tokens\n",
        "    model_inputs = tokenizer(\n",
        "        prompts,\n",
        "        max_length=max_length,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\"\n",
        "    )\n",
        "\n",
        "    model_inputs[\"labels\"] = [\n",
        "        [(label if label != tokenizer.pad_token_id else -100) for label in input_ids]\n",
        "        for input_ids in model_inputs[\"input_ids\"]\n",
        "    ]\n",
        "    return model_inputs\n",
        "\n",
        "def minerva_base_prompt_it_it_eval(examples):\n",
        "    prompts = [\n",
        "\n",
        "        f\"\"\"riscrivi la seguente frase '{src}' scritta in italiano arcaico in Italiano moderno: \"\"\"\n",
        "        for src in examples[SRC_L]\n",
        "    ]\n",
        "\n",
        "    # Tokenizes input+target and creates label with same tokens\n",
        "    model_inputs = tokenizer(\n",
        "        prompts,\n",
        "        max_length=max_length,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\"\n",
        "    )\n",
        "\n",
        "def mistral_base_prompt_it_it_train(examples):\n",
        "\n",
        "    prompts = [\n",
        "        f\"\"\"###Contesto\n",
        "        Sei un linguista specializzato in lingua Italiana. Il tuo compito è fornire in modo conciso più versioni in Italiano moderno di una frase scritta in Italiano arcaico.\n",
        "        Attieniti strettamente al tuo compito.\n",
        "        \n",
        "        ###frase in Italiano Arcaico\n",
        "        {src}\n",
        "        \n",
        "        ###Risposta\n",
        "        1. \"{dst}\"\n",
        "        2. \n",
        "        \"\"\"\n",
        "        \n",
        "        for src, dst, dat, dia in zip(examples[SRC_L], examples[TRG_L], examples[\"Date\"], examples[\"Region\"])\n",
        "    ]\n",
        "\n",
        "    # Tokenizes input+target and creates label with same tokens\n",
        "    model_inputs = tokenizer(\n",
        "        prompts,\n",
        "        max_length=max_length,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\"\n",
        "    )\n",
        "\n",
        "    model_inputs[\"labels\"] = [\n",
        "        [(label if label != tokenizer.pad_token_id else -100) for label in input_ids]\n",
        "        for input_ids in model_inputs[\"input_ids\"]\n",
        "    ]\n",
        "    return model_inputs\n",
        "\n",
        "def mistral_base_prompt_it_it_eval(examples):\n",
        "    prompts = [\n",
        "\n",
        "        f\"\"\"###Contesto\n",
        "        Sei un linguista specializzato in lingua Italiana. Il tuo compito è fornire in modo conciso più versioni in Italiano moderno di una frase scritta in Italiano arcaico.\n",
        "        Attieniti strettamente al tuo compito.\n",
        "        \n",
        "        ###frase in Italiano Arcaico\n",
        "        {src} \n",
        "        \"\"\"\n",
        "        for src in examples[SRC_L]\n",
        "    ]\n",
        "\n",
        "    # Tokenizes input+target and creates label with same tokens\n",
        "    model_inputs = tokenizer(\n",
        "        prompts,\n",
        "        max_length=max_length,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\"\n",
        "    )\n",
        "\n",
        "    return model_inputs\n",
        "\n",
        "def base_prompt_en(examples):\n",
        "    inputs = [\"translate from Ancient Italian to Modern Italian: \" + example for example in examples[SRC_L]]\n",
        "\n",
        "    # Tokenizes only inputs\n",
        "    model_inputs = tokenizer(inputs, max_length=max_length, truncation=True, padding=\"max_length\")\n",
        "    targets = [example for example in examples[TRG_L]]\n",
        "\n",
        "    model_inputs = tokenizer(inputs, text_target=targets, max_length=max_length, truncation=True, padding=\"longest\")\n",
        "\n",
        "    return model_inputs\n",
        "\n",
        "def base_prompt_it_it(examples):\n",
        "    inputs = [\"Riscrivi dall'Italiano Antico a l'Italiano Moderno: \" + example for example in examples[SRC_L]]\n",
        "\n",
        "    # Tokenizes only inputs\n",
        "    model_inputs = tokenizer(inputs, max_length=max_length, truncation=True, padding=\"max_length\")\n",
        "    targets = [example for example in examples[TRG_L]]\n",
        "\n",
        "    model_inputs = tokenizer(inputs, text_target=targets, max_length=max_length, truncation=True, padding=\"longest\")\n",
        "\n",
        "    return model_inputs\n",
        "\n",
        "def parafrasi_prompt_it_it(examples):\n",
        "    inputs = [\"Scrivi la parafrasi di questo testo: \" + example for example in examples[SRC_L]]\n",
        "    targets = [example for example in examples[TRG_L]]\n",
        "\n",
        "    model_inputs = tokenizer(inputs, text_target=targets, max_length=max_length, truncation=True, padding=\"longest\")\n",
        "    return model_inputs\n",
        "\n",
        "def informative_prompt_it_it(examples):\n",
        "    inputs = [f\"Riscrivi in uno stile più moderno il testo del seguente Autore: '{author}', anno di scrittura: {date}, luogo: Italia, dialetto: '{region}', testo: '{text}'.\" for text, date, region, author in zip(examples[SRC_L], examples[\"Date\"], examples[\"Region\"], examples[\"Author\"]) ]\n",
        "    targets = [example for example in examples[TRG_L]]\n",
        "\n",
        "    model_inputs = tokenizer(inputs, text_target=targets, max_length=max_length, truncation=True, padding=\"longest\")\n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BFXkm0YJ6Av"
      },
      "source": [
        "## Tokenizer Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "e439f25244e041b8b00b62a4e79edd36",
            "86c58244034945728ce237f1d28e215d"
          ]
        },
        "id": "luEQkXHIJ6Aw",
        "outputId": "297d2697-d874-441c-85ef-bf3f45f1cfde"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f5d8e00086ee44759d222ae90ce011db",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/97 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dd10d9c811fc4ecf95b351273de06e62",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "match tokenization_method:\n",
        "    \n",
        "    case \"minerva_base\":\n",
        "        map_callback_train = minerva_base_prompt_it_it_train\n",
        "        map_callback_eval   = minerva_base_prompt_it_it_eval\n",
        "\n",
        "        hf_tokenized = DatasetDict({\n",
        "            \"train\": hf_train.map(map_callback_train, batched=True),\n",
        "            \"test\":  hf_test.map(map_callback_eval, batched=True)\n",
        "        })\n",
        "\n",
        "        hf_tokenized.set_format(type=\"torch\", columns=['input_ids', 'attention_mask'])\n",
        "    case \"mistral_base\":\n",
        "        map_callback_train =  mistral_base_prompt_it_it_train\n",
        "        map_callback_eval   = mistral_base_prompt_it_it_eval\n",
        "\n",
        "        hf_tokenized = DatasetDict({\n",
        "            \"train\": hf_train.map(map_callback_train, batched=True),\n",
        "            \"test\":  hf_test.map(map_callback_eval, batched=True)\n",
        "        })\n",
        "\n",
        "        hf_tokenized.set_format(type=\"torch\", columns=['input_ids', 'attention_mask'])\n",
        "\n",
        "    case \"base_prompt_en\":\n",
        "        map_callback = base_prompt_en\n",
        "        hf_tokenized = DatasetDict({\n",
        "            \"train\": hf_train.map(base_prompt_en, batched=True),\n",
        "            \"test\":  hf_test.map(base_prompt_en, batched=True)\n",
        "        })\n",
        "\n",
        "    case \"base_prompt_it_it\":\n",
        "        map_callback = base_prompt_it_it\n",
        "        hf_tokenized = DatasetDict({\n",
        "            \"train\": hf_train.map(base_prompt_it_it, batched=True),\n",
        "            \"test\":  hf_test.map(base_prompt_it_it, batched=True)\n",
        "        })\n",
        "        \n",
        "    case _:\n",
        "        raise ValueError(\"Tokenization method not avaiable\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "X8_EGew_J6Aw",
        "outputId": "c5da6260-613d-4cb6-e04f-c3b475fa36b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'train': ['Sentence', 'Target', 'Date', 'Author', 'Region', 'input_ids', 'attention_mask', 'labels'], 'test': ['Sentence', 'Target', 'Date', 'Author', 'Region', 'input_ids', 'attention_mask']}\n",
            "{'train': (97, 8), 'test': (10, 7)}\n"
          ]
        }
      ],
      "source": [
        "print(hf_tokenized.column_names)\n",
        "print(hf_tokenized.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "eC3T3SlkJ6Aw",
        "outputId": "32aa2760-ca3b-4bb7-82ec-a67f56d5d680"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===:(sentence n°1):===\n",
            "Sentence:###Contesto\n",
            "        Sei un linguista specializzato in lingua Italiana. Il tuo compito è fornire in modo conciso più versioni in Italiano moderno di una frase scritta in Italiano arcaico.\n",
            "        Attieniti strettamente al tuo compito.\n",
            "\n",
            "        ###frase in Italiano Arcaico\n",
            "        et nonn ebbe in fastido Cristo cotali parole d'udire.\n",
            "\n",
            "        ###Risposta\n",
            "        1. \"E Cristo non provò fastidio ad ascoltare parole di quel genere.\"\n",
            "        2. \n",
            "        \n",
            "===:(sentence n°2):===\n",
            "Sentence:###Contesto\n",
            "        Sei un linguista specializzato in lingua Italiana. Il tuo compito è fornire in modo conciso più versioni in Italiano moderno di una frase scritta in Italiano arcaico.\n",
            "        Attieniti strettamente al tuo compito.\n",
            "\n",
            "        ###frase in Italiano Arcaico\n",
            "        e, cosa che anche è più grave, cioè essere preso, o vero fuggire, e suo Comune lasciare vincere.\n",
            "\n",
            "        ###Risposta\n",
            "        1. \"E, cosa ancor più grave che essere catturato, fuggire e lasciare che il proprio Comune venga sconfitto.\"\n",
            "        2. \n",
            "        \n",
            "===:(sentence n°3):===\n",
            "Sentence:###Contesto\n",
            "        Sei un linguista specializzato in lingua Italiana. Il tuo compito è fornire in modo conciso più versioni in Italiano moderno di una frase scritta in Italiano arcaico.\n",
            "        Attieniti strettamente al tuo compito.\n",
            "\n",
            "        ###frase in Italiano Arcaico\n",
            "        Se questo piace a tutti e se 'l tempo hae bisogno d'avere Pompeio per cavaliere e non per compagno, non riterrò più i fati.\n",
            "\n",
            "        ###Risposta\n",
            "        1. \"Se questo è quello che tutti desiderano e se l'attuale situazione richiede che Pompeo sia un leader e non solo un alleato, allora non cercherò\n",
            "===:(sentence n°4):===\n",
            "Sentence:###Contesto\n",
            "        Sei un linguista specializzato in lingua Italiana. Il tuo compito è fornire in modo conciso più versioni in Italiano moderno di una frase scritta in Italiano arcaico.\n",
            "        Attieniti strettamente al tuo compito.\n",
            "\n",
            "        ###frase in Italiano Arcaico\n",
            "        la moltitudine de' quali tu ài potuto vedere e riguardare lo studio e poco dinanzi udire le voci, e lle cui mani e lance apena posso ritenere.\n",
            "\n",
            "        ###Risposta\n",
            "        1. \"La moltitudine dei quali hai potuto osservare l’impegno e poco fa udirne le voci, e le cui man\n",
            "===:(sentence n°5):===\n",
            "Sentence:###Contesto\n",
            "        Sei un linguista specializzato in lingua Italiana. Il tuo compito è fornire in modo conciso più versioni in Italiano moderno di una frase scritta in Italiano arcaico.\n",
            "        Attieniti strettamente al tuo compito.\n",
            "\n",
            "        ###frase in Italiano Arcaico\n",
            "        Ma no sapeano già le nomora di coloro dela congiurazione, ché la donna no nominava già li nomi.\n",
            "\n",
            "        ###Risposta\n",
            "        1. \"Ma non conoscevano ancora i nomi di quelli della congiura, perché la donna non li rivelava.\"\n",
            "        2. \n",
            "        \n"
          ]
        }
      ],
      "source": [
        "for idx, s in enumerate(hf_tokenized[\"train\"].take(5), 1):\n",
        "    print(f\"===:(sentence n°{idx}):===\")\n",
        "    print(f\"{SRC_L}:{tokenizer.decode(s['input_ids'], attention_mask=s['attention_mask'], skip_special_tokens=True)}\" )\n",
        "    #print(f\"{TRG_L}:{tokenizer.decode(s[\"labels\"], skip_special_tokens=True)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "WB1tROKuJ6Ax",
        "outputId": "82cde9f8-a3c9-454a-ed9b-a6e497b7c4dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===:(sentence n°1):===\n",
            "Sentence:###Contesto\n",
            "        Sei un linguista specializzato in lingua Italiana. Il tuo compito è fornire in modo conciso più versioni in Italiano moderno di una frase scritta in Italiano arcaico.\n",
            "        Attieniti strettamente al tuo compito.\n",
            "\n",
            "        ###frase in Italiano Arcaico\n",
            "        Nella quale battaglia, certo io ebbi sempre ardire di ragionare di pace e sempre mi dolfi che non solamente la pace era schifata, \n",
            "        \n",
            "===:(sentence n°2):===\n",
            "Sentence:###Contesto\n",
            "        Sei un linguista specializzato in lingua Italiana. Il tuo compito è fornire in modo conciso più versioni in Italiano moderno di una frase scritta in Italiano arcaico.\n",
            "        Attieniti strettamente al tuo compito.\n",
            "\n",
            "        ###frase in Italiano Arcaico\n",
            "        sono due già non in una carne, ma in uno spirito, cioè Iddio, e l' anima. Onde in altro luogo dice S. Paolo: Chi s' accosta a Dio è uno spirito \n",
            "        \n",
            "===:(sentence n°3):===\n",
            "Sentence:###Contesto\n",
            "        Sei un linguista specializzato in lingua Italiana. Il tuo compito è fornire in modo conciso più versioni in Italiano moderno di una frase scritta in Italiano arcaico.\n",
            "        Attieniti strettamente al tuo compito.\n",
            "\n",
            "        ###frase in Italiano Arcaico\n",
            "        Altressì uno amante chiamando merzé alla sua donna dice parole e ragioni molte, et ella si difende in suo dire. \n",
            "        \n",
            "===:(sentence n°4):===\n",
            "Sentence:###Contesto\n",
            "        Sei un linguista specializzato in lingua Italiana. Il tuo compito è fornire in modo conciso più versioni in Italiano moderno di una frase scritta in Italiano arcaico.\n",
            "        Attieniti strettamente al tuo compito.\n",
            "\n",
            "        ###frase in Italiano Arcaico\n",
            "        Pietro, essendogli mostrato in figura il populo Gentile, sì gli fu detto: ammazza, e mangia \n",
            "        \n",
            "===:(sentence n°5):===\n",
            "Sentence:###Contesto\n",
            "        Sei un linguista specializzato in lingua Italiana. Il tuo compito è fornire in modo conciso più versioni in Italiano moderno di una frase scritta in Italiano arcaico.\n",
            "        Attieniti strettamente al tuo compito.\n",
            "\n",
            "        ###frase in Italiano Arcaico\n",
            "        A Milano fue ripressa la malvagità d' una donna in simile bugìa, nel tempo medesimo di questo signore della republica, in questo modo:  \n",
            "        \n"
          ]
        }
      ],
      "source": [
        "for idx, s in enumerate(hf_tokenized[\"test\"].take(5), 1):\n",
        "    print(f\"===:(sentence n°{idx}):===\")\n",
        "    print(f\"{SRC_L}:{tokenizer.decode(s['input_ids'], attention_mask=s['attention_mask'], skip_special_tokens=True)}\" )\n",
        "    #print(f\"{TRG_L}:{tokenizer.decode(s[\"labels\"], skip_special_tokens=True)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLsr_a4RJ6Ax"
      },
      "source": [
        "## Models & Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aNXSOX0J6Ay"
      },
      "source": [
        "### PEFT Fine-Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "KGZ5c0DcJ6Ay",
        "outputId": "67462809-6c0c-40af-a623-f77612bdd67c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CAUSAL Generation\n"
          ]
        }
      ],
      "source": [
        "if isinstance(model, PeftModelForSeq2SeqLM):\n",
        "    print(\"[SEQ2SEQ Generation]\")\n",
        "    trainer = MyTrainerSeq2Seq(\n",
        "                model=model,\n",
        "                args=training_args,\n",
        "                train_dataset=hf_tokenized[\"train\"],\n",
        "                eval_dataset=hf_tokenized[\"test\"],\n",
        "                processing_class=tokenizer,\n",
        "                data_collator=data_collator,\n",
        "                compute_metrics=compute_metrics,\n",
        "                callbacks=[Report(OUT_DIR)]\n",
        "            )\n",
        "else:\n",
        "    print(\"CAUSAL Generation\")\n",
        "    trainer = MyTrainer(\n",
        "                model=model,\n",
        "                args=training_args,\n",
        "                train_dataset=hf_tokenized[\"train\"],\n",
        "                eval_dataset=hf_tokenized[\"test\"],\n",
        "                processing_class=tokenizer,\n",
        "                data_collator=data_collator,\n",
        "                compute_metrics=compute_metrics,\n",
        "                callbacks=[Report(OUT_DIR), early_callback]\n",
        "            )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "UojACT4JJ6Ay",
        "outputId": "bda716fb-9292-48bc-f5ee-d9464f454b6f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [200/200 05:54, Epoch 8/8]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Bleu</th>\n",
              "      <th>Rouge1</th>\n",
              "      <th>Rouge2</th>\n",
              "      <th>Rougel</th>\n",
              "      <th>Rougelsum</th>\n",
              "      <th>Meteor</th>\n",
              "      <th>Chrf++</th>\n",
              "      <th>Ter</th>\n",
              "      <th>Gen Len</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.355200</td>\n",
              "      <td>2.731568</td>\n",
              "      <td>3.036700</td>\n",
              "      <td>0.215600</td>\n",
              "      <td>0.041500</td>\n",
              "      <td>0.184900</td>\n",
              "      <td>0.200600</td>\n",
              "      <td>0.277200</td>\n",
              "      <td>34.977100</td>\n",
              "      <td>82.038000</td>\n",
              "      <td>180.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.939300</td>\n",
              "      <td>1.956422</td>\n",
              "      <td>7.834800</td>\n",
              "      <td>0.452100</td>\n",
              "      <td>0.294400</td>\n",
              "      <td>0.427400</td>\n",
              "      <td>0.441200</td>\n",
              "      <td>0.401400</td>\n",
              "      <td>47.189700</td>\n",
              "      <td>111.053500</td>\n",
              "      <td>180.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.253100</td>\n",
              "      <td>1.391456</td>\n",
              "      <td>57.890600</td>\n",
              "      <td>0.702500</td>\n",
              "      <td>0.624200</td>\n",
              "      <td>0.684800</td>\n",
              "      <td>0.689300</td>\n",
              "      <td>0.662700</td>\n",
              "      <td>72.851500</td>\n",
              "      <td>37.823800</td>\n",
              "      <td>180.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.029400</td>\n",
              "      <td>1.351187</td>\n",
              "      <td>61.912500</td>\n",
              "      <td>0.699300</td>\n",
              "      <td>0.623700</td>\n",
              "      <td>0.683100</td>\n",
              "      <td>0.687300</td>\n",
              "      <td>0.657900</td>\n",
              "      <td>73.185600</td>\n",
              "      <td>36.269400</td>\n",
              "      <td>180.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.900600</td>\n",
              "      <td>1.246749</td>\n",
              "      <td>64.591800</td>\n",
              "      <td>0.722700</td>\n",
              "      <td>0.646300</td>\n",
              "      <td>0.706200</td>\n",
              "      <td>0.708700</td>\n",
              "      <td>0.698600</td>\n",
              "      <td>74.577300</td>\n",
              "      <td>34.542300</td>\n",
              "      <td>180.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.792400</td>\n",
              "      <td>1.268232</td>\n",
              "      <td>64.648800</td>\n",
              "      <td>0.713000</td>\n",
              "      <td>0.642100</td>\n",
              "      <td>0.702100</td>\n",
              "      <td>0.702800</td>\n",
              "      <td>0.694600</td>\n",
              "      <td>74.546600</td>\n",
              "      <td>34.715000</td>\n",
              "      <td>180.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.730800</td>\n",
              "      <td>1.286449</td>\n",
              "      <td>64.529800</td>\n",
              "      <td>0.714300</td>\n",
              "      <td>0.641900</td>\n",
              "      <td>0.701200</td>\n",
              "      <td>0.703900</td>\n",
              "      <td>0.699300</td>\n",
              "      <td>74.258200</td>\n",
              "      <td>34.715000</td>\n",
              "      <td>180.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.700500</td>\n",
              "      <td>1.289964</td>\n",
              "      <td>64.644200</td>\n",
              "      <td>0.711800</td>\n",
              "      <td>0.641300</td>\n",
              "      <td>0.700600</td>\n",
              "      <td>0.703000</td>\n",
              "      <td>0.695000</td>\n",
              "      <td>74.284300</td>\n",
              "      <td>34.715000</td>\n",
              "      <td>180.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training done. Generating graphs...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=200, training_loss=1.2126712131500244, metrics={'train_runtime': 356.3035, 'train_samples_per_second': 2.178, 'train_steps_per_second': 0.561, 'total_flos': 5964794454343680.0, 'train_loss': 1.2126712131500244, 'epoch': 8.0})"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "5ba49f79e3d2462baf2deaf3898bea07"
          ]
        },
        "id": "Mvm49C6vJ6Az",
        "outputId": "dbb38e13-993c-4b56-8056-d4e1557a9628"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start generation on cuda\n",
            "=============================\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8df8d078ccef4934abe0d998cda406f0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===:0-(Model for Prompt)===\n",
            "###Contesto\n",
            "        Sei un linguista specializzato in lingua Italiana. Il tuo compito è fornire in modo conciso più versioni in Italiano moderno di una frase scritta in Italiano arcaico.\n",
            "        Attieniti strettamente al tuo compito.\n",
            "\n",
            "        ###frase in Italiano Arcaico\n",
            "        Nella quale battaglia, certo io ebbi sempre ardire di ragionare di pace e sempre mi dolfi che non solamente la pace era schifata, \n",
            "        \n",
            "=========================\n",
            "===:(model mistralai/Mistral-7B-v0.3):===\n",
            "###Contesto\n",
            "        Sei un linguista specializzato in lingua Italiana. Il tuo compito è fornire in modo conciso più versioni in Italiano moderno di una frase scritta in Italiano arcaico.\n",
            "        Attieniti strettamente al tuo compito.\n",
            "\n",
            "        ###frase in Italiano Arcaico\n",
            "        Nella quale battaglia, certo io ebbi sempre ardire di ragionare di pace e sempre mi dolfi che non solamente la pace era schifata, \n",
            "        \n",
            "        ###Risposta\n",
            "        1. \"In quella battaglia, certo io ho sempre avuto coraggio di parlare di pace e sempre mi dispiace che non solo la pace fosse rifiutata, \n",
            "        2. \n",
            "        3. \n",
            "        4. \n",
            "        5. \n",
            "        6. \n",
            "        7. \n",
            "        8. \n",
            "        9. \n",
            "        10. \n",
            "        11. \n",
            "        12. \n",
            "        13. \n",
            "        14. \n",
            "        15. \n",
            "        16. \n",
            "        17. \n",
            "        18. \n",
            "        19. \n",
            "        20. \n",
            "        21. \n",
            "        22. \n",
            "        23. \n",
            "        24. \n",
            "=================================\n",
            "===:1-(Model for Prompt)===\n",
            "###Contesto\n",
            "        Sei un linguista specializzato in lingua Italiana. Il tuo compito è fornire in modo conciso più versioni in Italiano moderno di una frase scritta in Italiano arcaico.\n",
            "        Attieniti strettamente al tuo compito.\n",
            "\n",
            "        ###frase in Italiano Arcaico\n",
            "        sono due già non in una carne, ma in uno spirito, cioè Iddio, e l' anima. Onde in altro luogo dice S. Paolo: Chi s' accosta a Dio è uno spirito \n",
            "        \n",
            "=========================\n",
            "===:(model mistralai/Mistral-7B-v0.3):===\n",
            "###Contesto\n",
            "        Sei un linguista specializzato in lingua Italiana. Il tuo compito è fornire in modo conciso più versioni in Italiano moderno di una frase scritta in Italiano arcaico.\n",
            "        Attieniti strettamente al tuo compito.\n",
            "\n",
            "        ###frase in Italiano Arcaico\n",
            "        sono due già non in una carne, ma in uno spirito, cioè Iddio, e l' anima. Onde in altro luogo dice S. Paolo: Chi s' accosta a Dio è uno spirito \n",
            "        \n",
            "        ###Risposta\n",
            "        1. \"Sono due già non in una carne, ma in uno spirito, cioè Dio, e l’anima. E in altro luogo S. Paolo dice: Chi si avvicina a Dio è uno spirito.\"\n",
            "        2. \n",
            "        3. \n",
            "        4. \n",
            "        5. \n",
            "\n",
            "        ###Attenzione\n",
            "\n",
            "       \n",
            "       \n",
            "=================================\n",
            "===:2-(Model for Prompt)===\n",
            "###Contesto\n",
            "        Sei un linguista specializzato in lingua Italiana. Il tuo compito è fornire in modo conciso più versioni in Italiano moderno di una frase scritta in Italiano arcaico.\n",
            "        Attieniti strettamente al tuo compito.\n",
            "\n",
            "        ###frase in Italiano Arcaico\n",
            "        Altressì uno amante chiamando merzé alla sua donna dice parole e ragioni molte, et ella si difende in suo dire. \n",
            "        \n",
            "=========================\n",
            "===:(model mistralai/Mistral-7B-v0.3):===\n",
            "###Contesto\n",
            "        Sei un linguista specializzato in lingua Italiana. Il tuo compito è fornire in modo conciso più versioni in Italiano moderno di una frase scritta in Italiano arcaico.\n",
            "        Attieniti strettamente al tuo compito.\n",
            "\n",
            "        ###frase in Italiano Arcaico\n",
            "        Altressì uno amante chiamando merzé alla sua donna dice parole e ragioni molte, et ella si difende in suo dire. \n",
            "        \n",
            "        ###Risposta\n",
            "        1. \"Altrettanto, un amante chiamando misericordia per sé verso la sua donna, dice molte parole e ragioni, e lei si difende con le sue parole.\"\n",
            "        2. \n",
            "        3. \n",
            "        4. \n",
            "        5. \n",
            "\n",
            "        ###Attenzione\n",
            "       \n",
            "       \n",
            "        ###flag\n",
            "       \n",
            "       \n",
            "=================================\n",
            "===:3-(Model for Prompt)===\n",
            "###Contesto\n",
            "        Sei un linguista specializzato in lingua Italiana. Il tuo compito è fornire in modo conciso più versioni in Italiano moderno di una frase scritta in Italiano arcaico.\n",
            "        Attieniti strettamente al tuo compito.\n",
            "\n",
            "        ###frase in Italiano Arcaico\n",
            "        Pietro, essendogli mostrato in figura il populo Gentile, sì gli fu detto: ammazza, e mangia \n",
            "        \n",
            "=========================\n",
            "===:(model mistralai/Mistral-7B-v0.3):===\n",
            "###Contesto\n",
            "        Sei un linguista specializzato in lingua Italiana. Il tuo compito è fornire in modo conciso più versioni in Italiano moderno di una frase scritta in Italiano arcaico.\n",
            "        Attieniti strettamente al tuo compito.\n",
            "\n",
            "        ###frase in Italiano Arcaico\n",
            "        Pietro, essendogli mostrato in figura il populo Gentile, sì gli fu detto: ammazza, e mangia \n",
            "        \n",
            "        ###Risposta\n",
            "        1. \"Pietro, quando gli fu mostrato in figura il popolo gentile, gli fu detto: uccidi, e mangia.\"\n",
            "        2. \n",
            "        3. \n",
            "        4. \n",
            "        5. \n",
            "\n",
            "        ###Attenzione\n",
            "       \n",
            "       \n",
            "        ###flag\n",
            "       \n",
            "       \n",
            "=================================\n",
            "===:4-(Model for Prompt)===\n",
            "###Contesto\n",
            "        Sei un linguista specializzato in lingua Italiana. Il tuo compito è fornire in modo conciso più versioni in Italiano moderno di una frase scritta in Italiano arcaico.\n",
            "        Attieniti strettamente al tuo compito.\n",
            "\n",
            "        ###frase in Italiano Arcaico\n",
            "        A Milano fue ripressa la malvagità d' una donna in simile bugìa, nel tempo medesimo di questo signore della republica, in questo modo:  \n",
            "        \n",
            "=========================\n",
            "===:(model mistralai/Mistral-7B-v0.3):===\n",
            "###Contesto\n",
            "        Sei un linguista specializzato in lingua Italiana. Il tuo compito è fornire in modo conciso più versioni in Italiano moderno di una frase scritta in Italiano arcaico.\n",
            "        Attieniti strettamente al tuo compito.\n",
            "\n",
            "        ###frase in Italiano Arcaico\n",
            "        A Milano fue ripressa la malvagità d' una donna in simile bugìa, nel tempo medesimo di questo signore della republica, in questo modo:  \n",
            "        \n",
            "        ###Risposta\n",
            "        1. \"A Milano, una donna fu liberata dalla sua malvagità in una simile circostanza, nello stesso tempo di questo signore della repubblica, in questo modo: \"\n",
            "        2. \n",
            "        3. \n",
            "        4. \n",
            "        5. \n",
            "\n",
            "        ###Attenzione\n",
            "       \n",
            "        ###flag\n",
            "       \n",
            "       \n",
            "        ###Conseguenza\n",
            "       \n",
            "       \n",
            "       \n",
            "        ###flag\n",
            "       \n",
            "       \n",
            "       \n",
            "        ###Conseguenza\n",
            "       \n",
            "       \n",
            "       \n",
            "        ###flag\n",
            "       \n",
            "       \n",
            "       \n",
            "        ###Conseguenza\n",
            "       \n",
            "       \n",
            "       \n",
            "        ###flag\n",
            "       \n",
            "       \n",
            "       \n",
            "        ###Conseguenza\n",
            "       \n",
            "       \n",
            "       \n",
            "        ###flag\n",
            "       \n",
            "=================================\n",
            "===:5-(Model for Prompt)===\n",
            "###Contesto\n",
            "        Sei un linguista specializzato in lingua Italiana. Il tuo compito è fornire in modo conciso più versioni in Italiano moderno di una frase scritta in Italiano arcaico.\n",
            "        Attieniti strettamente al tuo compito.\n",
            "\n",
            "        ###frase in Italiano Arcaico\n",
            "        Il re entrò in uno giardino dietro al suo albergo, quasi come s'egli andasse pensando alla risposta. \n",
            "        \n",
            "=========================\n",
            "===:(model mistralai/Mistral-7B-v0.3):===\n",
            "###Contesto\n",
            "        Sei un linguista specializzato in lingua Italiana. Il tuo compito è fornire in modo conciso più versioni in Italiano moderno di una frase scritta in Italiano arcaico.\n",
            "        Attieniti strettamente al tuo compito.\n",
            "\n",
            "        ###frase in Italiano Arcaico\n",
            "        Il re entrò in uno giardino dietro al suo albergo, quasi come s'egli andasse pensando alla risposta. \n",
            "        \n",
            "        ###Risposta\n",
            "        1. \"Il re entrò in un giardino dietro l’albergo, quasi come si stesse pensando alla risposta.\"\n",
            "        2. \n",
            "        3. \n",
            "        4. \n",
            "        5. \n",
            "\n",
            "        ###Attenzione\n",
            "       \n",
            "       \n",
            "        ###frase in Italiano Arcaico\n",
            "       \n",
            "       \n",
            "        ###Risposta\n",
            "        1.\n",
            "        2.\n",
            "        3.\n",
            "        4.\n",
            "        5.\n",
            "       \n",
            "        ###Attenzione\n",
            "       \n",
            "       \n",
            "        ###frase in Italiano Arcaico\n",
            "       \n",
            "       \n",
            "        ###Risposta\n",
            "        1.\n",
            "        2.\n",
            "        3.\n",
            "        4.\n",
            "        5.\n",
            "       \n",
            "        ###Attenzione\n",
            "       \n",
            "       \n",
            "=================================\n",
            "===:6-(Model for Prompt)===\n",
            "###Contesto\n",
            "        Sei un linguista specializzato in lingua Italiana. Il tuo compito è fornire in modo conciso più versioni in Italiano moderno di una frase scritta in Italiano arcaico.\n",
            "        Attieniti strettamente al tuo compito.\n",
            "\n",
            "        ###frase in Italiano Arcaico\n",
            "        Gorgone, e ho questa proprietà che io volo per l'aire sì come uno ucello\". \n",
            "        \n",
            "=========================\n",
            "===:(model mistralai/Mistral-7B-v0.3):===\n",
            "###Contesto\n",
            "        Sei un linguista specializzato in lingua Italiana. Il tuo compito è fornire in modo conciso più versioni in Italiano moderno di una frase scritta in Italiano arcaico.\n",
            "        Attieniti strettamente al tuo compito.\n",
            "\n",
            "        ###frase in Italiano Arcaico\n",
            "        Gorgone, e ho questa proprietà che io volo per l'aire sì come uno ucello\". \n",
            "        \n",
            "        ###Risposta\n",
            "        1. \"Gorgone, e ho questa capacità di volare per l’aria come un uccello.\"\n",
            "        2. \n",
            "        3. \n",
            "        4. \n",
            "        5. \n",
            "\n",
            "        ###Contesto\n",
            "        Sei un linguista specializzato in lingua Italiana. Il tuo compito è fornire in modo conciso più versioni in Italiano moderno di una frase scritta in Italiano arcaico.\n",
            "        Attieniti strettamente al tuo compito.\n",
            "\n",
            "        ###frase in Italiano Arcaico\n",
            "        E così, perché io non mi volsi più a la caccia, ma mi volsi a la caccia di sapienza.\n",
            "        \n",
            "        ###Risposta\n",
            "        \n",
            "=================================\n",
            "===:7-(Model for Prompt)===\n",
            "###Contesto\n",
            "        Sei un linguista specializzato in lingua Italiana. Il tuo compito è fornire in modo conciso più versioni in Italiano moderno di una frase scritta in Italiano arcaico.\n",
            "        Attieniti strettamente al tuo compito.\n",
            "\n",
            "        ###frase in Italiano Arcaico\n",
            "        Ma l' occhio della intelligenza è più alto. Perciò che, passata la grandezza della universitade, quella medesima semplice forma vede nella sottil vista \n",
            "        \n",
            "=========================\n",
            "===:(model mistralai/Mistral-7B-v0.3):===\n",
            "###Contesto\n",
            "        Sei un linguista specializzato in lingua Italiana. Il tuo compito è fornire in modo conciso più versioni in Italiano moderno di una frase scritta in Italiano arcaico.\n",
            "        Attieniti strettamente al tuo compito.\n",
            "\n",
            "        ###frase in Italiano Arcaico\n",
            "        Ma l' occhio della intelligenza è più alto. Perciò che, passata la grandezza della universitade, quella medesima semplice forma vede nella sottil vista \n",
            "        \n",
            "        ###Risposta\n",
            "        1. \"Ma l’occhio dell’intelligenza è più alto. Perché, superata la grandezza dell’università, quella stessa semplice forma vede nella sottilissima visione ...\"\n",
            "        2. \n",
            "        3. \n",
            "        4. \n",
            "        5. \n",
            "\n",
            "        ###Attenzione\n",
            "       \n",
            "       \n",
            "        ###flag\n",
            "       \n",
            "       \n",
            "        ###Conseguenza\n",
            "       \n",
            "       \n",
            "        ###flag\n",
            "       \n",
            "       \n",
            "        ###Conseguenza\n",
            "       \n",
            "       \n",
            "        ###flag\n",
            "       \n",
            "       \n",
            "        ###Conseguenza\n",
            "       \n",
            "       \n",
            "        ###flag\n",
            "       \n",
            "       \n",
            "        ###Conseguenza\n",
            "       \n",
            "       \n",
            "        ###flag\n",
            "       \n",
            "       \n",
            "        ###Conse\n",
            "=================================\n",
            "===:0-(Model for Prompt)===\n",
            "###Contesto\n",
            "        Sei un linguista specializzato in lingua Italiana. Il tuo compito è fornire in modo conciso più versioni in Italiano moderno di una frase scritta in Italiano arcaico.\n",
            "        Attieniti strettamente al tuo compito.\n",
            "\n",
            "        ###frase in Italiano Arcaico\n",
            "        e l' acconciamento a fare grandissime cose, cioè a ttenere pace et amare Idio e 'l proximo, a ffare cittadi, castella e magioni \n",
            "        \n",
            "=========================\n",
            "===:(model mistralai/Mistral-7B-v0.3):===\n",
            "###Contesto\n",
            "        Sei un linguista specializzato in lingua Italiana. Il tuo compito è fornire in modo conciso più versioni in Italiano moderno di una frase scritta in Italiano arcaico.\n",
            "        Attieniti strettamente al tuo compito.\n",
            "\n",
            "        ###frase in Italiano Arcaico\n",
            "        e l' acconciamento a fare grandissime cose, cioè a ttenere pace et amare Idio e 'l proximo, a ffare cittadi, castella e magioni \n",
            "        \n",
            "        ###Risposta\n",
            "        1. \"L’acconciamento è in grado di compiere grandi cose, cioè di mantenere la pace e amare Dio e il prossimo, di costruire città, castelli e magioni.\"\n",
            "        2. \n",
            "        3. \n",
            "        4. \n",
            "        5. \n",
            "\n",
            "        ###Attenzione\n",
            "       \n",
            "       \n",
            "        ###flag\n",
            "       \n",
            "       \n",
            "        ###Informazioni Aggiuntive\n",
            "        N.B.: \n",
            "        N.B.: \n",
            "        N.B.: \n",
            "        N.B.: \n",
            "        N.B.: \n",
            "        N.B.: \n",
            "        N.B.: \n",
            "        N.B.: \n",
            "        N.B.: \n",
            "        N.B.:\n",
            "=================================\n",
            "===:1-(Model for Prompt)===\n",
            "###Contesto\n",
            "        Sei un linguista specializzato in lingua Italiana. Il tuo compito è fornire in modo conciso più versioni in Italiano moderno di una frase scritta in Italiano arcaico.\n",
            "        Attieniti strettamente al tuo compito.\n",
            "\n",
            "        ###frase in Italiano Arcaico\n",
            "        Gregorio. Non udii mai che egli avesse maestro; ma il dono dello Spirito Santo non si può stringere a legge. \n",
            "        \n",
            "=========================\n",
            "===:(model mistralai/Mistral-7B-v0.3):===\n",
            "###Contesto\n",
            "        Sei un linguista specializzato in lingua Italiana. Il tuo compito è fornire in modo conciso più versioni in Italiano moderno di una frase scritta in Italiano arcaico.\n",
            "        Attieniti strettamente al tuo compito.\n",
            "\n",
            "        ###frase in Italiano Arcaico\n",
            "        Gregorio. Non udii mai che egli avesse maestro; ma il dono dello Spirito Santo non si può stringere a legge. \n",
            "        \n",
            "        ###Risposta\n",
            "        1. \"Gregorio. Non ho mai sentito che avesse un maestro; ma il dono dello Spirito Santo non può essere limitato dalle leggi.\"\n",
            "        2. \n",
            "        3. \n",
            "        4. \n",
            "        5. \n",
            "\n",
            "        ###Contesto\n",
            "        Sei un linguista specializzato in lingua Italiana. Il tuo compito è fornire in modo conciso più versioni in Italiano moderno di una frase scritta in Italiano arcaico.\n",
            "        Attieniti strettamente al tuo compito.\n",
            "\n",
            "        ###frase in Italiano Arcaico\n",
            "        Gregorio. Non udii mai che egli avesse maestro; ma il dono dello Spirito Santo non si può\n",
            "=================================\n",
            "\n",
            "Generation completed.\n"
          ]
        }
      ],
      "source": [
        "# Sets model in evaluation mode and moves it on device\n",
        "model = model.eval()\n",
        "model = model.to(device)\n",
        "\n",
        "# Creates DataLoader\n",
        "hf_tokenized.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
        "loader = torch.utils.data.DataLoader(hf_tokenized[\"test\"], batch_size=8)\n",
        "\n",
        "print(f\"Start generation on {device}\")\n",
        "print(\"=============================\")\n",
        "\n",
        "for batch in tqdm(loader):\n",
        "    # Move the entire batch to the device\n",
        "    # Note: DataLoader returns a batch as a dictionary of tensors\n",
        "\n",
        "    batch[\"input_ids\"] = batch[\"input_ids\"].to(device)\n",
        "    batch[\"attention_mask\"] = batch[\"attention_mask\"].to(device)\n",
        "\n",
        "    # Generate output for the batch\n",
        "    # Note: The model.generate method expects input_ids and attention_mask\n",
        "    result_ids = model.generate(\n",
        "        input_ids=batch[\"input_ids\"],\n",
        "        attention_mask=batch[\"attention_mask\"],\n",
        "        max_new_tokens=max_length,\n",
        "    )\n",
        "\n",
        "    # Decodes separately each prompt and each generated result\n",
        "    # Note: result_ids has shape (batch_size, seq_len_output)\n",
        "    # Iterate on batch to decode one by one\n",
        "    # batch[\"input_ids\"] has shape (batch_size, seq_len_input)\n",
        "\n",
        "    # Decode original prompts\n",
        "    decoded_prompts = tokenizer.batch_decode(batch[\"input_ids\"], skip_special_tokens=True)\n",
        "\n",
        "    # Decode gnerated results\n",
        "    decoded_results = tokenizer.batch_decode(result_ids, skip_special_tokens=True)\n",
        "\n",
        "    # Prints results for each batch element\n",
        "    with open(OUT_DIR + \"/output_chat.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "        for i in range(len(decoded_prompts)):\n",
        "            print(f\"===:{i}-(Model for Prompt)===\")\n",
        "            print(f\"{decoded_prompts[i]}\")\n",
        "            print(\"=========================\")\n",
        "\n",
        "            print(f\"===:(model {network}):===\")\n",
        "            print(decoded_results[i])\n",
        "            print(\"=================================\")\n",
        "\n",
        "            ############################################\n",
        "\n",
        "            f.write(\"f===:({i}Model for Prompt)===\")\n",
        "            f.write(f\"{decoded_prompts[i]}\")\n",
        "            f.write(\"=========================\")\n",
        "\n",
        "            f.write(f\"===:(model {network}):===\")\n",
        "            f.write(decoded_results[i])\n",
        "            f.write(\"==========================\")\n",
        "\n",
        "print(\"\\nGeneration completed.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "MNLP",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
