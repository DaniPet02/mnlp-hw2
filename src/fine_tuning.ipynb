{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asJxkfJNJ6Ab"
      },
      "source": [
        "# Fine-tuning for Translation from Ancient to Modern Italian"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZXyfYF5KTyH"
      },
      "source": [
        "# System Setup 🖥️"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWshcyakjQV8"
      },
      "source": [
        "### Drive Interface 📁"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bn2BVRjjKKUZ",
        "outputId": "95a55d51-b204-47e2-85d1-008a0984ccf5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Local env dected\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "    %cp /content/drive/MyDrive/MNLP_HW_2/Many_Naps_Little_Progress/*.* .\n",
        "    %ls\n",
        "except ModuleNotFoundError:\n",
        "    print(\"Local env dected\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgbPZ6tqKLDD"
      },
      "source": [
        "### Additional Dependencies 🐍"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNhSDgXeKKBS",
        "outputId": "a0f02e11-0497-4e18-e78b-45ee5a4dd38f"
      },
      "outputs": [],
      "source": [
        "#!bash ../install_colab.sh >> /dev/null"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_q2wvHwFKt5F"
      },
      "source": [
        "### Hugging Face 🤗"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Ccb4L42zKuVB"
      },
      "outputs": [],
      "source": [
        "import huggingface_hub\n",
        "TOKEN = \"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\"\n",
        "huggingface_hub.login(token=TOKEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LN3PEzDWJ6Ai"
      },
      "outputs": [],
      "source": [
        "# Import Datases to work with Transformers by Hugging-Face\n",
        "import torch\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "# Imports for Transformers\n",
        "from transformers import AutoTokenizer  # Datasets\n",
        "from datasets import Dataset, DatasetDict\n",
        "from utils import Report\n",
        "import numpy as np\n",
        "import evaluate\n",
        "from transformers import DataCollatorForLanguageModeling, Trainer, TrainingArguments, AutoModelForCausalLM          # imports for causal Learning\n",
        "from transformers import DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer, AutoModelForSeq2SeqLM    # imports for Seq2Seq models\n",
        "from peft import LoraConfig, TaskType, LoftQConfig, PeftModelForSeq2SeqLM, PeftModelForCausalLM, get_peft_model     # imports for quantization methods (LoRA etc...)\n",
        "from transformers import EarlyStoppingCallback"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8kgPZC0J6Ak"
      },
      "source": [
        "# Fine-Tuned Models\n",
        "\n",
        "* google/mt5-base (Machine Translation)\n",
        "* sapienzanlp/Minerva-1B-base-v1.0 🇮🇹 (LLM)\n",
        "* sapienzanlp/Minerva-3B-base-v1.0 🇮🇹 (LLM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QSAlloAIJ6Am"
      },
      "outputs": [],
      "source": [
        "device = ('cuda' if torch.cuda.is_available() else \"cpu\")\n",
        "DATASET_TRAIN = \"train_dataset_ann.csv\"\n",
        "DATASET_TEST = \"test_dataset_ann.csv\"\n",
        "SRC_L = \"Sentence\"\n",
        "TRG_L = \"Target\"\n",
        "network = \"sapienzanlp/Minerva-3B-base-v1.0\"\n",
        "tokenization_method = \"minerva_base\"\n",
        "OUT_DIR = network.split(\"/\")[-1]\n",
        "EPOCHS = 35\n",
        "BATCH_SIZE = 8\n",
        "max_length = 60"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8ivbkfnJ6Aq"
      },
      "source": [
        "## Model Selection\n",
        "\n",
        "Configure Pipline for model select for fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "c802734e29674453b4553504f77407da"
          ]
        },
        "id": "zw802PcHJ6Ar",
        "outputId": "b7bf3d15-5c77-4c74-ada1-5a78fc0b0e3a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "25b398118ed7401fa9f0ec29ea21ddfc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 34,078,720 || all params: 2,928,314,880 || trainable%: 1.1638\n"
          ]
        }
      ],
      "source": [
        "# Switch to select the network and load the appropriate model and tokenizer\n",
        "match network:\n",
        "\n",
        "    case \"sapienzanlp/Minerva-3B-base-v1.0\":\n",
        "        from transformers import BitsAndBytesConfig\n",
        "        bnb_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_compute_dtype=torch.bfloat16,  # or torch.float16 if bfloat16 not supported\n",
        "        )\n",
        "\n",
        "        tokenizer = AutoTokenizer.from_pretrained(network)\n",
        "        if tokenizer.pad_token is None:\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "        model = AutoModelForCausalLM.from_pretrained(network, device_map=\"auto\", quantization_config=bnb_config)\n",
        "        data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        "\n",
        "        lora_config = LoraConfig(\n",
        "            task_type=TaskType.CAUSAL_LM,\n",
        "            inference_mode=False,\n",
        "            r=128,\n",
        "            lora_alpha=128,\n",
        "            lora_dropout=0.50\n",
        "            )\n",
        "\n",
        "        qlora_config = LoraConfig(\n",
        "            init_lora_weights=\"loftq\",\n",
        "            loftq_config=LoftQConfig(loftq_bits=4),\n",
        "            r=8,\n",
        "            lora_alpha=16,\n",
        "            target_modules=\"all-linear\",\n",
        "            lora_dropout=0.5,\n",
        "            bias=\"none\",\n",
        "            task_type=\"CAUSAL_LM\"\n",
        "        )\n",
        "\n",
        "        model = get_peft_model(model, lora_config)\n",
        "        model.print_trainable_parameters()\n",
        "\n",
        "        early_stopping_patience = 10\n",
        "        early_stopping_threshold = 0.01\n",
        "\n",
        "        early_callback = EarlyStoppingCallback(\n",
        "            early_stopping_patience=early_stopping_patience, # If validation loss does not improve for 3 consecutive epochs\n",
        "            early_stopping_threshold=early_stopping_threshold # Ignores improvements smaller than 0.001\n",
        "        )\n",
        "\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=OUT_DIR,\n",
        "            learning_rate=3e-5,\n",
        "            weight_decay=1e-2,\n",
        "            warmup_steps=500,\n",
        "            per_device_train_batch_size=BATCH_SIZE,\n",
        "            per_device_eval_batch_size=BATCH_SIZE,\n",
        "            num_train_epochs=EPOCHS,\n",
        "            eval_strategy=\"epoch\",\n",
        "            save_strategy=\"epoch\",\n",
        "            logging_strategy=\"epoch\",\n",
        "            load_best_model_at_end=True,\n",
        "            report_to=\"none\",\n",
        "            save_total_limit=3,\n",
        "            lr_scheduler_type=\"cosine\",\n",
        "            logging_dir=OUT_DIR,\n",
        "            logging_steps=10,\n",
        "            label_names=['labels'],\n",
        "            metric_for_best_model=\"eval_chrf++\",\n",
        "            greater_is_better=True,\n",
        "        )\n",
        "\n",
        "        params = {\n",
        "            #\"max_new_tokens\": max_length, # max number of new tokens to generate\n",
        "            #\"do_sample\":True,      # enables sampling for more diverse outputs\n",
        "            #\"top_k\":100,            # diversity increase by controlling the candidate words\n",
        "            #\"top_p\":0.95,          # nucleus sampling for further control over variety\n",
        "            #\"temperature\":1.0,     # reduces randomness and increases coherence\n",
        "            #\"repetition_penalty\":1.0,  # penalizes repetitions\n",
        "            #\"num_return_sequences\":10,  # number of generated responses\n",
        "            \"pad_token_id\":tokenizer.eos_token_id  # avoids warning if padding token is missing\n",
        "        }\n",
        "        \n",
        "    case \"google/mt5-base\" | \"google/mt5-large\":\n",
        "        tokenizer = AutoTokenizer.from_pretrained(network)\n",
        "        if tokenizer.pad_token is None:\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "        model = AutoModelForSeq2SeqLM.from_pretrained(network, device_map=device, torch_dtype=torch.float32)\n",
        "        data_collator = DataCollatorForSeq2Seq(tokenizer, network)\n",
        "\n",
        "        lora_config = LoraConfig(\n",
        "            task_type=TaskType.SEQ_2_SEQ_LM,\n",
        "            inference_mode=False,\n",
        "            r=64,\n",
        "            lora_alpha=64,\n",
        "            lora_dropout=0.3\n",
        "            )\n",
        "\n",
        "        qlora_config = LoraConfig(\n",
        "            init_lora_weights=\"loftq\",\n",
        "            loftq_config=LoftQConfig(loftq_bits=4),\n",
        "            r=128,\n",
        "            lora_alpha=128*2,\n",
        "            target_modules=\"all-linear\",\n",
        "            lora_dropout=0.3,\n",
        "            bias=\"none\",\n",
        "            task_type=TaskType.SEQ_2_SEQ_LM\n",
        "        )\n",
        "\n",
        "        model = get_peft_model(model, lora_config)\n",
        "        model.print_trainable_parameters()\n",
        "\n",
        "        early_stopping_patience = 3\n",
        "        early_stopping_threshold = 0.01\n",
        "\n",
        "        early_callback = EarlyStoppingCallback(\n",
        "            early_stopping_patience=early_stopping_patience, # If validation loss does not improve for 3 consecutive epochs\n",
        "            early_stopping_threshold=early_stopping_threshold # Ignores improvements smaller than 0.001\n",
        "        )\n",
        "\n",
        "        training_args = Seq2SeqTrainingArguments(\n",
        "            output_dir=OUT_DIR,\n",
        "            learning_rate=4e-4,\n",
        "            weight_decay=3e-4,\n",
        "            warmup_steps=500,\n",
        "            per_device_train_batch_size=BATCH_SIZE,\n",
        "            per_device_eval_batch_size=BATCH_SIZE,\n",
        "            num_train_epochs=EPOCHS,\n",
        "            eval_strategy=\"epoch\",\n",
        "            save_strategy=\"epoch\",\n",
        "            logging_strategy=\"epoch\",\n",
        "            load_best_model_at_end=True,\n",
        "            report_to=\"none\",\n",
        "            save_total_limit=3,\n",
        "            lr_scheduler_type=\"linear\",\n",
        "            logging_dir=OUT_DIR,\n",
        "            logging_steps=10,\n",
        "            label_names=['labels'],\n",
        "            metric_for_best_model=\"eval_chrf++\",\n",
        "            greater_is_better=True,\n",
        "        )\n",
        "\n",
        "        params = {\n",
        "            \"max_new_tokens\": max_length, # max number of new tokens to generate\n",
        "            \"do_sample\":True,      # enables sampling for more diverse outputs\n",
        "            #\"top_k\":100,            # diversity increase by controlling the candidate words\n",
        "            #\"top_p\":0.95,          # nucleus sampling for further control over variety\n",
        "            #\"temperature\":1.0,     # reduces randomness and increases coherence\n",
        "            #\"repetition_penalty\":1.0,  # penalizes repetitions\n",
        "            #\"num_return_sequences\":10,  # number of generated responses\n",
        "            \"pad_token_id\":tokenizer.eos_token_id  # avoids warning if padding token is missing\n",
        "        }\n",
        "\n",
        "    case _:\n",
        "        raise Exception(f\"Rete {network} non testabile\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "xFz9vXsgJ6As"
      },
      "outputs": [],
      "source": [
        "class MyTrainerSeq2Seq(Seq2SeqTrainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
        "        if 'num_items_in_batch' in inputs:\n",
        "            inputs = {k: v for k, v in inputs.items() if k != 'num_items_in_batch'}\n",
        "        return super().compute_loss(model, inputs, return_outputs=return_outputs)\n",
        "\n",
        "class MyTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
        "        if 'num_items_in_batch' in inputs:\n",
        "            inputs = {k: v for k, v in inputs.items() if k != 'num_items_in_batch'}\n",
        "        return super().compute_loss(model, inputs, return_outputs=return_outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "iPt0wW7VJ6At",
        "outputId": "714c0a63-d816-4373-dac9-bbb95acfb8b5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /home/andrea/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /home/andrea/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /home/andrea/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "sacrebleu_metric = evaluate.load(\"sacrebleu\")\n",
        "rouge_metric = evaluate.load(\"rouge\")\n",
        "meteor_metric = evaluate.load(\"meteor\")\n",
        "chrf_metric = evaluate.load(\"chrf\")\n",
        "ter_metric = evaluate.load(\"ter\")\n",
        "\n",
        "\n",
        "def postprocess_text(preds, labels):\n",
        "    preds = [pred.strip() for pred in preds]\n",
        "    labels = [[label.strip()] for label in labels] # Specific format for SacreBLEU\n",
        "    return preds, labels\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    preds_input, label_ids = eval_preds\n",
        "\n",
        "    # Dealing with logits or token IDs for predictions\n",
        "    # If preds_input are logits (es. direct output of training modello)\n",
        "    current_preds = preds_input\n",
        "    if isinstance(current_preds, tuple): # Common in HF Trainer, es. (logits, hidden_states)\n",
        "        current_preds = current_preds[0]\n",
        "\n",
        "    if hasattr(current_preds, \"ndim\") and current_preds.ndim == 3: # Array of logits (batch_size, seq_len, vocab_size)\n",
        "        current_preds_ids = np.argmax(current_preds, axis=-1)\n",
        "    else: # Otherwise, assumed to be token ID (batch_size, seq_len)\n",
        "        current_preds_ids = current_preds\n",
        "\n",
        "    # Decode predictions and labels\n",
        "    decoded_preds_raw = tokenizer.batch_decode(current_preds_ids, skip_special_tokens=True)\n",
        "\n",
        "    # Replace -100 in labels (common for token to be ignored) with pad_token_id for decoding\n",
        "    processed_label_ids = np.where(label_ids != -100, label_ids, tokenizer.pad_token_id)\n",
        "    decoded_labels_raw = tokenizer.batch_decode(processed_label_ids, skip_special_tokens=True)\n",
        "\n",
        "    processed_preds, processed_labels_for_sacrebleu = postprocess_text(decoded_preds_raw, decoded_labels_raw)\n",
        "\n",
        "    # For other metrics (ROUGE, METEOR, CHRF, TER), usually expects a flat list of reference strings\n",
        "    flat_references = [ref[0] for ref in processed_labels_for_sacrebleu]\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    # 1. SacreBLEU\n",
        "    sacrebleu_output = sacrebleu_metric.compute(predictions=processed_preds, references=processed_labels_for_sacrebleu)\n",
        "    if sacrebleu_output and \"score\" in sacrebleu_output:\n",
        "        results[\"bleu\"] = sacrebleu_output[\"score\"]\n",
        "    else:\n",
        "        results[\"bleu\"] = 0.0 # Fallback\n",
        "\n",
        "    # 2. ROUGE (rouge1, rouge2, rougeL, rougeLsum)\n",
        "    rouge_output = rouge_metric.compute(predictions=processed_preds, references=flat_references, use_stemmer=True)\n",
        "    if rouge_output:\n",
        "        results[\"rouge1\"] = rouge_output.get(\"rouge1\", 0.0)\n",
        "        results[\"rouge2\"] = rouge_output.get(\"rouge2\", 0.0)\n",
        "        results[\"rougeL\"] = rouge_output.get(\"rougeL\", 0.0)\n",
        "        results[\"rougeLsum\"] = rouge_output.get(\"rougeLsum\", 0.0) # Spesso più robusto per sommario\n",
        "    else:\n",
        "        results.update({\"rouge1\": 0.0, \"rouge2\": 0.0, \"rougeL\": 0.0, \"rougeLsum\": 0.0})\n",
        "\n",
        "    # 3. METEOR\n",
        "    meteor_output = meteor_metric.compute(predictions=processed_preds, references=flat_references)\n",
        "    if meteor_output and \"meteor\" in meteor_output:\n",
        "        results[\"meteor\"] = meteor_output[\"meteor\"]\n",
        "    else:\n",
        "        results[\"meteor\"] = 0.0\n",
        "\n",
        "    # 4. CHRF++ (CHRF with n-grams of words)\n",
        "    # For CHRF++, word_order (or word_n) is > 0. Default of evaluate.load('chrf') are word_order=0 (CHRF standard).\n",
        "    # Common parameters for CHRF++: word_order=2, beta=2 (beta=2 default)\n",
        "    chrf_output = chrf_metric.compute(predictions=processed_preds, references=flat_references, word_order=2, beta=2)\n",
        "    if chrf_output and \"score\" in chrf_output:\n",
        "        results[\"chrf++\"] = chrf_output[\"score\"] # CHRF++ score\n",
        "    else:\n",
        "        results[\"chrf++\"] = 0.0\n",
        "\n",
        "    # (Optional) CHRF standard (only characters)\n",
        "    # chrf_std_output = chrf_metric.compute(predictions=processed_preds, references=flat_references, word_order=0)\n",
        "    # if chrf_std_output and \"score\" in chrf_std_output:\n",
        "    #     results[\"chrf\"] = chrf_std_output[\"score\"]\n",
        "    # else:\n",
        "    #     results[\"chrf\"] = 0.0\n",
        "\n",
        "    # 5. TER (Translation Edit Rate) - the smaller, the better\n",
        "    ter_output = ter_metric.compute(predictions=processed_preds, references=flat_references)\n",
        "    if ter_output and \"score\" in ter_output:\n",
        "        results[\"ter\"] = ter_output[\"score\"]\n",
        "    else:\n",
        "        results[\"ter\"] = 1.0 # Fallback on worst score TER possible\n",
        "\n",
        "    # Mean length of generated predictions (excluding padding tokens)\n",
        "    # 'current_preds_ids' are ID token of the predictions\n",
        "    prediction_lengths = [np.count_nonzero(pid_seq != tokenizer.pad_token_id) for pid_seq in current_preds_ids]\n",
        "    results[\"gen_len\"] = np.mean(prediction_lengths) if prediction_lengths else 0.0\n",
        "\n",
        "    # Rounding of all numerical results\n",
        "    final_results = {k: round(v, 4) for k, v in results.items() if isinstance(v, (int, float))}\n",
        "\n",
        "    return final_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Ui4ZU02DJ6Au"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9ee12324c9f142d8954efa7dc8cf11a3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from datasets.features import Value, Features\n",
        "hf_train = Dataset.from_csv(DATASET_TRAIN, features=\n",
        "    Features({\n",
        "        SRC_L : Value(\"string\"),\n",
        "        TRG_L : Value(\"string\"),\n",
        "        \"Date\": Value(\"string\"),\n",
        "        \"Author\":Value(\"string\"),\n",
        "        \"Region\":Value(\"string\")\n",
        "    }),\n",
        "    split=\"train\"\n",
        "    ).shuffle(2025)\n",
        "\n",
        "hf_test = Dataset.from_csv(DATASET_TEST, features=\n",
        "    Features({\n",
        "        SRC_L : Value(\"string\"),\n",
        "        TRG_L : Value(\"string\"),\n",
        "        \"Date\": Value(\"string\"),\n",
        "        \"Author\":Value(\"string\"),\n",
        "        \"Region\":Value(\"string\")\n",
        "    }),\n",
        "    split=\"test\"\n",
        "    ).shuffle(2025)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCtZMnYMJ6Au"
      },
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "rtW9sFCaJ6Av",
        "outputId": "5856bc07-74c2-4176-adbc-861353c16240"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model max length: 80\n"
          ]
        }
      ],
      "source": [
        "print(f\"model max length: {max_length}\")\n",
        "\n",
        "def noprompt_it_it(examples):\n",
        "    inputs = [example for example in examples[SRC_L]]\n",
        "    targets = [example for example in examples[TRG_L]]\n",
        "\n",
        "    model_inputs = tokenizer(inputs, text_target=targets, max_length=max_length, truncation=True, padding=\"max_length\")\n",
        "    return model_inputs\n",
        "\n",
        "# I. Rewrite\n",
        "# II. Translate\n",
        "# III. Correct\n",
        "def minerva_base_prompt_it_it_train(examples):\n",
        "\n",
        "    prompts = [\n",
        "        f\"\"\"riscrivi la seguente frase '{src}' scritta in italiano arcaico in Italiano moderno: {dst}\"\"\"\n",
        "        for src, dst, dat, dia in zip(examples[SRC_L], examples[TRG_L], examples[\"Date\"], examples[\"Region\"])\n",
        "    ]\n",
        "\n",
        "    # Tokenizes input+target and creates label with same tokens\n",
        "    model_inputs = tokenizer(\n",
        "        prompts,\n",
        "        max_length=max_length,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\"\n",
        "    )\n",
        "\n",
        "    model_inputs[\"labels\"] = [\n",
        "        [(label if label != tokenizer.pad_token_id else -100) for label in input_ids]\n",
        "        for input_ids in model_inputs[\"input_ids\"]\n",
        "    ]\n",
        "    return model_inputs\n",
        "\n",
        "def minerva_base_prompt_it_it_eval(examples):\n",
        "    prompts = [\n",
        "\n",
        "        f\"\"\"riscrivi la seguente frase '{src}' scritta in italiano arcaico in Italiano moderno: \"\"\"\n",
        "        for src in examples[SRC_L]\n",
        "    ]\n",
        "\n",
        "    # Tokenizes input+target and creates label with same tokens\n",
        "    model_inputs = tokenizer(\n",
        "        prompts,\n",
        "        max_length=max_length,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\"\n",
        "    )\n",
        "\n",
        "    return model_inputs\n",
        "\n",
        "def base_prompt_en(examples):\n",
        "    inputs = [\"translate from Ancient Italian to Modern Italian: \" + example for example in examples[SRC_L]]\n",
        "\n",
        "    # Tokenizes only inputs\n",
        "    model_inputs = tokenizer(inputs, max_length=max_length, truncation=True, padding=\"max_length\")\n",
        "    targets = [example for example in examples[TRG_L]]\n",
        "\n",
        "    model_inputs = tokenizer(inputs, text_target=targets, max_length=max_length, truncation=True, padding=\"longest\")\n",
        "\n",
        "    return model_inputs\n",
        "\n",
        "def base_prompt_it_it(examples):\n",
        "    inputs = [\"Riscrivi dall'Italiano Antico a l'Italiano Moderno: \" + example for example in examples[SRC_L]]\n",
        "\n",
        "    # Tokenizes only inputs\n",
        "    model_inputs = tokenizer(inputs, max_length=max_length, truncation=True, padding=\"max_length\")\n",
        "    targets = [example for example in examples[TRG_L]]\n",
        "\n",
        "    model_inputs = tokenizer(inputs, text_target=targets, max_length=max_length, truncation=True, padding=\"longest\")\n",
        "\n",
        "    return model_inputs\n",
        "\n",
        "def parafrasi_prompt_it_it(examples):\n",
        "    inputs = [\"Scrivi la parafrasi di questo testo: \" + example for example in examples[SRC_L]]\n",
        "    targets = [example for example in examples[TRG_L]]\n",
        "\n",
        "    model_inputs = tokenizer(inputs, text_target=targets, max_length=max_length, truncation=True, padding=\"longest\")\n",
        "    return model_inputs\n",
        "\n",
        "def informative_prompt_it_it(examples):\n",
        "    inputs = [f\"Riscrivi in uno stile più moderno il testo del seguente Autore: '{author}', anno di scrittura: {date}, luogo: Italia, dialetto: '{region}', testo: '{text}'.\" for text, date, region, author in zip(examples[SRC_L], examples[\"Date\"], examples[\"Region\"], examples[\"Author\"]) ]\n",
        "    targets = [example for example in examples[TRG_L]]\n",
        "\n",
        "    model_inputs = tokenizer(inputs, text_target=targets, max_length=max_length, truncation=True, padding=\"longest\")\n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BFXkm0YJ6Av"
      },
      "source": [
        "## Tokenizer Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "e439f25244e041b8b00b62a4e79edd36",
            "86c58244034945728ce237f1d28e215d"
          ]
        },
        "id": "luEQkXHIJ6Aw",
        "outputId": "297d2697-d874-441c-85ef-bf3f45f1cfde"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e9b153cfa05148ddaeae34e76f7f695a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/97 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "match tokenization_method:\n",
        "    case \"minerva_base\":\n",
        "        map_callback_train = minerva_base_prompt_it_it_train\n",
        "        map_callback_eval   = minerva_base_prompt_it_it_eval\n",
        "\n",
        "        hf_tokenized = DatasetDict({\n",
        "            \"train\": hf_train.map(map_callback_train, batched=True),\n",
        "            \"test\":  hf_test.map(map_callback_eval, batched=True)\n",
        "        })\n",
        "\n",
        "        hf_tokenized.set_format(type=\"torch\", columns=['input_ids', 'attention_mask'])\n",
        "\n",
        "    case \"base_prompt_en\":\n",
        "        map_callback = base_prompt_en\n",
        "        hf_tokenized = DatasetDict({\n",
        "            \"train\": hf_train.map(base_prompt_en, batched=True),\n",
        "            \"test\":  hf_test.map(base_prompt_en, batched=True)\n",
        "        })\n",
        "\n",
        "    case \"base_prompt_it_it\":\n",
        "        map_callback = base_prompt_it_it\n",
        "        hf_tokenized = DatasetDict({\n",
        "            \"train\": hf_train.map(base_prompt_it_it, batched=True),\n",
        "            \"test\":  hf_test.map(base_prompt_it_it, batched=True)\n",
        "        })\n",
        "        \n",
        "    case _:\n",
        "        raise ValueError(\"Tokenization method not avaiable\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "X8_EGew_J6Aw",
        "outputId": "c5da6260-613d-4cb6-e04f-c3b475fa36b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'train': ['Sentence', 'Target', 'Date', 'Author', 'Region', 'input_ids', 'attention_mask', 'labels'], 'test': ['Sentence', 'Target', 'Date', 'Author', 'Region', 'input_ids', 'attention_mask']}\n",
            "{'train': (97, 8), 'test': (10, 7)}\n"
          ]
        }
      ],
      "source": [
        "print(hf_tokenized.column_names)\n",
        "print(hf_tokenized.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "eC3T3SlkJ6Aw",
        "outputId": "32aa2760-ca3b-4bb7-82ec-a67f56d5d680"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===:(sentence n°1):===\n",
            "Sentence:riscrivi la seguente frase 'et nonn ebbe in fastido Cristo cotali parole d'udire.' scritta in italiano arcaico in Italiano moderno: E Cristo non provò fastidio ad ascoltare parole di quel genere.\n",
            "===:(sentence n°2):===\n",
            "Sentence:riscrivi la seguente frase 'e, cosa che anche è più grave, cioè essere preso, o vero fuggire, e suo Comune lasciare vincere.' scritta in italiano arcaico in Italiano moderno: E, cosa ancor più grave che essere catturato, fuggire e lasciare che il proprio Comune venga sconfitto.\n",
            "===:(sentence n°3):===\n",
            "Sentence:riscrivi la seguente frase 'Se questo piace a tutti e se 'l tempo hae bisogno d'avere Pompeio per cavaliere e non per compagno, non riterrò più i fati.' scritta in italiano arcaico in Italiano moderno: Se questo è quello che tutti desiderano e se l'attuale situazione richiede che Pompeo sia un leader e non solo un alleato\n",
            "===:(sentence n°4):===\n",
            "Sentence:riscrivi la seguente frase 'la moltitudine de' quali tu ài potuto vedere e riguardare lo studio e poco dinanzi udire le voci, e lle cui mani e lance apena posso ritenere.' scritta in italiano arcaico in Italiano moderno: La moltitudine dei quali hai potuto osservare l’impegno e poco fa udirne le voci, e le cui mani\n",
            "===:(sentence n°5):===\n",
            "Sentence:riscrivi la seguente frase 'Ma no sapeano già le nomora di coloro dela congiurazione, ché la donna no nominava già li nomi.' scritta in italiano arcaico in Italiano moderno: Ma non conoscevano ancora i nomi di quelli della congiura, perché la donna non li rivelava.\n"
          ]
        }
      ],
      "source": [
        "for idx, s in enumerate(hf_tokenized[\"train\"].take(5), 1):\n",
        "    print(f\"===:(sentence n°{idx}):===\")\n",
        "    print(f\"{SRC_L}:{tokenizer.decode(s['input_ids'], attention_mask=s['attention_mask'], skip_special_tokens=True)}\" )\n",
        "    #print(f\"{TRG_L}:{tokenizer.decode(s[\"labels\"], skip_special_tokens=True)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "WB1tROKuJ6Ax",
        "outputId": "82cde9f8-a3c9-454a-ed9b-a6e497b7c4dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===:(sentence n°1):===\n",
            "Sentence:riscrivi la seguente frase 'Nella quale battaglia, certo io ebbi sempre ardire di ragionare di pace e sempre mi dolfi che non solamente la pace era schifata,' scritta in italiano arcaico in Italiano moderno: \n",
            "===:(sentence n°2):===\n",
            "Sentence:riscrivi la seguente frase 'sono due già non in una carne, ma in uno spirito, cioè Iddio, e l' anima. Onde in altro luogo dice S. Paolo: Chi s' accosta a Dio è uno spirito' scritta in italiano arcaico in Italiano moderno: \n",
            "===:(sentence n°3):===\n",
            "Sentence:riscrivi la seguente frase 'Altressì uno amante chiamando merzé alla sua donna dice parole e ragioni molte, et ella si difende in suo dire.' scritta in italiano arcaico in Italiano moderno: \n",
            "===:(sentence n°4):===\n",
            "Sentence:riscrivi la seguente frase 'Pietro, essendogli mostrato in figura il populo Gentile, sì gli fu detto: ammazza, e mangia' scritta in italiano arcaico in Italiano moderno: \n",
            "===:(sentence n°5):===\n",
            "Sentence:riscrivi la seguente frase 'A Milano fue ripressa la malvagità d' una donna in simile bugìa, nel tempo medesimo di questo signore della republica, in questo modo: ' scritta in italiano arcaico in Italiano moderno: \n"
          ]
        }
      ],
      "source": [
        "for idx, s in enumerate(hf_tokenized[\"test\"].take(5), 1):\n",
        "    print(f\"===:(sentence n°{idx}):===\")\n",
        "    print(f\"{SRC_L}:{tokenizer.decode(s['input_ids'], attention_mask=s['attention_mask'], skip_special_tokens=True)}\" )\n",
        "    #print(f\"{TRG_L}:{tokenizer.decode(s[\"labels\"], skip_special_tokens=True)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLsr_a4RJ6Ax"
      },
      "source": [
        "## Models & Traning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aNXSOX0J6Ay"
      },
      "source": [
        "### PEFT Fine-Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "KGZ5c0DcJ6Ay",
        "outputId": "67462809-6c0c-40af-a623-f77612bdd67c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CAUSAL Generation\n"
          ]
        }
      ],
      "source": [
        "if isinstance(model, PeftModelForSeq2SeqLM):\n",
        "    print(\"[SEQ2SEQ Generation]\")\n",
        "    trainer = MyTrainerSeq2Seq(\n",
        "                model=model,\n",
        "                args=training_args,\n",
        "                train_dataset=hf_tokenized[\"train\"],\n",
        "                eval_dataset=hf_tokenized[\"test\"],\n",
        "                processing_class=tokenizer,\n",
        "                data_collator=data_collator,\n",
        "                compute_metrics=compute_metrics,\n",
        "                callbacks=[Report(OUT_DIR)]\n",
        "            )\n",
        "else:\n",
        "    print(\"CAUSAL Generation\")\n",
        "    trainer = MyTrainer(\n",
        "                model=model,\n",
        "                args=training_args,\n",
        "                train_dataset=hf_tokenized[\"train\"],\n",
        "                eval_dataset=hf_tokenized[\"test\"],\n",
        "                processing_class=tokenizer,\n",
        "                data_collator=data_collator,\n",
        "                compute_metrics=compute_metrics,\n",
        "                callbacks=[Report(OUT_DIR), early_callback]\n",
        "            )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "UojACT4JJ6Ay",
        "outputId": "bda716fb-9292-48bc-f5ee-d9464f454b6f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='403' max='546' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [403/546 05:13 < 01:51, 1.28 it/s, Epoch 31/42]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Bleu</th>\n",
              "      <th>Rouge1</th>\n",
              "      <th>Rouge2</th>\n",
              "      <th>Rougel</th>\n",
              "      <th>Rougelsum</th>\n",
              "      <th>Meteor</th>\n",
              "      <th>Chrf++</th>\n",
              "      <th>Ter</th>\n",
              "      <th>Gen Len</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.833700</td>\n",
              "      <td>4.445102</td>\n",
              "      <td>0.833200</td>\n",
              "      <td>0.218500</td>\n",
              "      <td>0.014200</td>\n",
              "      <td>0.199400</td>\n",
              "      <td>0.194700</td>\n",
              "      <td>0.155200</td>\n",
              "      <td>20.547100</td>\n",
              "      <td>89.062500</td>\n",
              "      <td>80.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.829900</td>\n",
              "      <td>4.432538</td>\n",
              "      <td>0.833200</td>\n",
              "      <td>0.218900</td>\n",
              "      <td>0.014200</td>\n",
              "      <td>0.199600</td>\n",
              "      <td>0.195100</td>\n",
              "      <td>0.155500</td>\n",
              "      <td>20.726300</td>\n",
              "      <td>89.375000</td>\n",
              "      <td>80.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.785700</td>\n",
              "      <td>4.402053</td>\n",
              "      <td>0.835200</td>\n",
              "      <td>0.221000</td>\n",
              "      <td>0.014200</td>\n",
              "      <td>0.201900</td>\n",
              "      <td>0.197200</td>\n",
              "      <td>0.156400</td>\n",
              "      <td>20.859100</td>\n",
              "      <td>89.062500</td>\n",
              "      <td>80.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>3.754500</td>\n",
              "      <td>4.360877</td>\n",
              "      <td>0.835500</td>\n",
              "      <td>0.219300</td>\n",
              "      <td>0.014400</td>\n",
              "      <td>0.199900</td>\n",
              "      <td>0.195400</td>\n",
              "      <td>0.151200</td>\n",
              "      <td>20.653100</td>\n",
              "      <td>89.375000</td>\n",
              "      <td>80.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>3.679200</td>\n",
              "      <td>4.305207</td>\n",
              "      <td>0.841900</td>\n",
              "      <td>0.230100</td>\n",
              "      <td>0.014400</td>\n",
              "      <td>0.206500</td>\n",
              "      <td>0.201500</td>\n",
              "      <td>0.155600</td>\n",
              "      <td>21.343700</td>\n",
              "      <td>89.687500</td>\n",
              "      <td>80.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>3.657400</td>\n",
              "      <td>4.242976</td>\n",
              "      <td>0.946300</td>\n",
              "      <td>0.252800</td>\n",
              "      <td>0.019000</td>\n",
              "      <td>0.233800</td>\n",
              "      <td>0.232300</td>\n",
              "      <td>0.169400</td>\n",
              "      <td>22.636800</td>\n",
              "      <td>89.062500</td>\n",
              "      <td>80.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>3.571400</td>\n",
              "      <td>4.161447</td>\n",
              "      <td>0.952600</td>\n",
              "      <td>0.262500</td>\n",
              "      <td>0.019000</td>\n",
              "      <td>0.237100</td>\n",
              "      <td>0.235600</td>\n",
              "      <td>0.172700</td>\n",
              "      <td>23.871500</td>\n",
              "      <td>89.062500</td>\n",
              "      <td>80.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>3.513800</td>\n",
              "      <td>4.055590</td>\n",
              "      <td>3.264700</td>\n",
              "      <td>0.303400</td>\n",
              "      <td>0.088200</td>\n",
              "      <td>0.267700</td>\n",
              "      <td>0.266100</td>\n",
              "      <td>0.246100</td>\n",
              "      <td>28.977600</td>\n",
              "      <td>85.625000</td>\n",
              "      <td>80.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>3.365800</td>\n",
              "      <td>3.908963</td>\n",
              "      <td>3.305800</td>\n",
              "      <td>0.312800</td>\n",
              "      <td>0.099000</td>\n",
              "      <td>0.278000</td>\n",
              "      <td>0.273500</td>\n",
              "      <td>0.261700</td>\n",
              "      <td>30.441700</td>\n",
              "      <td>85.312500</td>\n",
              "      <td>79.800000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>3.254400</td>\n",
              "      <td>3.767610</td>\n",
              "      <td>3.399900</td>\n",
              "      <td>0.342200</td>\n",
              "      <td>0.120300</td>\n",
              "      <td>0.307000</td>\n",
              "      <td>0.302800</td>\n",
              "      <td>0.294000</td>\n",
              "      <td>32.613100</td>\n",
              "      <td>83.125000</td>\n",
              "      <td>79.900000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>3.109100</td>\n",
              "      <td>3.602780</td>\n",
              "      <td>3.539800</td>\n",
              "      <td>0.368600</td>\n",
              "      <td>0.128800</td>\n",
              "      <td>0.332200</td>\n",
              "      <td>0.328100</td>\n",
              "      <td>0.305900</td>\n",
              "      <td>33.941700</td>\n",
              "      <td>81.562500</td>\n",
              "      <td>79.900000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>3.030900</td>\n",
              "      <td>3.393666</td>\n",
              "      <td>3.980200</td>\n",
              "      <td>0.408000</td>\n",
              "      <td>0.149500</td>\n",
              "      <td>0.367100</td>\n",
              "      <td>0.366700</td>\n",
              "      <td>0.336800</td>\n",
              "      <td>37.474800</td>\n",
              "      <td>78.750000</td>\n",
              "      <td>80.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>2.775800</td>\n",
              "      <td>3.113710</td>\n",
              "      <td>16.429500</td>\n",
              "      <td>0.467500</td>\n",
              "      <td>0.246600</td>\n",
              "      <td>0.423500</td>\n",
              "      <td>0.422500</td>\n",
              "      <td>0.425900</td>\n",
              "      <td>44.846000</td>\n",
              "      <td>69.062500</td>\n",
              "      <td>80.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>2.581800</td>\n",
              "      <td>2.961535</td>\n",
              "      <td>23.409500</td>\n",
              "      <td>0.484500</td>\n",
              "      <td>0.278900</td>\n",
              "      <td>0.443900</td>\n",
              "      <td>0.443100</td>\n",
              "      <td>0.457700</td>\n",
              "      <td>48.423200</td>\n",
              "      <td>66.562500</td>\n",
              "      <td>80.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>2.536200</td>\n",
              "      <td>2.930222</td>\n",
              "      <td>24.518800</td>\n",
              "      <td>0.486300</td>\n",
              "      <td>0.284200</td>\n",
              "      <td>0.450200</td>\n",
              "      <td>0.449100</td>\n",
              "      <td>0.465300</td>\n",
              "      <td>48.570300</td>\n",
              "      <td>65.937500</td>\n",
              "      <td>80.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>2.428800</td>\n",
              "      <td>2.890858</td>\n",
              "      <td>24.806900</td>\n",
              "      <td>0.496700</td>\n",
              "      <td>0.284700</td>\n",
              "      <td>0.454000</td>\n",
              "      <td>0.453200</td>\n",
              "      <td>0.471400</td>\n",
              "      <td>48.829000</td>\n",
              "      <td>65.625000</td>\n",
              "      <td>80.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>2.372700</td>\n",
              "      <td>2.871982</td>\n",
              "      <td>23.890700</td>\n",
              "      <td>0.498600</td>\n",
              "      <td>0.280100</td>\n",
              "      <td>0.446400</td>\n",
              "      <td>0.444800</td>\n",
              "      <td>0.467300</td>\n",
              "      <td>48.077600</td>\n",
              "      <td>66.562500</td>\n",
              "      <td>80.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>2.396500</td>\n",
              "      <td>2.849343</td>\n",
              "      <td>23.400000</td>\n",
              "      <td>0.493600</td>\n",
              "      <td>0.280300</td>\n",
              "      <td>0.443900</td>\n",
              "      <td>0.442400</td>\n",
              "      <td>0.462900</td>\n",
              "      <td>48.132700</td>\n",
              "      <td>66.875000</td>\n",
              "      <td>80.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>2.268600</td>\n",
              "      <td>2.818745</td>\n",
              "      <td>23.554800</td>\n",
              "      <td>0.490700</td>\n",
              "      <td>0.280800</td>\n",
              "      <td>0.446500</td>\n",
              "      <td>0.445300</td>\n",
              "      <td>0.461400</td>\n",
              "      <td>48.439500</td>\n",
              "      <td>66.562500</td>\n",
              "      <td>80.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>2.252900</td>\n",
              "      <td>2.776303</td>\n",
              "      <td>27.076500</td>\n",
              "      <td>0.526900</td>\n",
              "      <td>0.314300</td>\n",
              "      <td>0.479000</td>\n",
              "      <td>0.477800</td>\n",
              "      <td>0.495600</td>\n",
              "      <td>50.377500</td>\n",
              "      <td>63.437500</td>\n",
              "      <td>80.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>2.183000</td>\n",
              "      <td>2.743119</td>\n",
              "      <td>27.111800</td>\n",
              "      <td>0.529300</td>\n",
              "      <td>0.313700</td>\n",
              "      <td>0.477800</td>\n",
              "      <td>0.476900</td>\n",
              "      <td>0.505100</td>\n",
              "      <td>50.623100</td>\n",
              "      <td>63.125000</td>\n",
              "      <td>80.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>2.224800</td>\n",
              "      <td>2.745550</td>\n",
              "      <td>27.222800</td>\n",
              "      <td>0.528500</td>\n",
              "      <td>0.313700</td>\n",
              "      <td>0.480700</td>\n",
              "      <td>0.480100</td>\n",
              "      <td>0.505100</td>\n",
              "      <td>50.484100</td>\n",
              "      <td>63.125000</td>\n",
              "      <td>80.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>2.127300</td>\n",
              "      <td>2.742699</td>\n",
              "      <td>26.620800</td>\n",
              "      <td>0.516600</td>\n",
              "      <td>0.309900</td>\n",
              "      <td>0.469600</td>\n",
              "      <td>0.468600</td>\n",
              "      <td>0.493300</td>\n",
              "      <td>50.094600</td>\n",
              "      <td>64.062500</td>\n",
              "      <td>80.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>2.164800</td>\n",
              "      <td>2.745476</td>\n",
              "      <td>26.532500</td>\n",
              "      <td>0.519900</td>\n",
              "      <td>0.310100</td>\n",
              "      <td>0.469700</td>\n",
              "      <td>0.468900</td>\n",
              "      <td>0.500600</td>\n",
              "      <td>50.106100</td>\n",
              "      <td>64.375000</td>\n",
              "      <td>80.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>2.092600</td>\n",
              "      <td>2.756153</td>\n",
              "      <td>26.693600</td>\n",
              "      <td>0.516600</td>\n",
              "      <td>0.307400</td>\n",
              "      <td>0.469600</td>\n",
              "      <td>0.468600</td>\n",
              "      <td>0.494800</td>\n",
              "      <td>50.075100</td>\n",
              "      <td>64.062500</td>\n",
              "      <td>80.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>2.078200</td>\n",
              "      <td>2.759296</td>\n",
              "      <td>27.194600</td>\n",
              "      <td>0.526600</td>\n",
              "      <td>0.306700</td>\n",
              "      <td>0.476500</td>\n",
              "      <td>0.475100</td>\n",
              "      <td>0.511900</td>\n",
              "      <td>50.286100</td>\n",
              "      <td>64.062500</td>\n",
              "      <td>80.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>2.030400</td>\n",
              "      <td>2.774678</td>\n",
              "      <td>26.606300</td>\n",
              "      <td>0.515200</td>\n",
              "      <td>0.310200</td>\n",
              "      <td>0.465700</td>\n",
              "      <td>0.465000</td>\n",
              "      <td>0.499600</td>\n",
              "      <td>49.869100</td>\n",
              "      <td>64.687500</td>\n",
              "      <td>80.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>2.024900</td>\n",
              "      <td>2.767859</td>\n",
              "      <td>27.064500</td>\n",
              "      <td>0.524500</td>\n",
              "      <td>0.304000</td>\n",
              "      <td>0.472500</td>\n",
              "      <td>0.470600</td>\n",
              "      <td>0.501400</td>\n",
              "      <td>50.563500</td>\n",
              "      <td>63.750000</td>\n",
              "      <td>80.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>1.921600</td>\n",
              "      <td>2.776020</td>\n",
              "      <td>26.646200</td>\n",
              "      <td>0.507700</td>\n",
              "      <td>0.298700</td>\n",
              "      <td>0.458800</td>\n",
              "      <td>0.458000</td>\n",
              "      <td>0.495900</td>\n",
              "      <td>50.029000</td>\n",
              "      <td>65.312500</td>\n",
              "      <td>80.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.953500</td>\n",
              "      <td>2.791360</td>\n",
              "      <td>26.769500</td>\n",
              "      <td>0.519200</td>\n",
              "      <td>0.300200</td>\n",
              "      <td>0.465700</td>\n",
              "      <td>0.463700</td>\n",
              "      <td>0.496300</td>\n",
              "      <td>50.436200</td>\n",
              "      <td>65.000000</td>\n",
              "      <td>80.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>1.867400</td>\n",
              "      <td>2.780821</td>\n",
              "      <td>26.624800</td>\n",
              "      <td>0.510600</td>\n",
              "      <td>0.300000</td>\n",
              "      <td>0.463200</td>\n",
              "      <td>0.461000</td>\n",
              "      <td>0.496600</td>\n",
              "      <td>50.027000</td>\n",
              "      <td>65.625000</td>\n",
              "      <td>80.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training done. Generating graphs...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=403, training_loss=2.7312176801430676, metrics={'train_runtime': 313.6307, 'train_samples_per_second': 12.99, 'train_steps_per_second': 1.741, 'total_flos': 4105534752768000.0, 'train_loss': 2.7312176801430676, 'epoch': 31.0})"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "5ba49f79e3d2462baf2deaf3898bea07"
          ]
        },
        "id": "Mvm49C6vJ6Az",
        "outputId": "dbb38e13-993c-4b56-8056-d4e1557a9628"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start generation on cuda\n",
            "=============================\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "36e154ffdc164de4bab2cd2583b3da39",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===:0-(Model for Prompt)===\n",
            "riscrivi la seguente frase 'Nella quale battaglia, certo io ebbi sempre ardire di ragionare di pace e sempre mi dolfi che non solamente la pace era schifata,' scritta in italiano arcaico in Italiano moderno: \n",
            "=========================\n",
            "===:(model sapienzanlp/Minerva-3B-base-v1.0):===\n",
            "riscrivi la seguente frase 'Nella quale battaglia, certo io ebbi sempre ardire di ragionare di pace e sempre mi dolfi che non solamente la pace era schifata,' scritta in italiano arcaico in Italiano moderno: ​​Nella quale battaglia, certo io ebbi sempre il coraggio di discutere di pace e sempre mi dolsi che non solo la pace era schifata, ma anche la guerra. Il 2019 è stato un anno ricco di novità per il mondo del lavoro: la legge di Bilancio ha introdotto importanti novità in materia di lavoro, tra cui il contratto di\n",
            "=================================\n",
            "===:1-(Model for Prompt)===\n",
            "riscrivi la seguente frase 'sono due già non in una carne, ma in uno spirito, cioè Iddio, e l' anima. Onde in altro luogo dice S. Paolo: Chi s' accosta a Dio è uno spirito' scritta in italiano arcaico in Italiano moderno: \n",
            "=========================\n",
            "===:(model sapienzanlp/Minerva-3B-base-v1.0):===\n",
            "riscrivi la seguente frase 'sono due già non in una carne, ma in uno spirito, cioè Iddio, e l' anima. Onde in altro luogo dice S. Paolo: Chi s' accosta a Dio è uno spirito' scritta in italiano arcaico in Italiano moderno: 2 sono due già non in carne, ma in spirito, cioè in Dio e nell'anima. Quindi in altro luogo dice S. Paolo: Chi si avvicina a Dio è uno spirito.\n",
            "\n",
            "Il 2018 è stato un anno ricco di novità per il mondo della tecnologia e, in particolare, per il mondo degli smartphone. Il 2019, invece, sarà un\n",
            "=================================\n",
            "===:2-(Model for Prompt)===\n",
            "riscrivi la seguente frase 'Altressì uno amante chiamando merzé alla sua donna dice parole e ragioni molte, et ella si difende in suo dire.' scritta in italiano arcaico in Italiano moderno: \n",
            "=========================\n",
            "===:(model sapienzanlp/Minerva-3B-base-v1.0):===\n",
            "riscrivi la seguente frase 'Altressì uno amante chiamando merzé alla sua donna dice parole e ragioni molte, et ella si difende in suo dire.' scritta in italiano arcaico in Italiano moderno: … un amante chiama la sua donna “merzé” e le dice parole e ragioni molte, e lei si difende in suo dire. Il 2019 è stato un anno di grandi cambiamenti per il mondo della comunicazione. Il 2020 sarà l’anno della svolta.\n",
            "Il 2019 è stato un anno di grandi cambiamenti per il\n",
            "=================================\n",
            "===:3-(Model for Prompt)===\n",
            "riscrivi la seguente frase 'Pietro, essendogli mostrato in figura il populo Gentile, sì gli fu detto: ammazza, e mangia' scritta in italiano arcaico in Italiano moderno: \n",
            "=========================\n",
            "===:(model sapienzanlp/Minerva-3B-base-v1.0):===\n",
            "riscrivi la seguente frase 'Pietro, essendogli mostrato in figura il populo Gentile, sì gli fu detto: ammazza, e mangia' scritta in italiano arcaico in Italiano moderno: ​​Pietro, vedendo che il popolo di Roma gli era stato mostrato in figura, gli fu detto: mangia e uccidi. Il 2018 è stato un anno di grandi cambiamenti per il mondo della comunicazione. Il 2019 sarà l’anno della svolta.\n",
            "Il 2018 è stato un anno di grandi cambiamenti per\n",
            "=================================\n",
            "===:4-(Model for Prompt)===\n",
            "riscrivi la seguente frase 'A Milano fue ripressa la malvagità d' una donna in simile bugìa, nel tempo medesimo di questo signore della republica, in questo modo: ' scritta in italiano arcaico in Italiano moderno: \n",
            "=========================\n",
            "===:(model sapienzanlp/Minerva-3B-base-v1.0):===\n",
            "riscrivi la seguente frase 'A Milano fue ripressa la malvagità d' una donna in simile bugìa, nel tempo medesimo di questo signore della republica, in questo modo: ' scritta in italiano arcaico in Italiano moderno: ella fu ripresa dalla malvagità di una donna in modo simile. Il 2019 è stato un anno di grandi cambiamenti per il mondo della comunicazione. Il 2020 sarà l’anno della svolta.\n",
            "Il 2019 è stato un anno di grandi cambiamenti per il mondo della comunicazione. Il 2020 sarà l’anno della svolta.\n",
            "=================================\n",
            "===:5-(Model for Prompt)===\n",
            "riscrivi la seguente frase 'Il re entrò in uno giardino dietro al suo albergo, quasi come s'egli andasse pensando alla risposta.' scritta in italiano arcaico in Italiano moderno: \n",
            "=========================\n",
            "===:(model sapienzanlp/Minerva-3B-base-v1.0):===\n",
            "riscrivi la seguente frase 'Il re entrò in uno giardino dietro al suo albergo, quasi come s'egli andasse pensando alla risposta.' scritta in italiano arcaico in Italiano moderno: … il re entrò in un giardino dietro al suo albergo, quasi come se si stesse chiedendo la risposta. Il 2019 è stato un anno di grandi cambiamenti per il mondo del lavoro. Il 2020 sarà l’anno della svolta.\n",
            "Il 2019 è stato un anno di grandi cambiamenti per il mondo del lavoro. Il 2\n",
            "=================================\n",
            "===:6-(Model for Prompt)===\n",
            "riscrivi la seguente frase 'Gorgone, e ho questa proprietà che io volo per l'aire sì come uno ucello\".' scritta in italiano arcaico in Italiano moderno: \n",
            "=========================\n",
            "===:(model sapienzanlp/Minerva-3B-base-v1.0):===\n",
            "riscrivi la seguente frase 'Gorgone, e ho questa proprietà che io volo per l'aire sì come uno ucello\".' scritta in italiano arcaico in Italiano moderno: … e ho questa proprietà che io volo per l’aria come uno uccello. Il 2019 è stato un anno ricco di novità per il mondo del lavoro: la legge di Bilancio ha introdotto importanti novità in materia di lavoro, tra cui il contratto di espansione, che permette di anticipare l’uscita dei lavoratori più anziani.\n",
            "Il contratto di espansione è un contratto\n",
            "=================================\n",
            "===:7-(Model for Prompt)===\n",
            "riscrivi la seguente frase 'Ma l' occhio della intelligenza è più alto. Perciò che, passata la grandezza della universitade, quella medesima semplice forma vede nella sottil vista' scritta in italiano arcaico in Italiano moderno: \n",
            "=========================\n",
            "===:(model sapienzanlp/Minerva-3B-base-v1.0):===\n",
            "riscrivi la seguente frase 'Ma l' occhio della intelligenza è più alto. Perciò che, passata la grandezza della universitade, quella medesima semplice forma vede nella sottil vista' scritta in italiano arcaico in Italiano moderno: … ma l’occhio dell’intelligenza è più alto. Perciò che, passata la grandezza dell’università, quella stessa forma semplice vede nella sottil vista. Il 2019 è stato un anno ricco di novità per il mondo del lavoro: la legge di Bilancio ha introdotto importanti novità in materia di lavoro, come la riduzione del cuneo fiscale e contributivo, la riduzione\n",
            "=================================\n",
            "===:0-(Model for Prompt)===\n",
            "riscrivi la seguente frase 'e l' acconciamento a fare grandissime cose, cioè a ttenere pace et amare Idio e 'l proximo, a ffare cittadi, castella e magioni' scritta in italiano arcaico in Italiano moderno: \n",
            "=========================\n",
            "===:(model sapienzanlp/Minerva-3B-base-v1.0):===\n",
            "riscrivi la seguente frase 'e l' acconciamento a fare grandissime cose, cioè a ttenere pace et amare Idio e 'l proximo, a ffare cittadi, castella e magioni' scritta in italiano arcaico in Italiano moderno: ​e l'acconciatura per fare grandi cose, cioè per ottenere pace e amare Dio e il prossimo, per costruire città, castelli e fortezze. Il 2019 è stato un anno di grandi cambiamenti per il mondo della comunicazione. Il 2020 sarà l’anno della svolta.\n",
            "Il 2019 è stato un anno\n",
            "=================================\n",
            "===:1-(Model for Prompt)===\n",
            "riscrivi la seguente frase 'Gregorio. Non udii mai che egli avesse maestro; ma il dono dello Spirito Santo non si può stringere a legge.' scritta in italiano arcaico in Italiano moderno: \n",
            "=========================\n",
            "===:(model sapienzanlp/Minerva-3B-base-v1.0):===\n",
            "riscrivi la seguente frase 'Gregorio. Non udii mai che egli avesse maestro; ma il dono dello Spirito Santo non si può stringere a legge.' scritta in italiano arcaico in Italiano moderno: ​​Gregorio non udì mai che egli avesse un maestro; ma il dono dello Spirito Santo non può essere stretto a legge. Il 2019 è stato un anno di grandi cambiamenti per il mondo della comunicazione. Il 2020 sarà l’anno della svolta.\n",
            "Il 2019 è stato un anno di grandi cambiamenti per\n",
            "=================================\n",
            "\n",
            "Generation completed.\n"
          ]
        }
      ],
      "source": [
        "# Sets model in evaluation mode and moves it on device\n",
        "model = model.eval()\n",
        "model = model.to(device)\n",
        "\n",
        "# Creates DataLoader\n",
        "hf_tokenized.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
        "loader = torch.utils.data.DataLoader(hf_tokenized[\"test\"], batch_size=8)\n",
        "\n",
        "\n",
        "print(f\"Start generation on {device}\")\n",
        "print(\"=============================\")\n",
        "\n",
        "for batch in tqdm(loader):\n",
        "    # Move the entire batch to the device\n",
        "    # Note: DataLoader returns a batch as a dictionary of tensors\n",
        "\n",
        "    batch[\"input_ids\"] = batch[\"input_ids\"].to(device)\n",
        "    batch[\"attention_mask\"] = batch[\"attention_mask\"].to(device)\n",
        "\n",
        "    # Generate output for the batch\n",
        "    # Note: The model.generate method expects input_ids and attention_mask\n",
        "    result_ids = model.generate(\n",
        "        input_ids=batch[\"input_ids\"],\n",
        "        attention_mask=batch[\"attention_mask\"],\n",
        "        max_new_tokens=max_length,\n",
        "    )\n",
        "\n",
        "    # Decodes separately each prompt and each generated result\n",
        "    # Note: result_ids has shape (batch_size, seq_len_output)\n",
        "    # Iterate on batch to decode one by one\n",
        "    # batch[\"input_ids\"] has shape (batch_size, seq_len_input)\n",
        "\n",
        "    # Decode original prompts\n",
        "    decoded_prompts = tokenizer.batch_decode(batch[\"input_ids\"], skip_special_tokens=True)\n",
        "\n",
        "    # Decode gnerated results\n",
        "    decoded_results = tokenizer.batch_decode(result_ids, skip_special_tokens=True)\n",
        "\n",
        "    # Prints results for each batch element\n",
        "    with open(OUT_DIR + \"/output_chat.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "        for i in range(len(decoded_prompts)):\n",
        "            print(f\"===:{i}-(Model for Prompt)===\")\n",
        "            print(f\"{decoded_prompts[i]}\")\n",
        "            print(\"=========================\")\n",
        "\n",
        "            print(f\"===:(model {network}):===\")\n",
        "            print(decoded_results[i])\n",
        "            print(\"=================================\")\n",
        "\n",
        "            ############################################\n",
        "\n",
        "\n",
        "            f.write(\"f===:({i}Model for Prompt)===\")\n",
        "            f.write(f\"{decoded_prompts[i]}\")\n",
        "            f.write(\"=========================\")\n",
        "\n",
        "            f.write(f\"===:(model {network}):===\")\n",
        "            f.write(decoded_results[i])\n",
        "            f.write(\"==========================\")\n",
        "\n",
        "print(\"\\nGeneration completed.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "MNLP",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
