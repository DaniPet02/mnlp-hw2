{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translation from Ancient to Modern Italian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Datases to work with Transformers by Hugging-Face\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "# Imports for Transformers\n",
    "from transformers import AutoTokenizer  # Datasets\n",
    "from datasets import Dataset, DatasetDict\n",
    "from utils import Report\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling, Trainer, TrainingArguments, AutoModelForCausalLM          # imports for causal Learning\n",
    "from transformers import DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer, AutoModelForSeq2SeqLM    # imports for Seq2Seq models\n",
    "from peft import LoraConfig, TaskType, LoftQConfig, PeftModelForSeq2SeqLM, PeftModelForCausalLM, get_peft_model     # imports for quantization methods (LoRA etc...)\n",
    "from transformers import EarlyStoppingCallback  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Promposed Models\n",
    "* google/gemma-3-1b-it (LLM) üöÄ\n",
    "* mistralai/Mistral-7B-Instruct-v0.2\n",
    "* sapienzanlp/Minerva-1B-base-v1.0 üáÆüáπ (LMM)\n",
    "* Helsinki-NLP/opus-mt-itc-itc (Machine Translation) üèÜ - use OpusPrompt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = ('cuda' if torch.cuda.is_available() else \"cpu\")\n",
    "DATASET = \"dataset_ann.csv\"\n",
    "SRC_L = \"Sentence\"\n",
    "TRG_L = \"Target\"\n",
    "network = \"sapienzanlp/Minerva-1B-base-v1.0\"\n",
    "tokenization_method = \"minerva_base\"\n",
    "OUT_DIR = network.split(\"/\")[-1]\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATASET, sep=\",\", index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length mean Sentence text: 20.04123711340206\n",
      "length mean Target text: 20.690721649484537\n"
     ]
    }
   ],
   "source": [
    "print(f\"length mean {SRC_L} text: {df[SRC_L].apply(lambda x: len(x.split())).mean()}\")\n",
    "print(f\"length mean {TRG_L} text: {df[TRG_L].apply(lambda x: len(x.split())).mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Author",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Date",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Region",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Sentence",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Target",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "2b49bb03-abb4-4e2b-808a-fda176f3d4f1",
       "rows": [
        [
         "0",
         "Brunetto Latini",
         "1260-61",
         "fior.",
         "quella guerra ben fatta l' opera perch√© etc. Et dall' altra parte Aiaces era uno cavaliere franco e prode all' arme, di gran guisa, ma non era pieno di grande senno",
         "Quella guerra fu ben condotta per via delle azioni compiute, eccetera. Dall‚Äôaltra parte, Aiace era un cavaliere valoroso e coraggioso nelle armi, di grande fama, ma non era molto saggio n√© dotato di grande intelligenza."
        ],
        [
         "1",
         "Bono Giamboni",
         "1292",
         "fior.",
         "crudele, e di tutte le colpe pigli vendetta, come dice la legge, ed a neuno cavaliere perdoni che pecchi.",
         "√à severo, e punisce tutte le colpe come prescrive la legge, e non perdona nessun cavaliere che commetta errori."
        ],
        [
         "2",
         "Valerio Massimo (red. V1",
         "1336",
         "fior.",
         "Non d' altra forza d' animo fue ornato Ponzio Aufidiano, romano cavaliere.",
         "Ponzio Aufidiano, cavaliere romano, fu dotato dello stesso coraggio d‚Äôanimo."
        ],
        [
         "3",
         "Lucano volg. (ed. Marinoni)",
         "1330/40",
         "prat.",
         "Se questo piace a tutti e se 'l tempo hae bisogno d'avere Pompeio per cavaliere e non per compagno, non riterr√≤ pi√π i fati.",
         "Se questo √® quello che tutti desiderano e se l'attuale situazione richiede che Pompeo sia un leader e non solo un alleato, allora non cercher√≤ pi√π di oppormi al destino."
        ],
        [
         "4",
         "Brunetto Latini",
         "1260-61",
         "fior.",
         "Officio di questa arte pare che sia dicere appostatamente per fare credere, fine √® far credere per lo dire.",
         "Il compito di quest‚Äôarte sembra essere quello di parlare in modo appropriato per convincere, e di far credere qualcosa attraverso le parole."
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Date</th>\n",
       "      <th>Region</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Brunetto Latini</td>\n",
       "      <td>1260-61</td>\n",
       "      <td>fior.</td>\n",
       "      <td>quella guerra ben fatta l' opera perch√© etc. E...</td>\n",
       "      <td>Quella guerra fu ben condotta per via delle az...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bono Giamboni</td>\n",
       "      <td>1292</td>\n",
       "      <td>fior.</td>\n",
       "      <td>crudele, e di tutte le colpe pigli vendetta, c...</td>\n",
       "      <td>√à severo, e punisce tutte le colpe come prescr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Valerio Massimo (red. V1</td>\n",
       "      <td>1336</td>\n",
       "      <td>fior.</td>\n",
       "      <td>Non d' altra forza d' animo fue ornato Ponzio ...</td>\n",
       "      <td>Ponzio Aufidiano, cavaliere romano, fu dotato ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lucano volg. (ed. Marinoni)</td>\n",
       "      <td>1330/40</td>\n",
       "      <td>prat.</td>\n",
       "      <td>Se questo piace a tutti e se 'l tempo hae biso...</td>\n",
       "      <td>Se questo √® quello che tutti desiderano e se l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Brunetto Latini</td>\n",
       "      <td>1260-61</td>\n",
       "      <td>fior.</td>\n",
       "      <td>Officio di questa arte pare che sia dicere app...</td>\n",
       "      <td>Il compito di quest‚Äôarte sembra essere quello ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Author     Date Region  \\\n",
       "0              Brunetto Latini  1260-61  fior.   \n",
       "1                Bono Giamboni     1292  fior.   \n",
       "2     Valerio Massimo (red. V1     1336  fior.   \n",
       "3  Lucano volg. (ed. Marinoni)  1330/40  prat.   \n",
       "4              Brunetto Latini  1260-61  fior.   \n",
       "\n",
       "                                            Sentence  \\\n",
       "0  quella guerra ben fatta l' opera perch√© etc. E...   \n",
       "1  crudele, e di tutte le colpe pigli vendetta, c...   \n",
       "2  Non d' altra forza d' animo fue ornato Ponzio ...   \n",
       "3  Se questo piace a tutti e se 'l tempo hae biso...   \n",
       "4  Officio di questa arte pare che sia dicere app...   \n",
       "\n",
       "                                              Target  \n",
       "0  Quella guerra fu ben condotta per via delle az...  \n",
       "1  √à severo, e punisce tutte le colpe come prescr...  \n",
       "2  Ponzio Aufidiano, cavaliere romano, fu dotato ...  \n",
       "3  Se questo √® quello che tutti desiderano e se l...  \n",
       "4  Il compito di quest‚Äôarte sembra essere quello ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 851,968 || all params: 1,007,552,512 || trainable%: 0.0846\n"
     ]
    }
   ],
   "source": [
    "# Switch to select the network and load the appropriate model and tokenizer\n",
    "match network:\n",
    "    \n",
    "    case \"sapienzanlp/Minerva-1B-base-v1.0\":\n",
    "        \n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(network)\n",
    "        max_length = 280\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token \n",
    "\n",
    "        \n",
    "        model = AutoModelForCausalLM.from_pretrained(network, device_map=device, torch_dtype=torch.float16)\n",
    "        data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "        \n",
    "        lora_config = LoraConfig(\n",
    "            task_type=TaskType.CAUSAL_LM, \n",
    "            inference_mode=False, \n",
    "            r=8, \n",
    "            lora_alpha=16, \n",
    "            lora_dropout=0.5\n",
    "            )\n",
    "\n",
    "        qlora_config = LoraConfig(\n",
    "            init_lora_weights=\"loftq\",\n",
    "            loftq_config=LoftQConfig(loftq_bits=4),\n",
    "            r=8,\n",
    "            lora_alpha=16,\n",
    "            target_modules=\"all-linear\",\n",
    "            lora_dropout=0.5,\n",
    "            bias=\"none\",\n",
    "            task_type=\"CAUSAL_LM\"\n",
    "        )\n",
    "\n",
    "        model = get_peft_model(model, lora_config)\n",
    "        model.print_trainable_parameters()\n",
    "\n",
    "        \n",
    "\n",
    "        early_stopping_patience = 3 \n",
    "        early_stopping_threshold = 0.01 \n",
    "\n",
    "        early_callback = EarlyStoppingCallback(\n",
    "            early_stopping_patience=early_stopping_patience, # Se la loss di valutazione non migliora per 3 epoche consecutive\n",
    "            early_stopping_threshold=early_stopping_threshold # Ignora miglioramenti inferiori a 0.001\n",
    "        )\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=OUT_DIR,\n",
    "            learning_rate=4e-4,\n",
    "            weight_decay=1e-5,\n",
    "            #warmup_steps=20,\n",
    "            per_device_train_batch_size=BATCH_SIZE,\n",
    "            per_device_eval_batch_size=BATCH_SIZE,\n",
    "            num_train_epochs=EPOCHS,\n",
    "            eval_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            logging_strategy=\"epoch\",\n",
    "            load_best_model_at_end=True,\n",
    "            report_to=\"none\",\n",
    "            save_total_limit=3,\n",
    "            lr_scheduler_type=\"linear\",\n",
    "            logging_dir=OUT_DIR,\n",
    "            logging_steps=10,\n",
    "            label_names=['labels'],\n",
    "            metric_for_best_model=\"eval_loss\", \n",
    "            greater_is_better=False,\n",
    "        )\n",
    "\n",
    "        params = {\n",
    "            \n",
    "            \"max_new_tokens\": 60, # max number of new tokens to generate\n",
    "            \"do_sample\":True,      # enables sampling for more diverse outputs\n",
    "            \"top_k\":50,            # diversity increase by controlling the candidate words\n",
    "            \"top_p\":0.95,          # nucleus sampling for further control over variety\n",
    "            \"temperature\":1.0,     # reduces randomness and increases coherence\n",
    "            \"repetition_penalty\":0.9,  # penalizza ripetizioni\n",
    "            \"num_return_sequences\":10,  # number of generated responses\n",
    "            \"pad_token_id\":tokenizer.eos_token_id  # avoids warning if padding token is missing\n",
    "        }\n",
    "\n",
    "    case _:\n",
    "        raise Exception(f\"Rete {network} non testabile\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTrainerSeq2Seq(Seq2SeqTrainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        if 'num_items_in_batch' in inputs:\n",
    "            inputs = {k: v for k, v in inputs.items() if k != 'num_items_in_batch'}\n",
    "        return super().compute_loss(model, inputs, return_outputs=return_outputs)\n",
    "\n",
    "class MyTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        if 'num_items_in_batch' in inputs:\n",
    "            inputs = {k: v for k, v in inputs.items() if k != 'num_items_in_batch'}\n",
    "        return super().compute_loss(model, inputs, return_outputs=return_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/andrea/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/andrea/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/andrea/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "sacrebleu_metric = evaluate.load(\"sacrebleu\")\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "meteor_metric = evaluate.load(\"meteor\")\n",
    "chrf_metric = evaluate.load(\"chrf\")\n",
    "ter_metric = evaluate.load(\"ter\")\n",
    "\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels] # Specific format for SacreBLEU\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds_input, label_ids = eval_preds\n",
    "\n",
    "    # Dealing with logits or token IDs for predictions\n",
    "    # If preds_input are logits (es. direct output of training modello)\n",
    "    current_preds = preds_input\n",
    "    if isinstance(current_preds, tuple): # Common in HF Trainer, es. (logits, hidden_states)\n",
    "        current_preds = current_preds[0]\n",
    "    \n",
    "    if hasattr(current_preds, \"ndim\") and current_preds.ndim == 3: # Array of logits (batch_size, seq_len, vocab_size)\n",
    "        current_preds_ids = np.argmax(current_preds, axis=-1)\n",
    "    else: # Otherwise, assumed to be token ID (batch_size, seq_len)\n",
    "        current_preds_ids = current_preds\n",
    "\n",
    "    # Decode predictions and labels\n",
    "    decoded_preds_raw = tokenizer.batch_decode(current_preds_ids, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100 in labels (common for token to be ignored) with pad_token_id for decoding\n",
    "    processed_label_ids = np.where(label_ids != -100, label_ids, tokenizer.pad_token_id)\n",
    "    decoded_labels_raw = tokenizer.batch_decode(processed_label_ids, skip_special_tokens=True)\n",
    "    \n",
    "    processed_preds, processed_labels_for_sacrebleu = postprocess_text(decoded_preds_raw, decoded_labels_raw)\n",
    "\n",
    "    # For other metrics (ROUGE, METEOR, CHRF, TER), usually expects a flat list of reference strings\n",
    "    flat_references = [ref[0] for ref in processed_labels_for_sacrebleu]\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # 1. SacreBLEU\n",
    "    sacrebleu_output = sacrebleu_metric.compute(predictions=processed_preds, references=processed_labels_for_sacrebleu)\n",
    "    if sacrebleu_output and \"score\" in sacrebleu_output:\n",
    "        results[\"bleu\"] = sacrebleu_output[\"score\"]\n",
    "    else:\n",
    "        results[\"bleu\"] = 0.0 # Fallback\n",
    "\n",
    "    # 2. ROUGE (rouge1, rouge2, rougeL, rougeLsum)\n",
    "    rouge_output = rouge_metric.compute(predictions=processed_preds, references=flat_references, use_stemmer=True)\n",
    "    if rouge_output:\n",
    "        results[\"rouge1\"] = rouge_output.get(\"rouge1\", 0.0)\n",
    "        results[\"rouge2\"] = rouge_output.get(\"rouge2\", 0.0)\n",
    "        results[\"rougeL\"] = rouge_output.get(\"rougeL\", 0.0)\n",
    "        results[\"rougeLsum\"] = rouge_output.get(\"rougeLsum\", 0.0) # Spesso pi√π robusto per sommario\n",
    "    else:\n",
    "        results.update({\"rouge1\": 0.0, \"rouge2\": 0.0, \"rougeL\": 0.0, \"rougeLsum\": 0.0})\n",
    "\n",
    "    # 3. METEOR\n",
    "    meteor_output = meteor_metric.compute(predictions=processed_preds, references=flat_references)\n",
    "    if meteor_output and \"meteor\" in meteor_output:\n",
    "        results[\"meteor\"] = meteor_output[\"meteor\"]\n",
    "    else:\n",
    "        results[\"meteor\"] = 0.0\n",
    "\n",
    "    # 4. CHRF++ (CHRF with n-grams of words)\n",
    "    # For CHRF++, word_order (or word_n) is > 0. Default of evaluate.load('chrf') are word_order=0 (CHRF standard).\n",
    "    # Common parameters for CHRF++: word_order=2, beta=2 (beta=2 default)\n",
    "    chrf_output = chrf_metric.compute(predictions=processed_preds, references=flat_references, word_order=2, beta=2)\n",
    "    if chrf_output and \"score\" in chrf_output:\n",
    "        results[\"chrf++\"] = chrf_output[\"score\"] # CHRF++ score\n",
    "    else:\n",
    "        results[\"chrf++\"] = 0.0\n",
    "        \n",
    "    # (Optional) CHRF standard (only characters)\n",
    "    # chrf_std_output = chrf_metric.compute(predictions=processed_preds, references=flat_references, word_order=0)\n",
    "    # if chrf_std_output and \"score\" in chrf_std_output:\n",
    "    #     results[\"chrf\"] = chrf_std_output[\"score\"]\n",
    "    # else:\n",
    "    #     results[\"chrf\"] = 0.0\n",
    "\n",
    "    # 5. TER (Translation Edit Rate) - the smaller, the better\n",
    "    ter_output = ter_metric.compute(predictions=processed_preds, references=flat_references)\n",
    "    if ter_output and \"score\" in ter_output:\n",
    "        results[\"ter\"] = ter_output[\"score\"]\n",
    "    else:\n",
    "        results[\"ter\"] = 1.0 # Fallback on worst score TER possible\n",
    "\n",
    "    # Mean length of generated predictions (excluding padding tokens)\n",
    "    # 'current_preds_ids' are ID token of the predictions\n",
    "    prediction_lengths = [np.count_nonzero(pid_seq != tokenizer.pad_token_id) for pid_seq in current_preds_ids]\n",
    "    results[\"gen_len\"] = np.mean(prediction_lengths) if prediction_lengths else 0.0\n",
    "\n",
    "    # Rounding of all numerical results\n",
    "    final_results = {k: round(v, 4) for k, v in results.items() if isinstance(v, (int, float))}\n",
    "    \n",
    "    return final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.features import Value, Features\n",
    "hf = Dataset.from_csv(DATASET, features=\n",
    "    Features({\n",
    "        SRC_L : Value(\"string\"),\n",
    "        TRG_L : Value(\"string\"),\n",
    "        \"Date\": Value(\"string\"),\n",
    "        \"Author\":Value(\"string\"),\n",
    "        \"Region\":Value(\"string\")\n",
    "    })          \n",
    "                      \n",
    "    ).shuffle(2025).train_test_split(test_size=0.10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model max length: 280\n"
     ]
    }
   ],
   "source": [
    "print(f\"model max length: {max_length}\")\n",
    "\n",
    "\n",
    "def noprompt_it_it(examples):\n",
    "    inputs = [example for example in examples[SRC_L]]\n",
    "    targets = [example for example in examples[TRG_L]]\n",
    "    \n",
    "    model_inputs = tokenizer(inputs, text_target=targets, max_length=max_length, truncation=True, padding=\"max_length\")\n",
    "    return model_inputs\n",
    "\n",
    "def minerva_base_prompt_it_it_train(examples):\n",
    "    prompts = [\n",
    "        f\"Riscrivi '{src}' dall'Italiano Antico a l'Italiano Moderno\\n Risposta\\n\" \n",
    "        for src in examples[SRC_L]\n",
    "    ]\n",
    "    targets = [f\" {trg}\" for trg in examples[TRG_L]]\n",
    "\n",
    "    # Concatena input e target in un'unica stringa\n",
    "    full_texts = [prompt + target for prompt, target in zip(prompts, targets)]\n",
    "\n",
    "    # Tokenizza input+target e crea label con gli stessi token\n",
    "    model_inputs = tokenizer(\n",
    "        full_texts,\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "   \n",
    "    return model_inputs\n",
    "    \n",
    "def minerva_base_prompt_it_it_eval(examples):\n",
    "    prompts = [\n",
    "        f\"Riscrivi '{src}' dall'Italiano Antico a l'Italiano Moderno\\n Risposta\\n\" \n",
    "        for src in examples[SRC_L]\n",
    "    ]\n",
    "\n",
    "    # Tokenizza input+target e crea label con gli stessi token\n",
    "    model_inputs = tokenizer(\n",
    "        prompts,\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "   \n",
    "    return model_inputs\n",
    "\n",
    "def base_prompt_it_it(examples):\n",
    "    inputs = [\"Riscrivi dall'Italiano Antico a l'Italiano Moderno: \" + example for example in examples[SRC_L]]\n",
    "\n",
    "    # Tokenizza solo gli input\n",
    "    model_inputs = tokenizer(inputs, max_length=max_length, truncation=True, padding=\"max_length\")\n",
    "    targets = [example for example in examples[TRG_L]]\n",
    "\n",
    "    model_inputs = tokenizer(inputs, text_target=targets, max_length=max_length, truncation=True, padding=\"longest\")\n",
    "    \n",
    "    return model_inputs\n",
    "\n",
    "def parafrasi_prompt_it_it(examples):\n",
    "    inputs = [\"Scrivi la parafrasi di questo testo: \" + example for example in examples[SRC_L]]\n",
    "    targets = [example for example in examples[TRG_L]]\n",
    "    \n",
    "    model_inputs = tokenizer(inputs, text_target=targets, max_length=max_length, truncation=True, padding=\"longest\")\n",
    "    return model_inputs\n",
    "\n",
    "def informative_prompt_it_it(examples):\n",
    "    inputs = [f\"Riscrivi in uno stile pi√π moderno il testo del seguente Autore: '{author}', anno di scrittura: {date}, luogo: Italia, dialetto: '{region}', testo: '{text}'.\" for text, date, region, author in zip(examples[SRC_L], examples[\"Date\"], examples[\"Region\"], examples[\"Author\"]) ]\n",
    "    targets = [example for example in examples[TRG_L]]\n",
    "    \n",
    "    model_inputs = tokenizer(inputs, text_target=targets, max_length=max_length, truncation=True, padding=\"longest\")\n",
    "    return model_inputs\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c17ef4b6b58f4a5d88bd5c1537164493",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/87 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7132395741c54ea58fdeb346e3440120",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "match tokenization_method:\n",
    "    case \"minerva_base\":\n",
    "        map_callback_train = minerva_base_prompt_it_it_train\n",
    "        map_callback_eval   = minerva_base_prompt_it_it_eval\n",
    "\n",
    "        hf_tokenized = DatasetDict({\n",
    "            \"train\": hf[\"train\"].map(map_callback_train, batched=True),\n",
    "            \"test\":  hf[\"test\"].map(map_callback_eval, batched=True)\n",
    "        })\n",
    "        \n",
    "        hf_tokenized.set_format(type=\"torch\", columns=['input_ids', 'attention_mask'])\n",
    "\n",
    "    \n",
    "    case _:\n",
    "        raise ValueError(\"Tokenization method not avaiable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': ['Sentence', 'Target', 'Date', 'Author', 'Region', 'input_ids', 'attention_mask'], 'test': ['Sentence', 'Target', 'Date', 'Author', 'Region', 'input_ids', 'attention_mask']}\n",
      "{'train': (87, 7), 'test': (10, 7)}\n"
     ]
    }
   ],
   "source": [
    "print(hf_tokenized.column_names)\n",
    "print(hf_tokenized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===:(sentence n¬∞1):===\n",
      "Sentence:Riscrivi 'Creti?  Certo quand'elli si mosse, elli ti dixe: \"O fedele mia donna, fa' che in mio luogo ti sia racomandato il nostro hoste troiano\".' dall'Italiano Antico a l'Italiano Moderno\n",
      " Risposta\n",
      " Creti? Con convinzione, prima di partire, ti disse: 'O mia fedele donna, fa‚Äô in modo che, per tua volont√†, il nostro ospite troiano sieda al mio posto.'\n",
      "===:(sentence n¬∞2):===\n",
      "Sentence:Riscrivi 'Officio di questa arte pare che sia dicere appostatamente per fare credere, fine √® far credere per lo dire.' dall'Italiano Antico a l'Italiano Moderno\n",
      " Risposta\n",
      " Il compito di quest‚Äôarte sembra essere quello di parlare in modo appropriato per convincere, e di far credere qualcosa attraverso le parole.\n",
      "===:(sentence n¬∞3):===\n",
      "Sentence:Riscrivi 'l' anima muta la sua forza per la propietade di quello corpo a cui ella si congiunge.' dall'Italiano Antico a l'Italiano Moderno\n",
      " Risposta\n",
      " L‚Äôanima modifica la sua forza in base alla natura del corpo al quale si unisce.\n",
      "===:(sentence n¬∞4):===\n",
      "Sentence:Riscrivi 'per tanto che Lucano il disse, il vi racontiamo. Ivi, quando l'anima di Pompeo ebe sentito la chiarit√† di lasusso, ella s√¨ cognobe prima in  grande' dall'Italiano Antico a l'Italiano Moderno\n",
      " Risposta\n",
      " Poich√© lo disse Lucano, ve lo raccontiamo. L√¨, quando l‚Äôanima di Pompeo percep√¨ la luce dell‚Äôalto, allora per la prima volta la riconobbe nella sua grandezza.\n",
      "===:(sentence n¬∞5):===\n",
      "Sentence:Riscrivi 'Vede anche le ragioni del volo degli uccielli e di tutte le cose sa rendere vero giudicio.' dall'Italiano Antico a l'Italiano Moderno\n",
      " Risposta\n",
      " Comprende anche le leggi del volo degli uccelli e sa dare un giudizio veritiero su ogni cosa.\n"
     ]
    }
   ],
   "source": [
    "for idx, s in enumerate(hf_tokenized[\"train\"].take(5), 1):\n",
    "    print(f\"===:(sentence n¬∞{idx}):===\")\n",
    "    print(f\"{SRC_L}:{tokenizer.decode(s[\"input_ids\"], attention_mask=s[\"attention_mask\"], skip_special_tokens=True)}\" )\n",
    "    #print(f\"{TRG_L}:{tokenizer.decode(s[\"labels\"], skip_special_tokens=True)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===:(sentence n¬∞1):===\n",
      "Sentence:Riscrivi 'Io spero in messer Ies√π di mandare tosto a voi Timoteo, acciocch√© io sia d'animo buono' dall'Italiano Antico a l'Italiano Moderno\n",
      " Risposta\n",
      "\n",
      "===:(sentence n¬∞2):===\n",
      "Sentence:Riscrivi 'noi iscaciati e dipartiti per debito dela cittade, e tutti iscaciati da fama e da ventura buona.' dall'Italiano Antico a l'Italiano Moderno\n",
      " Risposta\n",
      "\n",
      "===:(sentence n¬∞3):===\n",
      "Sentence:Riscrivi 'And√≤ nel campo de' Cartaginesi e tutta la legione trasse seco.' dall'Italiano Antico a l'Italiano Moderno\n",
      " Risposta\n",
      "\n",
      "===:(sentence n¬∞4):===\n",
      "Sentence:Riscrivi 'Altress√¨ uno amante chiamando merz√© alla sua donna dice parole e ragioni molte, et ella si difende in suo dire.' dall'Italiano Antico a l'Italiano Moderno\n",
      " Risposta\n",
      "\n",
      "===:(sentence n¬∞5):===\n",
      "Sentence:Riscrivi 'Ed ecco di subito tutta questa turba degli uccelli si lev√≤ a volo dietro all'aquila' dall'Italiano Antico a l'Italiano Moderno\n",
      " Risposta\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, s in enumerate(hf_tokenized[\"test\"].take(5), 1):\n",
    "    print(f\"===:(sentence n¬∞{idx}):===\")\n",
    "    print(f\"{SRC_L}:{tokenizer.decode(s[\"input_ids\"], attention_mask=s[\"attention_mask\"], skip_special_tokens=True)}\" )\n",
    "    #print(f\"{TRG_L}:{tokenizer.decode(s[\"labels\"], skip_special_tokens=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models & Traning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PEFT Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = MyTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=hf_tokenized[\"train\"],\n",
    "            eval_dataset=hf_tokenized[\"test\"],\n",
    "            processing_class=tokenizer,\n",
    "            data_collator=data_collator,\n",
    "            compute_metrics=compute_metrics,\n",
    "            callbacks=[Report(OUT_DIR), early_callback]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='77' max='1100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  77/1100 00:57 < 13:07, 1.30 it/s, Epoch 7/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Meteor</th>\n",
       "      <th>Chrf++</th>\n",
       "      <th>Ter</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.813600</td>\n",
       "      <td>3.558442</td>\n",
       "      <td>1.354700</td>\n",
       "      <td>0.247800</td>\n",
       "      <td>0.061200</td>\n",
       "      <td>0.193900</td>\n",
       "      <td>0.217900</td>\n",
       "      <td>0.129600</td>\n",
       "      <td>30.576400</td>\n",
       "      <td>93.723800</td>\n",
       "      <td>280.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.903900</td>\n",
       "      <td>2.762877</td>\n",
       "      <td>19.369200</td>\n",
       "      <td>0.436700</td>\n",
       "      <td>0.306200</td>\n",
       "      <td>0.394500</td>\n",
       "      <td>0.419300</td>\n",
       "      <td>0.365500</td>\n",
       "      <td>45.801300</td>\n",
       "      <td>74.895400</td>\n",
       "      <td>280.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.631900</td>\n",
       "      <td>2.671443</td>\n",
       "      <td>18.597400</td>\n",
       "      <td>0.444600</td>\n",
       "      <td>0.308900</td>\n",
       "      <td>0.404300</td>\n",
       "      <td>0.432000</td>\n",
       "      <td>0.365600</td>\n",
       "      <td>45.124900</td>\n",
       "      <td>75.313800</td>\n",
       "      <td>280.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.488200</td>\n",
       "      <td>2.653488</td>\n",
       "      <td>18.299000</td>\n",
       "      <td>0.439200</td>\n",
       "      <td>0.308100</td>\n",
       "      <td>0.398500</td>\n",
       "      <td>0.424100</td>\n",
       "      <td>0.352100</td>\n",
       "      <td>44.247900</td>\n",
       "      <td>75.313800</td>\n",
       "      <td>280.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.383700</td>\n",
       "      <td>2.644812</td>\n",
       "      <td>20.093700</td>\n",
       "      <td>0.449700</td>\n",
       "      <td>0.309900</td>\n",
       "      <td>0.406500</td>\n",
       "      <td>0.427600</td>\n",
       "      <td>0.382000</td>\n",
       "      <td>45.804600</td>\n",
       "      <td>74.895400</td>\n",
       "      <td>280.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.277700</td>\n",
       "      <td>2.674479</td>\n",
       "      <td>20.314300</td>\n",
       "      <td>0.471200</td>\n",
       "      <td>0.299500</td>\n",
       "      <td>0.433400</td>\n",
       "      <td>0.456900</td>\n",
       "      <td>0.407100</td>\n",
       "      <td>46.955400</td>\n",
       "      <td>70.711300</td>\n",
       "      <td>280.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.161300</td>\n",
       "      <td>2.663690</td>\n",
       "      <td>19.984600</td>\n",
       "      <td>0.465100</td>\n",
       "      <td>0.302500</td>\n",
       "      <td>0.433400</td>\n",
       "      <td>0.454900</td>\n",
       "      <td>0.405700</td>\n",
       "      <td>46.590200</td>\n",
       "      <td>71.966500</td>\n",
       "      <td>280.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training done. Generating graphs...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=77, training_loss=2.6657403475278385, metrics={'train_runtime': 57.9904, 'train_samples_per_second': 150.025, 'train_steps_per_second': 18.969, 'total_flos': 962186705141760.0, 'train_loss': 2.6657403475278385, 'epoch': 7.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inizio generazione su cuda\n",
      "=============================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ec6d718e305471689db23469be1f341",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===:(Model for Prompt)===\n",
      "Riscrivi 'Io spero in messer Ies√π di mandare tosto a voi Timoteo, acciocch√© io sia d'animo buono' dall'Italiano Antico a l'Italiano Moderno\n",
      " Risposta\n",
      "\n",
      "=========================\n",
      "===:(model sapienzanlp/Minerva-1B-base-v1.0):===\n",
      "Riscrivi 'Io spero in messer Ies√π di mandare tosto a voi Timoteo, acciocch√© io sia d'animo buono' dall'Italiano Antico a l'Italiano Moderno\n",
      " Risposta\n",
      " Spero che il Signore Ges√π, il quale √® venuto per salvare il mondo, mandi presto a voi Timoteo, affinch√© io possa essere un buon cristiano. 100000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "=================================\n",
      "===:(Model for Prompt)===\n",
      "Riscrivi 'noi iscaciati e dipartiti per debito dela cittade, e tutti iscaciati da fama e da ventura buona.' dall'Italiano Antico a l'Italiano Moderno\n",
      " Risposta\n",
      "\n",
      "=========================\n",
      "===:(model sapienzanlp/Minerva-1B-base-v1.0):===\n",
      "Riscrivi 'noi iscaciati e dipartiti per debito dela cittade, e tutti iscaciati da fama e da ventura buona.' dall'Italiano Antico a l'Italiano Moderno\n",
      " Risposta\n",
      " Noi, cittadini e cittadini, e tutti coloro che sono stati costretti a lasciare la citt√† per la guerra, e tutti coloro che sono stati costretti a lasciare la citt√† per la guerra. 10000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "=================================\n",
      "===:(Model for Prompt)===\n",
      "Riscrivi 'And√≤ nel campo de' Cartaginesi e tutta la legione trasse seco.' dall'Italiano Antico a l'Italiano Moderno\n",
      " Risposta\n",
      "\n",
      "=========================\n",
      "===:(model sapienzanlp/Minerva-1B-base-v1.0):===\n",
      "Riscrivi 'And√≤ nel campo de' Cartaginesi e tutta la legione trasse seco.' dall'Italiano Antico a l'Italiano Moderno\n",
      " Risposta\n",
      " And√≤ nel campo dei Cartaginesi e tutta la legione trasse seco. 10000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "=================================\n",
      "===:(Model for Prompt)===\n",
      "Riscrivi 'Altress√¨ uno amante chiamando merz√© alla sua donna dice parole e ragioni molte, et ella si difende in suo dire.' dall'Italiano Antico a l'Italiano Moderno\n",
      " Risposta\n",
      "\n",
      "=========================\n",
      "===:(model sapienzanlp/Minerva-1B-base-v1.0):===\n",
      "Riscrivi 'Altress√¨ uno amante chiamando merz√© alla sua donna dice parole e ragioni molte, et ella si difende in suo dire.' dall'Italiano Antico a l'Italiano Moderno\n",
      " Risposta\n",
      " Altrimenti uno amante chiamando la sua donna dice cose e ragioni molte, e tu rispondi in tuo dire. 10000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "=================================\n",
      "===:(Model for Prompt)===\n",
      "Riscrivi 'Ed ecco di subito tutta questa turba degli uccelli si lev√≤ a volo dietro all'aquila' dall'Italiano Antico a l'Italiano Moderno\n",
      " Risposta\n",
      "\n",
      "=========================\n",
      "===:(model sapienzanlp/Minerva-1B-base-v1.0):===\n",
      "Riscrivi 'Ed ecco di subito tutta questa turba degli uccelli si lev√≤ a volo dietro all'aquila' dall'Italiano Antico a l'Italiano Moderno\n",
      " Risposta\n",
      " Ed ecco che la turba degli uccelli si lev√≤ a volo dietro all‚Äôaquila.\n",
      " Risposta\n",
      " Ed ecco che la turba degli uccelli si lev√≤ a volo dietro all‚Äôaquila.\n",
      " Risposta\n",
      " Ed ecco che la turba degli uccelli si lev√≤ a volo dietro all‚Äôaquila.\n",
      " Risposta\n",
      " Ed ecco che la turba degli uccelli si lev√≤ a volo dietro all‚Äôaquila.\n",
      " Risposta\n",
      " Ed ecco che la turba degli uccelli si lev√≤ a volo dietro all‚Äôaquila.\n",
      " Risposta\n",
      " Ed ecco che la turba degli uccelli si lev√≤ a volo dietro all‚Äôaquila.\n",
      " Risposta\n",
      " Ed ecco che la turba degli uccelli si lev√≤ a volo dietro all‚Äôaquila.\n",
      " Risposta\n",
      " Ed ecco che la turba degli uccelli si lev√≤ a volo dietro all‚Äôaquila.\n",
      " Risposta\n",
      " Ed ecco che la turba degli uccelli si lev√≤ a volo dietro all‚Äôaquila.\n",
      " Risposta\n",
      " Ed ecco che la turba degli uccelli si lev√≤ a volo dietro all‚Äôaquila.\n",
      " Risposta\n",
      " Ed ecco che la turba degli uccelli si lev√≤ a volo dietro all‚Äôaquila.\n",
      " Risposta\n",
      " Ed ecco che la turba degli uccelli si lev√≤ a volo dietro all‚Äôaquila.\n",
      " Risposta\n",
      " Ed ecco che la turba degli uccelli si lev√≤ a volo dietro all‚Äô\n",
      "=================================\n",
      "===:(Model for Prompt)===\n",
      "Riscrivi 'Non d' altra forza d' animo fue ornato Ponzio Aufidiano, romano cavaliere.' dall'Italiano Antico a l'Italiano Moderno\n",
      " Risposta\n",
      "\n",
      "=========================\n",
      "===:(model sapienzanlp/Minerva-1B-base-v1.0):===\n",
      "Riscrivi 'Non d' altra forza d' animo fue ornato Ponzio Aufidiano, romano cavaliere.' dall'Italiano Antico a l'Italiano Moderno\n",
      " Risposta\n",
      " Non c‚Äô √® altra forza d‚Äôanimo che quella di un cuore puro.\n",
      " Risposta\n",
      " Non si pu√≤ dire che l‚Äôanimo di un uomo sia stato ornato da un corpo di marmo.\n",
      " Risposta\n",
      " Non si pu√≤ dire che l‚Äôanimo di un uomo sia stato ornato da un corpo di marmo. 10000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "=================================\n",
      "===:(Model for Prompt)===\n",
      "Riscrivi 'Ma l' occhio della intelligenza √® pi√π alto. Perci√≤ che, passata la grandezza della universitade, quella medesima semplice forma vede nella sottil vista' dall'Italiano Antico a l'Italiano Moderno\n",
      " Risposta\n",
      "\n",
      "=========================\n",
      "===:(model sapienzanlp/Minerva-1B-base-v1.0):===\n",
      "Riscrivi 'Ma l' occhio della intelligenza √® pi√π alto. Perci√≤ che, passata la grandezza della universitade, quella medesima semplice forma vede nella sottil vista' dall'Italiano Antico a l'Italiano Moderno\n",
      " Risposta\n",
      " L‚Äôocchio della mente √® pi√π alto della vista. Perci√≤, che la mente umana sia pi√π alta, √® un‚Äôopinione comune.\n",
      " Risposta\n",
      " L‚Äôocchio della mente √® pi√π alto della vista. Perci√≤, che la mente umana sia pi√π alta, √® un‚Äôopinione comune. 10000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "=================================\n",
      "===:(Model for Prompt)===\n",
      "Riscrivi 'cose ch'io sapeva che erano fatte in Italia.' dall'Italiano Antico a l'Italiano Moderno\n",
      " Risposta\n",
      "\n",
      "=========================\n",
      "===:(model sapienzanlp/Minerva-1B-base-v1.0):===\n",
      "Riscrivi 'cose ch'io sapeva che erano fatte in Italia.' dall'Italiano Antico a l'Italiano Moderno\n",
      " Risposta\n",
      " Cose che sapevano che erano fatte in Italia.\n",
      " Risposta\n",
      " Cose che sapevano che erano fatte in Italia.\n",
      " Riscrivi 'che sapeva che erano fatte in Italia.' dall'Italiano Antico a l'Italiano Moderno 10000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "=================================\n",
      "===:(Model for Prompt)===\n",
      "Riscrivi 'Vero √®, ma non tine rispondo in questo tempo, perci√≤ che ttu se' mio servo, o perci√≤ ch' √® tempo feriato, o perci√≤ ch' io non debbo risponderti' dall'Italiano Antico a l'Italiano Moderno\n",
      " Risposta\n",
      "\n",
      "=========================\n",
      "===:(model sapienzanlp/Minerva-1B-base-v1.0):===\n",
      "Riscrivi 'Vero √®, ma non tine rispondo in questo tempo, perci√≤ che ttu se' mio servo, o perci√≤ ch' √® tempo feriato, o perci√≤ ch' io non debbo risponderti' dall'Italiano Antico a l'Italiano Moderno\n",
      " Risposta\n",
      " √à vero, ma non tengo conto di questo tempo, perci√≤ che non posso rispondere.\n",
      " Risposta\n",
      " √à vero, ma non tengo conto di questo tempo, perci√≤ che non posso rispondere.\n",
      " Antico\n",
      " Italiano A-Latino\n",
      "Latino Antico\n",
      "Italiano Moderno\n",
      "Italiano Antico\n",
      "Italiano Medio\n",
      "Italiano Antico\n",
      "Italiano Moderno\n",
      "Italiano Scientifico\n",
      "Italiano Scientifico\n",
      "Italiano Scientifico\n",
      "Italiano Scientifico\n",
      "Italiano Scientifico\n",
      "Italiano Scientifico\n",
      "Italiano Scientifico\n",
      "Italiano Scientifico\n",
      "Italiano Scientifico\n",
      "Italiano Scientifico\n",
      "Italiano Scientifico\n",
      "Italiano Scientifico\n",
      "Italiano Scientifico\n",
      "Italiano Scientifico\n",
      "Italiano Scientifico\n",
      "Italiano Scientifico\n",
      "Italiano Scientifico\n",
      "Italiano Scientifico\n",
      "Italiano Scientifico\n",
      "Italiano Scientifico\n",
      "Italiano Scientifico\n",
      "Italiano Scientifico\n",
      "Italiano Scientifico\n",
      "Italiano Scientifico\n",
      "Italiano Scientifico\n",
      "Italiano Scientifico\n",
      "Italiano Scientifico\n",
      "Italiano Scientifico\n",
      "Italiano Scientifico\n",
      "Italiano Scientifico\n",
      "Italiano Scientifico\n",
      "Italiano Scientifico\n",
      "Italiano Scientifico\n",
      "Italiano Scientifico\n",
      "Italiano Scientifico\n",
      "Italiano Scientifico\n",
      "Italiano Scientifico\n",
      "Italiano Scientifico\n",
      "Italiano Scientifico\n",
      "Italiano Scientifico\n",
      "Italiano\n",
      "=================================\n",
      "===:(Model for Prompt)===\n",
      "Riscrivi 'per√≤ che, sse nobile cosa e alta √® abatte il nimico, ampoi nonn √® meno laudabile sapere avere misiricordia' dall'Italiano Antico a l'Italiano Moderno\n",
      " Risposta\n",
      "\n",
      "=========================\n",
      "===:(model sapienzanlp/Minerva-1B-base-v1.0):===\n",
      "Riscrivi 'per√≤ che, sse nobile cosa e alta √® abatte il nimico, ampoi nonn √® meno laudabile sapere avere misiricordia' dall'Italiano Antico a l'Italiano Moderno\n",
      " Risposta\n",
      " Per√≤, nobile cosa e alta √® la fama, cos√¨ anche il coraggio e la virt√π sono pi√π nobili. 100000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "=================================\n",
      "\n",
      "Generazione completata.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Imposta il modello in modalit√† valutazione e spostalo sul device\n",
    "model = model.eval()\n",
    "model = model.to(device)\n",
    "\n",
    "# Crea il DataLoader\n",
    "loader = torch.utils.data.DataLoader(hf_tokenized[\"test\"], batch_size=8)\n",
    "\n",
    "\n",
    "print(f\"Inizio generazione su {device}\")\n",
    "print(\"=============================\")\n",
    "\n",
    "for batch in tqdm(loader):\n",
    "    # Sposta l'intero batch sul device\n",
    "    # Nota: DataLoader restituisce un batch come dizionario di tensori\n",
    "    \n",
    "    batch[\"input_ids\"] = batch[\"input_ids\"].to(device)\n",
    "    batch[\"attention_mask\"] = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "    # Genera l'output per il batch\n",
    "    # Input al generate devono essere input_ids e attention_mask\n",
    "    result_ids = model.generate(\n",
    "        input_ids=batch[\"input_ids\"],\n",
    "        attention_mask=batch[\"attention_mask\"],\n",
    "        max_new_tokens=60,\n",
    "    )\n",
    "\n",
    "    # Decodifica *separatamente* ogni prompt e ogni risultato generato\n",
    "    # Iteriamo sul batch per decodificare uno per uno\n",
    "    # batch[\"input_ids\"] ha forma (batch_size, seq_len_input)\n",
    "    # result_ids ha forma (batch_size, seq_len_output)\n",
    "    \n",
    "    # Decodifica i prompt originali\n",
    "    decoded_prompts = tokenizer.batch_decode(batch[\"input_ids\"], skip_special_tokens=True)\n",
    "    \n",
    "    # Decodifica i risultati generati\n",
    "    decoded_results = tokenizer.batch_decode(result_ids, skip_special_tokens=True)\n",
    "\n",
    "    # Stampa i risultati per ogni elemento del batch\n",
    "    with open(OUT_DIR + \"/output_chat.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for i in range(len(decoded_prompts)):\n",
    "            print(\"===:(Model for Prompt)===\")\n",
    "            print(f\"{decoded_prompts[i]}\")\n",
    "            print(\"=========================\")\n",
    "            \n",
    "            print(f\"===:(model {network}):===\")\n",
    "            print(decoded_results[i])\n",
    "            print(\"=================================\")\n",
    "            \n",
    "            ############################################\n",
    "\n",
    "\n",
    "            f.write(\"===:(Model for Prompt)===\")\n",
    "            f.write(f\"{decoded_prompts[i]}\")\n",
    "            f.write(\"=========================\")\n",
    "\n",
    "            f.write(f\"===:(model {network}):===\")\n",
    "            f.write(decoded_results[i])\n",
    "            f.write(\"==========================\")\n",
    "\n",
    "print(\"\\nGenerazione completata.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MNLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
