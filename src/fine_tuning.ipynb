{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translation from Ancient to Modern Italian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Datases to work with Transformers by Hugging-Face\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "# Imports for Transformers\n",
    "from transformers import AutoTokenizer  # Datasets\n",
    "from datasets import Dataset, DatasetDict\n",
    "from utils import Report\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling, Trainer, TrainingArguments, AutoModelForCausalLM          # imports for causal Learning\n",
    "from transformers import DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer, AutoModelForSeq2SeqLM    # imports for Seq2Seq models\n",
    "from peft import LoraConfig, TaskType, LoftQConfig, PeftModelForSeq2SeqLM, PeftModelForCausalLM, get_peft_model     # imports for quantization methods (LoRA etc...)\n",
    "from transformers import EarlyStoppingCallback  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Promposed Models\n",
    "* google/gemma-3-1b-it (LLM) üöÄ\n",
    "* mistralai/Mistral-7B-Instruct-v0.2\n",
    "* sapienzanlp/Minerva-1B-base-v1.0 üáÆüáπ (LMM)\n",
    "* Helsinki-NLP/opus-mt-itc-itc (Machine Translation) üèÜ - use OpusPrompt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = ('cuda' if torch.cuda.is_available() else \"cpu\")\n",
    "DATASET = \"dataset_ann.csv\"\n",
    "SRC_L = \"Sentence\"\n",
    "TRG_L = \"Target\"\n",
    "network = \"sapienzanlp/Minerva-1B-base-v1.0\"\n",
    "tokenization_method = \"minerva_base\"\n",
    "OUT_DIR = network.split(\"/\")[-1]\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 8\n",
    "max_length = 120"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATASET, sep=\",\", index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length mean Sentence text: 20.04123711340206\n",
      "length mean Target text: 20.690721649484537\n"
     ]
    }
   ],
   "source": [
    "print(f\"length mean {SRC_L} text: {df[SRC_L].apply(lambda x: len(x.split())).mean()}\")\n",
    "print(f\"length mean {TRG_L} text: {df[TRG_L].apply(lambda x: len(x.split())).mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Date</th>\n",
       "      <th>Region</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Brunetto Latini</td>\n",
       "      <td>1260-61</td>\n",
       "      <td>fior.</td>\n",
       "      <td>quella guerra ben fatta l' opera perch√© etc. E...</td>\n",
       "      <td>Quella guerra fu ben condotta per via delle az...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bono Giamboni</td>\n",
       "      <td>1292</td>\n",
       "      <td>fior.</td>\n",
       "      <td>crudele, e di tutte le colpe pigli vendetta, c...</td>\n",
       "      <td>√à severo, e punisce tutte le colpe come prescr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Valerio Massimo (red. V1</td>\n",
       "      <td>1336</td>\n",
       "      <td>fior.</td>\n",
       "      <td>Non d' altra forza d' animo fue ornato Ponzio ...</td>\n",
       "      <td>Ponzio Aufidiano, cavaliere romano, fu dotato ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lucano volg. (ed. Marinoni)</td>\n",
       "      <td>1330/40</td>\n",
       "      <td>prat.</td>\n",
       "      <td>Se questo piace a tutti e se 'l tempo hae biso...</td>\n",
       "      <td>Se questo √® quello che tutti desiderano e se l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Brunetto Latini</td>\n",
       "      <td>1260-61</td>\n",
       "      <td>fior.</td>\n",
       "      <td>Officio di questa arte pare che sia dicere app...</td>\n",
       "      <td>Il compito di quest‚Äôarte sembra essere quello ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Author     Date Region  \\\n",
       "0              Brunetto Latini  1260-61  fior.   \n",
       "1                Bono Giamboni     1292  fior.   \n",
       "2     Valerio Massimo (red. V1     1336  fior.   \n",
       "3  Lucano volg. (ed. Marinoni)  1330/40  prat.   \n",
       "4              Brunetto Latini  1260-61  fior.   \n",
       "\n",
       "                                            Sentence  \\\n",
       "0  quella guerra ben fatta l' opera perch√© etc. E...   \n",
       "1  crudele, e di tutte le colpe pigli vendetta, c...   \n",
       "2  Non d' altra forza d' animo fue ornato Ponzio ...   \n",
       "3  Se questo piace a tutti e se 'l tempo hae biso...   \n",
       "4  Officio di questa arte pare che sia dicere app...   \n",
       "\n",
       "                                              Target  \n",
       "0  Quella guerra fu ben condotta per via delle az...  \n",
       "1  √à severo, e punisce tutte le colpe come prescr...  \n",
       "2  Ponzio Aufidiano, cavaliere romano, fu dotato ...  \n",
       "3  Se questo √® quello che tutti desiderano e se l...  \n",
       "4  Il compito di quest‚Äôarte sembra essere quello ...  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 851,968 || all params: 1,007,552,512 || trainable%: 0.0846\n"
     ]
    }
   ],
   "source": [
    "# Switch to select the network and load the appropriate model and tokenizer\n",
    "match network:\n",
    "    \n",
    "    case \"sapienzanlp/Minerva-1B-base-v1.0\" | \"sapienzanlp/Minerva-7B-base-v1.0\":\n",
    "        \n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(network)\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token \n",
    "\n",
    "        \n",
    "        model = AutoModelForCausalLM.from_pretrained(network, device_map=device, torch_dtype=torch.float16)\n",
    "        data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "        \n",
    "        lora_config = LoraConfig(\n",
    "            task_type=TaskType.CAUSAL_LM, \n",
    "            inference_mode=False, \n",
    "            r=8, \n",
    "            lora_alpha=16, \n",
    "            lora_dropout=0.65\n",
    "            )\n",
    "\n",
    "        qlora_config = LoraConfig(\n",
    "            init_lora_weights=\"loftq\",\n",
    "            loftq_config=LoftQConfig(loftq_bits=4),\n",
    "            r=8,\n",
    "            lora_alpha=16,\n",
    "            target_modules=\"all-linear\",\n",
    "            lora_dropout=0.5,\n",
    "            bias=\"none\",\n",
    "            task_type=\"CAUSAL_LM\"\n",
    "        )\n",
    "\n",
    "        model = get_peft_model(model, lora_config)\n",
    "        model.print_trainable_parameters()\n",
    "\n",
    "        \n",
    "\n",
    "        early_stopping_patience = 10\n",
    "        early_stopping_threshold = 0.01 \n",
    "\n",
    "        early_callback = EarlyStoppingCallback(\n",
    "            early_stopping_patience=early_stopping_patience, # Se la loss di valutazione non migliora per 3 epoche consecutive\n",
    "            early_stopping_threshold=early_stopping_threshold # Ignora miglioramenti inferiori a 0.001\n",
    "        )\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=OUT_DIR,\n",
    "            learning_rate=6e-5,\n",
    "            weight_decay=1e-4,\n",
    "            warmup_steps=80,\n",
    "            per_device_train_batch_size=BATCH_SIZE,\n",
    "            per_device_eval_batch_size=BATCH_SIZE,\n",
    "            num_train_epochs=EPOCHS,\n",
    "            eval_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            logging_strategy=\"epoch\",\n",
    "            load_best_model_at_end=True,\n",
    "            report_to=\"none\",\n",
    "            save_total_limit=3,\n",
    "            lr_scheduler_type=\"cosine\",\n",
    "            logging_dir=OUT_DIR,\n",
    "            logging_steps=10,\n",
    "            label_names=['labels'],\n",
    "            metric_for_best_model=\"eval_chrf++\", \n",
    "            greater_is_better=True,\n",
    "        )\n",
    "\n",
    "        params = {\n",
    "            \n",
    "            \"max_new_tokens\": max_length, # max number of new tokens to generate\n",
    "            \"do_sample\":True,      # enables sampling for more diverse outputs\n",
    "            \"top_k\":100,            # diversity increase by controlling the candidate words\n",
    "            \"top_p\":0.95,          # nucleus sampling for further control over variety\n",
    "            \"temperature\":1.0,     # reduces randomness and increases coherence\n",
    "            \"repetition_penalty\":1.0,  # penalizza ripetizioni\n",
    "            \"num_return_sequences\":10,  # number of generated responses\n",
    "            \"pad_token_id\":tokenizer.eos_token_id  # avoids warning if padding token is missing\n",
    "        }\n",
    "\n",
    "    case _:\n",
    "        raise Exception(f\"Rete {network} non testabile\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTrainerSeq2Seq(Seq2SeqTrainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        if 'num_items_in_batch' in inputs:\n",
    "            inputs = {k: v for k, v in inputs.items() if k != 'num_items_in_batch'}\n",
    "        return super().compute_loss(model, inputs, return_outputs=return_outputs)\n",
    "\n",
    "class MyTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        if 'num_items_in_batch' in inputs:\n",
    "            inputs = {k: v for k, v in inputs.items() if k != 'num_items_in_batch'}\n",
    "        return super().compute_loss(model, inputs, return_outputs=return_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/andrea/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/andrea/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/andrea/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "sacrebleu_metric = evaluate.load(\"sacrebleu\")\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "meteor_metric = evaluate.load(\"meteor\")\n",
    "chrf_metric = evaluate.load(\"chrf\")\n",
    "ter_metric = evaluate.load(\"ter\")\n",
    "\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels] # Specific format for SacreBLEU\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds_input, label_ids = eval_preds\n",
    "\n",
    "    # Dealing with logits or token IDs for predictions\n",
    "    # If preds_input are logits (es. direct output of training modello)\n",
    "    current_preds = preds_input\n",
    "    if isinstance(current_preds, tuple): # Common in HF Trainer, es. (logits, hidden_states)\n",
    "        current_preds = current_preds[0]\n",
    "    \n",
    "    if hasattr(current_preds, \"ndim\") and current_preds.ndim == 3: # Array of logits (batch_size, seq_len, vocab_size)\n",
    "        current_preds_ids = np.argmax(current_preds, axis=-1)\n",
    "    else: # Otherwise, assumed to be token ID (batch_size, seq_len)\n",
    "        current_preds_ids = current_preds\n",
    "\n",
    "    # Decode predictions and labels\n",
    "    decoded_preds_raw = tokenizer.batch_decode(current_preds_ids, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100 in labels (common for token to be ignored) with pad_token_id for decoding\n",
    "    processed_label_ids = np.where(label_ids != -100, label_ids, tokenizer.pad_token_id)\n",
    "    decoded_labels_raw = tokenizer.batch_decode(processed_label_ids, skip_special_tokens=True)\n",
    "    \n",
    "    processed_preds, processed_labels_for_sacrebleu = postprocess_text(decoded_preds_raw, decoded_labels_raw)\n",
    "\n",
    "    # For other metrics (ROUGE, METEOR, CHRF, TER), usually expects a flat list of reference strings\n",
    "    flat_references = [ref[0] for ref in processed_labels_for_sacrebleu]\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # 1. SacreBLEU\n",
    "    sacrebleu_output = sacrebleu_metric.compute(predictions=processed_preds, references=processed_labels_for_sacrebleu)\n",
    "    if sacrebleu_output and \"score\" in sacrebleu_output:\n",
    "        results[\"bleu\"] = sacrebleu_output[\"score\"]\n",
    "    else:\n",
    "        results[\"bleu\"] = 0.0 # Fallback\n",
    "\n",
    "    # 2. ROUGE (rouge1, rouge2, rougeL, rougeLsum)\n",
    "    rouge_output = rouge_metric.compute(predictions=processed_preds, references=flat_references, use_stemmer=True)\n",
    "    if rouge_output:\n",
    "        results[\"rouge1\"] = rouge_output.get(\"rouge1\", 0.0)\n",
    "        results[\"rouge2\"] = rouge_output.get(\"rouge2\", 0.0)\n",
    "        results[\"rougeL\"] = rouge_output.get(\"rougeL\", 0.0)\n",
    "        results[\"rougeLsum\"] = rouge_output.get(\"rougeLsum\", 0.0) # Spesso pi√π robusto per sommario\n",
    "    else:\n",
    "        results.update({\"rouge1\": 0.0, \"rouge2\": 0.0, \"rougeL\": 0.0, \"rougeLsum\": 0.0})\n",
    "\n",
    "    # 3. METEOR\n",
    "    meteor_output = meteor_metric.compute(predictions=processed_preds, references=flat_references)\n",
    "    if meteor_output and \"meteor\" in meteor_output:\n",
    "        results[\"meteor\"] = meteor_output[\"meteor\"]\n",
    "    else:\n",
    "        results[\"meteor\"] = 0.0\n",
    "\n",
    "    # 4. CHRF++ (CHRF with n-grams of words)\n",
    "    # For CHRF++, word_order (or word_n) is > 0. Default of evaluate.load('chrf') are word_order=0 (CHRF standard).\n",
    "    # Common parameters for CHRF++: word_order=2, beta=2 (beta=2 default)\n",
    "    chrf_output = chrf_metric.compute(predictions=processed_preds, references=flat_references, word_order=2, beta=2)\n",
    "    if chrf_output and \"score\" in chrf_output:\n",
    "        results[\"chrf++\"] = chrf_output[\"score\"] # CHRF++ score\n",
    "    else:\n",
    "        results[\"chrf++\"] = 0.0\n",
    "        \n",
    "    # (Optional) CHRF standard (only characters)\n",
    "    # chrf_std_output = chrf_metric.compute(predictions=processed_preds, references=flat_references, word_order=0)\n",
    "    # if chrf_std_output and \"score\" in chrf_std_output:\n",
    "    #     results[\"chrf\"] = chrf_std_output[\"score\"]\n",
    "    # else:\n",
    "    #     results[\"chrf\"] = 0.0\n",
    "\n",
    "    # 5. TER (Translation Edit Rate) - the smaller, the better\n",
    "    ter_output = ter_metric.compute(predictions=processed_preds, references=flat_references)\n",
    "    if ter_output and \"score\" in ter_output:\n",
    "        results[\"ter\"] = ter_output[\"score\"]\n",
    "    else:\n",
    "        results[\"ter\"] = 1.0 # Fallback on worst score TER possible\n",
    "\n",
    "    # Mean length of generated predictions (excluding padding tokens)\n",
    "    # 'current_preds_ids' are ID token of the predictions\n",
    "    prediction_lengths = [np.count_nonzero(pid_seq != tokenizer.pad_token_id) for pid_seq in current_preds_ids]\n",
    "    results[\"gen_len\"] = np.mean(prediction_lengths) if prediction_lengths else 0.0\n",
    "\n",
    "    # Rounding of all numerical results\n",
    "    final_results = {k: round(v, 4) for k, v in results.items() if isinstance(v, (int, float))}\n",
    "    \n",
    "    return final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.features import Value, Features\n",
    "hf = Dataset.from_csv(DATASET, features=\n",
    "    Features({\n",
    "        SRC_L : Value(\"string\"),\n",
    "        TRG_L : Value(\"string\"),\n",
    "        \"Date\": Value(\"string\"),\n",
    "        \"Author\":Value(\"string\"),\n",
    "        \"Region\":Value(\"string\")\n",
    "    })          \n",
    "                      \n",
    "    ).shuffle(2025).train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model max length: 120\n"
     ]
    }
   ],
   "source": [
    "print(f\"model max length: {max_length}\")\n",
    "\n",
    "\n",
    "def noprompt_it_it(examples):\n",
    "    inputs = [example for example in examples[SRC_L]]\n",
    "    targets = [example for example in examples[TRG_L]]\n",
    "    \n",
    "    model_inputs = tokenizer(inputs, text_target=targets, max_length=max_length, truncation=True, padding=\"max_length\")\n",
    "    return model_inputs\n",
    "\n",
    "# I. Riscrivi\n",
    "# II. Traduci\n",
    "# III. Correggi\n",
    "def minerva_base_prompt_it_it_train(examples):\n",
    "\n",
    "    prompts = [\n",
    "        f\"\"\"Riscrivi la seguente frase {src} scritta in {dia} italiano arcaico del {dat} in Italiano moderno: {dst}\"\"\" \n",
    "        for src, dst, dat, dia in zip(examples[SRC_L], examples[TRG_L], examples[\"Date\"], examples[\"Region\"])\n",
    "    ] \n",
    "\n",
    "    # Tokenizza input+target e crea label con gli stessi token\n",
    "    model_inputs = tokenizer(\n",
    "        prompts,\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    \n",
    "    model_inputs[\"labels\"] = [\n",
    "        [(label if label != tokenizer.pad_token_id else -100) for label in input_ids]\n",
    "        for input_ids in model_inputs[\"input_ids\"]\n",
    "    ]\n",
    "    return model_inputs\n",
    "    \n",
    "def minerva_base_prompt_it_it_eval(examples):\n",
    "    prompts = [\n",
    "        \n",
    "        f\"\"\"Riscrivi la seguente frase {src} scritta in Italiano arcaico in Italiano moderno: \"\"\"\n",
    "        for src in examples[SRC_L]\n",
    "    ]\n",
    "\n",
    "    # Tokenizza input+target e crea label con gli stessi token\n",
    "    model_inputs = tokenizer(\n",
    "        prompts,\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "   \n",
    "    return model_inputs\n",
    "\n",
    "def base_prompt_it_it(examples):\n",
    "    inputs = [\"Riscrivi dall'Italiano Antico a l'Italiano Moderno: \" + example for example in examples[SRC_L]]\n",
    "\n",
    "    # Tokenizza solo gli input\n",
    "    model_inputs = tokenizer(inputs, max_length=max_length, truncation=True, padding=\"max_length\")\n",
    "    targets = [example for example in examples[TRG_L]]\n",
    "\n",
    "    model_inputs = tokenizer(inputs, text_target=targets, max_length=max_length, truncation=True, padding=\"longest\")\n",
    "    \n",
    "    return model_inputs\n",
    "\n",
    "def parafrasi_prompt_it_it(examples):\n",
    "    inputs = [\"Scrivi la parafrasi di questo testo: \" + example for example in examples[SRC_L]]\n",
    "    targets = [example for example in examples[TRG_L]]\n",
    "    \n",
    "    model_inputs = tokenizer(inputs, text_target=targets, max_length=max_length, truncation=True, padding=\"longest\")\n",
    "    return model_inputs\n",
    "\n",
    "def informative_prompt_it_it(examples):\n",
    "    inputs = [f\"Riscrivi in uno stile pi√π moderno il testo del seguente Autore: '{author}', anno di scrittura: {date}, luogo: Italia, dialetto: '{region}', testo: '{text}'.\" for text, date, region, author in zip(examples[SRC_L], examples[\"Date\"], examples[\"Region\"], examples[\"Author\"]) ]\n",
    "    targets = [example for example in examples[TRG_L]]\n",
    "    \n",
    "    model_inputs = tokenizer(inputs, text_target=targets, max_length=max_length, truncation=True, padding=\"longest\")\n",
    "    return model_inputs\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eda73ed5a25c41afa96110bf7f2124d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "match tokenization_method:\n",
    "    case \"minerva_base\":\n",
    "        map_callback_train = minerva_base_prompt_it_it_train\n",
    "        map_callback_eval   = minerva_base_prompt_it_it_eval\n",
    "\n",
    "        hf_tokenized = DatasetDict({\n",
    "            \"train\": hf[\"train\"].map(map_callback_train, batched=True),\n",
    "            \"test\":  hf[\"test\"].map(map_callback_eval, batched=True)\n",
    "        })\n",
    "        \n",
    "        hf_tokenized.set_format(type=\"torch\", columns=['input_ids', 'attention_mask'])\n",
    "\n",
    "    \n",
    "    case _:\n",
    "        raise ValueError(\"Tokenization method not avaiable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': ['Sentence', 'Target', 'Date', 'Author', 'Region', 'input_ids', 'attention_mask', 'labels'], 'test': ['Sentence', 'Target', 'Date', 'Author', 'Region', 'input_ids', 'attention_mask']}\n",
      "{'train': (87, 8), 'test': (10, 7)}\n"
     ]
    }
   ],
   "source": [
    "print(hf_tokenized.column_names)\n",
    "print(hf_tokenized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===:(sentence n¬∞1):===\n",
      "Sentence:Riscrivi la seguente frase Se questo piace a tutti e se 'l tempo hae bisogno d'avere Pompeio per cavaliere e non per compagno, non riterr√≤ pi√π i fati. scritta in prat. italiano arcaico del 1330/40 in Italiano moderno: Se questo √® quello che tutti desiderano e se l'attuale situazione richiede che Pompeo sia un leader e non solo un alleato, allora non cercher√≤ pi√π di oppormi al destino.\n",
      "===:(sentence n¬∞2):===\n",
      "Sentence:Riscrivi la seguente frase Unde gli poeti, parlando de lloro, dicono le virtute loro e dicono fabolosamente gli loro difetti, quando alcuna passava l'ordine a lloro deputato scritta in umbr.-tosc. italiano arcaico del 1375-77 in Italiano moderno: Perci√≤ i poeti, quando parlano di loro, ne esaltano le virt√π e, in modo allegorico o fittizio, ne descrivono i difetti, specialmente quando qualcuno di essi superava i limiti stabiliti dal proprio ruolo.\n",
      "===:(sentence n¬∞3):===\n",
      "Sentence:Riscrivi la seguente frase ne sal√¨o in su l'argine del fosso, e in su lo steccato, se da alto si potessero difendere, o per alcuna maniera passare oltre e scampare. scritta in tosc. italiano arcaico del 1323 in Italiano moderno: Salirono sull‚Äôargine del fossato e sullo steccato, per vedere se dall‚Äôalto potessero difendersi o, in qualche modo, superarlo e mettersi in salvo.\n",
      "===:(sentence n¬∞4):===\n",
      "Sentence:Riscrivi la seguente frase da' monti de' Romani si feciero nuovi nemici; contra i quali √® conbactuto cum diversa ventura: perk√© nela primaia battaglia, essendo consolo Valerio, MMMD ne moriro de' Romani; scritta in fior. italiano arcaico del 1292 in Italiano moderno: Dai monti dei Romani arrivarono nuovi nemici, contro i quali si combatt√© con esiti alterni: poich√© nella prima battaglia, mentre Valerio era console, tremilacinquecento Romani vi trov\n",
      "===:(sentence n¬∞5):===\n",
      "Sentence:Riscrivi la seguente frase colui che ancora non sa amare il prossimo come s√© medesimo gi√† cominci a temere i giudicii di Dio. scritta in tosc. italiano arcaico del 1415 in Italiano moderno: Chi ancora non sa amare il prossimo come se stesso, cominci a temere i giudizi di Dio.\n"
     ]
    }
   ],
   "source": [
    "for idx, s in enumerate(hf_tokenized[\"train\"].take(5), 1):\n",
    "    print(f\"===:(sentence n¬∞{idx}):===\")\n",
    "    print(f\"{SRC_L}:{tokenizer.decode(s[\"input_ids\"], attention_mask=s[\"attention_mask\"], skip_special_tokens=True)}\" )\n",
    "    #print(f\"{TRG_L}:{tokenizer.decode(s[\"labels\"], skip_special_tokens=True)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===:(sentence n¬∞1):===\n",
      "Sentence:Riscrivi la seguente frase Io gli apersi, e quelli fuggitte. E che bisogno √®, che lo cuore tuo stia chiuso al tuo sposo Cristo? scritta in Italiano arcaico in Italiano moderno: \n",
      "===:(sentence n¬∞2):===\n",
      "Sentence:Riscrivi la seguente frase s√¨ convene che lli desse grande avere perch√© ¬∑ lasciasse andare via. scritta in Italiano arcaico in Italiano moderno: \n",
      "===:(sentence n¬∞3):===\n",
      "Sentence:Riscrivi la seguente frase noi iscaciati e dipartiti per debito dela cittade, e tutti iscaciati da fama e da ventura buona. scritta in Italiano arcaico in Italiano moderno: \n",
      "===:(sentence n¬∞4):===\n",
      "Sentence:Riscrivi la seguente frase una villa chiamata Vitermina, con ci√≤ fosse cosa che piusori principi di scherani corressero per ventura a quel tempo a vederlo, Scipione, stimando che venissero per isforzarlo, allog√≤ ne la casa scritta in Italiano arcaico in Italiano moderno: \n",
      "===:(sentence n¬∞5):===\n",
      "Sentence:Riscrivi la seguente frase Creti?  Certo quand'elli si mosse, elli ti dixe: \"O fedele mia donna, fa' che in mio luogo ti sia racomandato il nostro hoste troiano\". scritta in Italiano arcaico in Italiano moderno: \n"
     ]
    }
   ],
   "source": [
    "for idx, s in enumerate(hf_tokenized[\"test\"].take(5), 1):\n",
    "    print(f\"===:(sentence n¬∞{idx}):===\")\n",
    "    print(f\"{SRC_L}:{tokenizer.decode(s[\"input_ids\"], attention_mask=s[\"attention_mask\"], skip_special_tokens=True)}\" )\n",
    "    #print(f\"{TRG_L}:{tokenizer.decode(s[\"labels\"], skip_special_tokens=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models & Traning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PEFT Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = MyTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=hf_tokenized[\"train\"],\n",
    "            eval_dataset=hf_tokenized[\"test\"],\n",
    "            processing_class=tokenizer,\n",
    "            data_collator=data_collator,\n",
    "            compute_metrics=compute_metrics,\n",
    "            callbacks=[Report(OUT_DIR), early_callback]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='209' max='1100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 209/1100 01:04 < 04:35, 3.23 it/s, Epoch 19/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Meteor</th>\n",
       "      <th>Chrf++</th>\n",
       "      <th>Ter</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.156200</td>\n",
       "      <td>5.238818</td>\n",
       "      <td>0.454200</td>\n",
       "      <td>0.254300</td>\n",
       "      <td>0.028100</td>\n",
       "      <td>0.172600</td>\n",
       "      <td>0.172900</td>\n",
       "      <td>0.153200</td>\n",
       "      <td>21.141400</td>\n",
       "      <td>92.834900</td>\n",
       "      <td>120.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.145300</td>\n",
       "      <td>5.215823</td>\n",
       "      <td>0.495600</td>\n",
       "      <td>0.254300</td>\n",
       "      <td>0.028100</td>\n",
       "      <td>0.172600</td>\n",
       "      <td>0.172900</td>\n",
       "      <td>0.153300</td>\n",
       "      <td>21.263700</td>\n",
       "      <td>94.081000</td>\n",
       "      <td>120.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.123900</td>\n",
       "      <td>5.166306</td>\n",
       "      <td>0.641900</td>\n",
       "      <td>0.252800</td>\n",
       "      <td>0.030800</td>\n",
       "      <td>0.171100</td>\n",
       "      <td>0.168200</td>\n",
       "      <td>0.164100</td>\n",
       "      <td>21.745500</td>\n",
       "      <td>95.015600</td>\n",
       "      <td>120.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.065300</td>\n",
       "      <td>5.070263</td>\n",
       "      <td>0.746300</td>\n",
       "      <td>0.249100</td>\n",
       "      <td>0.027300</td>\n",
       "      <td>0.167300</td>\n",
       "      <td>0.164200</td>\n",
       "      <td>0.167000</td>\n",
       "      <td>21.554500</td>\n",
       "      <td>91.900300</td>\n",
       "      <td>120.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.977800</td>\n",
       "      <td>4.899768</td>\n",
       "      <td>0.906100</td>\n",
       "      <td>0.252000</td>\n",
       "      <td>0.027300</td>\n",
       "      <td>0.171600</td>\n",
       "      <td>0.171300</td>\n",
       "      <td>0.171600</td>\n",
       "      <td>22.252900</td>\n",
       "      <td>90.654200</td>\n",
       "      <td>120.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.830900</td>\n",
       "      <td>4.642314</td>\n",
       "      <td>1.625200</td>\n",
       "      <td>0.294200</td>\n",
       "      <td>0.063000</td>\n",
       "      <td>0.214900</td>\n",
       "      <td>0.214500</td>\n",
       "      <td>0.221800</td>\n",
       "      <td>26.406000</td>\n",
       "      <td>86.292800</td>\n",
       "      <td>120.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.612800</td>\n",
       "      <td>4.366135</td>\n",
       "      <td>3.207500</td>\n",
       "      <td>0.316000</td>\n",
       "      <td>0.096500</td>\n",
       "      <td>0.257300</td>\n",
       "      <td>0.257800</td>\n",
       "      <td>0.242900</td>\n",
       "      <td>29.581700</td>\n",
       "      <td>83.489100</td>\n",
       "      <td>120.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.356200</td>\n",
       "      <td>4.128733</td>\n",
       "      <td>3.193000</td>\n",
       "      <td>0.367600</td>\n",
       "      <td>0.114600</td>\n",
       "      <td>0.313700</td>\n",
       "      <td>0.311200</td>\n",
       "      <td>0.266000</td>\n",
       "      <td>31.282800</td>\n",
       "      <td>82.866000</td>\n",
       "      <td>120.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.093100</td>\n",
       "      <td>3.942444</td>\n",
       "      <td>3.353300</td>\n",
       "      <td>0.382500</td>\n",
       "      <td>0.144100</td>\n",
       "      <td>0.346000</td>\n",
       "      <td>0.350800</td>\n",
       "      <td>0.317600</td>\n",
       "      <td>35.044600</td>\n",
       "      <td>81.931500</td>\n",
       "      <td>120.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.875000</td>\n",
       "      <td>3.857544</td>\n",
       "      <td>3.211700</td>\n",
       "      <td>0.358700</td>\n",
       "      <td>0.100500</td>\n",
       "      <td>0.321300</td>\n",
       "      <td>0.323400</td>\n",
       "      <td>0.277400</td>\n",
       "      <td>33.383500</td>\n",
       "      <td>78.504700</td>\n",
       "      <td>120.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.729900</td>\n",
       "      <td>3.894149</td>\n",
       "      <td>3.584600</td>\n",
       "      <td>0.325300</td>\n",
       "      <td>0.111600</td>\n",
       "      <td>0.287600</td>\n",
       "      <td>0.288700</td>\n",
       "      <td>0.245800</td>\n",
       "      <td>32.793400</td>\n",
       "      <td>81.308400</td>\n",
       "      <td>120.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.645000</td>\n",
       "      <td>3.911382</td>\n",
       "      <td>3.624500</td>\n",
       "      <td>0.330000</td>\n",
       "      <td>0.107400</td>\n",
       "      <td>0.295100</td>\n",
       "      <td>0.297100</td>\n",
       "      <td>0.243900</td>\n",
       "      <td>33.156700</td>\n",
       "      <td>81.619900</td>\n",
       "      <td>120.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2.593000</td>\n",
       "      <td>3.888175</td>\n",
       "      <td>3.615500</td>\n",
       "      <td>0.326800</td>\n",
       "      <td>0.109300</td>\n",
       "      <td>0.289800</td>\n",
       "      <td>0.290400</td>\n",
       "      <td>0.250200</td>\n",
       "      <td>33.250500</td>\n",
       "      <td>80.996900</td>\n",
       "      <td>120.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2.555400</td>\n",
       "      <td>3.890359</td>\n",
       "      <td>3.625700</td>\n",
       "      <td>0.328400</td>\n",
       "      <td>0.114700</td>\n",
       "      <td>0.283500</td>\n",
       "      <td>0.284300</td>\n",
       "      <td>0.244900</td>\n",
       "      <td>33.545300</td>\n",
       "      <td>80.996900</td>\n",
       "      <td>120.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.519200</td>\n",
       "      <td>3.866922</td>\n",
       "      <td>3.592700</td>\n",
       "      <td>0.321300</td>\n",
       "      <td>0.117700</td>\n",
       "      <td>0.276800</td>\n",
       "      <td>0.277300</td>\n",
       "      <td>0.248000</td>\n",
       "      <td>33.286400</td>\n",
       "      <td>81.619900</td>\n",
       "      <td>120.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2.493100</td>\n",
       "      <td>3.861177</td>\n",
       "      <td>3.690200</td>\n",
       "      <td>0.342700</td>\n",
       "      <td>0.122100</td>\n",
       "      <td>0.299200</td>\n",
       "      <td>0.301100</td>\n",
       "      <td>0.264300</td>\n",
       "      <td>33.730400</td>\n",
       "      <td>80.373800</td>\n",
       "      <td>120.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>2.469000</td>\n",
       "      <td>3.862907</td>\n",
       "      <td>3.730800</td>\n",
       "      <td>0.350700</td>\n",
       "      <td>0.123700</td>\n",
       "      <td>0.292900</td>\n",
       "      <td>0.294200</td>\n",
       "      <td>0.271700</td>\n",
       "      <td>34.137000</td>\n",
       "      <td>77.570100</td>\n",
       "      <td>120.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>2.437600</td>\n",
       "      <td>3.861408</td>\n",
       "      <td>3.755900</td>\n",
       "      <td>0.360100</td>\n",
       "      <td>0.124100</td>\n",
       "      <td>0.310900</td>\n",
       "      <td>0.311500</td>\n",
       "      <td>0.284700</td>\n",
       "      <td>34.128600</td>\n",
       "      <td>77.881600</td>\n",
       "      <td>120.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>2.416000</td>\n",
       "      <td>3.869266</td>\n",
       "      <td>3.743600</td>\n",
       "      <td>0.359100</td>\n",
       "      <td>0.123300</td>\n",
       "      <td>0.306300</td>\n",
       "      <td>0.307400</td>\n",
       "      <td>0.282500</td>\n",
       "      <td>33.914300</td>\n",
       "      <td>77.881600</td>\n",
       "      <td>120.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training done. Generating graphs...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=209, training_loss=3.1628864416095057, metrics={'train_runtime': 63.7312, 'train_samples_per_second': 136.511, 'train_steps_per_second': 17.26, 'total_flos': 1119278412103680.0, 'train_loss': 3.1628864416095057, 'epoch': 19.0})"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inizio generazione su cuda\n",
      "=============================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb1dd7e717824f619f6eb6fe0dade790",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===:0-(Model for Prompt)===\n",
      "Riscrivi la seguente frase Io gli apersi, e quelli fuggitte. E che bisogno √®, che lo cuore tuo stia chiuso al tuo sposo Cristo? scritta in Italiano arcaico in Italiano moderno: \n",
      "=========================\n",
      "===:(model sapienzanlp/Minerva-1B-base-v1.0):===\n",
      "Riscrivi la seguente frase Io gli apersi, e quelli fuggitte. E che bisogno √®, che lo cuore tuo stia chiuso al tuo sposo Cristo? scritta in Italiano arcaico in Italiano moderno: 1800 in italiano moderno: Io gli apersi, e quelli fuggirono.\n",
      "Settima lettera di Giovanni apostolo, scritta in greco in italiano moderno: 1800 in italiano moderno: Io gli apersi, e quelli fuggirono.\n",
      "Versione del 1800 in italiano moderno\n",
      "Io gli apersi, e quelli fuggirono.\n",
      "Io gli apersi, e quelli fuggirono.\n",
      "Io gli apersi, e quelli fuggirono.\n",
      "Io gli apersi, e quelli fuggirono.\n",
      "Io gli apersi, e quelli fugg\n",
      "=================================\n",
      "===:1-(Model for Prompt)===\n",
      "Riscrivi la seguente frase s√¨ convene che lli desse grande avere perch√© ¬∑ lasciasse andare via. scritta in Italiano arcaico in Italiano moderno: \n",
      "=========================\n",
      "===:(model sapienzanlp/Minerva-1B-base-v1.0):===\n",
      "Riscrivi la seguente frase s√¨ convene che lli desse grande avere perch√© ¬∑ lasciasse andare via. scritta in Italiano arcaico in Italiano moderno: 1500-1501.\n",
      "Sulla base di questa frase, si pu√≤ affermare che l'uomo, dopo aver lasciato andare via il suo cavallo, si sarebbe trovato in una situazione di grande difficolt√†. Il 2018 √® stato un anno molto importante per la nostra azienda, che ha visto l‚Äôingresso di due nuovi soci: il Dott. Marco Battaglia e il Dott. Marco Battaglia.\n",
      "Il Dott. Marco Battaglia, laureato in Medicina e Chirurgia presso l‚ÄôUniversit√† degli Studi di Milano, ha conseguito la specializzazione\n",
      "=================================\n",
      "===:2-(Model for Prompt)===\n",
      "Riscrivi la seguente frase noi iscaciati e dipartiti per debito dela cittade, e tutti iscaciati da fama e da ventura buona. scritta in Italiano arcaico in Italiano moderno: \n",
      "=========================\n",
      "===:(model sapienzanlp/Minerva-1B-base-v1.0):===\n",
      "Riscrivi la seguente frase noi iscaciati e dipartiti per debito dela cittade, e tutti iscaciati da fama e da ventura buona. scritta in Italiano arcaico in Italiano moderno: 1600.\n",
      "Noi, cittadini e cittadini, e tutti i cittadini e tutti i cittadini, siamo iscritti per debito della cittade, e tutti i cittadini e tutti i cittadini, e tutti i cittadini e tutti i cittadini, sono iscritti per debito della cittade.\n",
      "Discorsi senza fonte. Vocabolario in linea risalente al 1825.\n",
      "Noi, cittadini e cittadini, e tutti i cittadini e tutti i cittadini, siamo iscritti per debito della cittade, e tutti i cittadini e tutti i cittadini, e tutti i cittadini e tutti i\n",
      "=================================\n",
      "===:3-(Model for Prompt)===\n",
      "Riscrivi la seguente frase una villa chiamata Vitermina, con ci√≤ fosse cosa che piusori principi di scherani corressero per ventura a quel tempo a vederlo, Scipione, stimando che venissero per isforzarlo, allog√≤ ne la casa scritta in Italiano arcaico in Italiano moderno: \n",
      "=========================\n",
      "===:(model sapienzanlp/Minerva-1B-base-v1.0):===\n",
      "Riscrivi la seguente frase una villa chiamata Vitermina, con ci√≤ fosse cosa che piusori principi di scherani corressero per ventura a quel tempo a vederlo, Scipione, stimando che venissero per isforzarlo, allog√≤ ne la casa scritta in Italiano arcaico in Italiano moderno: 1600. La frase √® incisa su una lapide posta nel 1600 in una villa chiamata Vitermina. Il 2018 √® stato un anno molto importante per la nostra azienda, che ha visto l‚Äôingresso di due nuovi soci: il Dott. Marco Battaglia e il Dott. Marco Battaglia.\n",
      "Il Dott. Marco Battaglia, laureato in Medicina e Chirurgia presso l‚ÄôUniversit√† degli Studi di Milano, ha conseguito la specializzazione in Medicina Interna presso l‚ÄôUniversit√† degli Studi di Milano.\n",
      "Il Dott.\n",
      "=================================\n",
      "===:4-(Model for Prompt)===\n",
      "Riscrivi la seguente frase Creti?  Certo quand'elli si mosse, elli ti dixe: \"O fedele mia donna, fa' che in mio luogo ti sia racomandato il nostro hoste troiano\". scritta in Italiano arcaico in Italiano moderno: \n",
      "=========================\n",
      "===:(model sapienzanlp/Minerva-1B-base-v1.0):===\n",
      "Riscrivi la seguente frase Creti?  Certo quand'elli si mosse, elli ti dixe: \"O fedele mia donna, fa' che in mio luogo ti sia racomandato il nostro hoste troiano\". scritta in Italiano arcaico in Italiano moderno:  Cretesi, che si sono mossi, che hanno fatto il nostro hoste.\n",
      "Suggerisci una nuova modifica\n",
      "Citazione su questo articolo\n",
      "Citazione su questo articolo\n",
      "Cretesi, che si sono mossi, che hanno fatto il nostro hoste.\n",
      "Cretesi, che si sono mossi, che hanno fatto il nostro hoste.\n",
      "Cretesi, che si sono mossi, che hanno fatto il nostro hoste.\n",
      "Cretesi, che si sono mossi, che hanno fatto il nostro hoste.\n",
      "Cretesi, che\n",
      "=================================\n",
      "===:5-(Model for Prompt)===\n",
      "Riscrivi la seguente frase sia in mezzo tra me e te: con noi non puo' tu gi√† pi√π lungamente dimorare, ch'io non lo sofferr√≤ e non lo lascer√≤. scritta in Italiano arcaico in Italiano moderno: \n",
      "=========================\n",
      "===:(model sapienzanlp/Minerva-1B-base-v1.0):===\n",
      "Riscrivi la seguente frase sia in mezzo tra me e te: con noi non puo' tu gi√† pi√π lungamente dimorare, ch'io non lo sofferr√≤ e non lo lascer√≤. scritta in Italiano arcaico in Italiano moderno: 1800 in italiano moderno: con noi non pu√≤ pi√π dimorare.\n",
      "Soggettivamente, la frase √® in italiano moderno: con noi non pu√≤ pi√π dimorare.\n",
      "La frase √® in italiano moderno: con noi non pu√≤ pi√π dimorare.\n",
      "La frase √® in italiano moderno: con noi non pu√≤ pi√π dimorare.\n",
      "La frase √® in italiano moderno: con noi non pu√≤ pi√π dimorare.\n",
      "La frase √® in italiano moderno: con noi non pu√≤ pi√π dimorare.\n",
      "La frase √® in italiano moderno: con noi non pu√≤ pi√π dimorare.\n",
      "La frase\n",
      "=================================\n",
      "===:6-(Model for Prompt)===\n",
      "Riscrivi la seguente frase Dio, per la quale si dispensano et iudicano tutte le cose. scritta in Italiano arcaico in Italiano moderno: \n",
      "=========================\n",
      "===:(model sapienzanlp/Minerva-1B-base-v1.0):===\n",
      "Riscrivi la seguente frase Dio, per la quale si dispensano et iudicano tutte le cose. scritta in Italiano arcaico in Italiano moderno: 1800 in italiano moderno: Dio, per la quale si dispensano et iudicano tutte le cose.\n",
      "Discorsi senza fonte. ‚Äì 1800 in italiano moderno: Dio, per la quale si dispensano et iudicano tutte le cose.\n",
      "Discorsi senza fonte. ‚Äì 1800 in italiano moderno: Dio, per la quale si dispensano et iudicano tutte le cose.\n",
      "Parole contenute in italiano moderno.\n",
      "Scartando le parti in comune (in coda e in capo), le parti in comune diventano d'\n",
      "=================================\n",
      "===:7-(Model for Prompt)===\n",
      "Riscrivi la seguente frase la seconda suole talora per la grande provedenzia fare timoroso, e la prima per l'ardire rendere altrui matto. scritta in Italiano arcaico in Italiano moderno: \n",
      "=========================\n",
      "===:(model sapienzanlp/Minerva-1B-base-v1.0):===\n",
      "Riscrivi la seguente frase la seconda suole talora per la grande provedenzia fare timoroso, e la prima per l'ardire rendere altrui matto. scritta in Italiano arcaico in Italiano moderno: 1. La seconda suole talora per la grande dimostranza fare timoroso, e la prima per l'ardire rendere altrui matto. 2. La prima suole talora per la grande dimostranza fare timoroso, e la prima per l'ardire rendere altrui matto. 3. La seconda suole talora per la grande dimostranza fare timoroso, e la prima per l'ardire rendere altrui matto. 4. La prima suole talora per la grande dimostranza fare timoroso, e la\n",
      "=================================\n",
      "===:0-(Model for Prompt)===\n",
      "Riscrivi la seguente frase Dice il poeta: oh, che bella cosa √® vedere apertamente con gli occhi quando tu glel fai diretro o in culo o in altro scritta in Italiano arcaico in Italiano moderno: \n",
      "=========================\n",
      "===:(model sapienzanlp/Minerva-1B-base-v1.0):===\n",
      "Riscrivi la seguente frase Dice il poeta: oh, che bella cosa √® vedere apertamente con gli occhi quando tu glel fai diretro o in culo o in altro scritta in Italiano arcaico in Italiano moderno: 1800: Il poeta, che si mostra con gli occhi, fa vedere con gli occhi il suo corpo, e con gli occhi fa vedere il suo corpo.\n",
      "Soggetti e argomenti: Il poeta, che si mostra con gli occhi, fa vedere con gli occhi il suo corpo, e con gli occhi fa vedere il suo corpo.\n",
      "Significato: Che bella cosa √® vedere apertamente con gli occhi quando tu glel fai diretro o in culo o in altro scritto in italiano moderno: 1800: Il poeta, che si mostra con gli occhi,\n",
      "=================================\n",
      "===:1-(Model for Prompt)===\n",
      "Riscrivi la seguente frase molto di maggiore memoria saranno faccendole al re, perci√≤ che nella nostra cittade sempre fue santo e glorioso il nome reale, e sse furono compagni fue il loro nome santissimo; scritta in Italiano arcaico in Italiano moderno: \n",
      "=========================\n",
      "===:(model sapienzanlp/Minerva-1B-base-v1.0):===\n",
      "Riscrivi la seguente frase molto di maggiore memoria saranno faccendole al re, perci√≤ che nella nostra cittade sempre fue santo e glorioso il nome reale, e sse furono compagni fue il loro nome santissimo; scritta in Italiano arcaico in Italiano moderno: 1600. in italiano moderno: 1600.\n",
      "S'intende che il nome di questo santo, che fu il primo dei santi, fu il suo nome reale, e fu compagno di santit√†, e glorioso, e santo.\n",
      "S'intende che il nome di questo santo, che fu il primo dei santi, fu il suo nome reale, e fu compagno di santit√†, e glorioso, e santo.\n",
      "S'intende che il nome di questo santo, che fu il primo dei santi, fu il suo nome reale, e fu\n",
      "=================================\n",
      "\n",
      "Generazione completata.\n"
     ]
    }
   ],
   "source": [
    "# Imposta il modello in modalit√† valutazione e spostalo sul device\n",
    "model = model.eval()\n",
    "model = model.to(device)\n",
    "\n",
    "# Crea il DataLoader\n",
    "loader = torch.utils.data.DataLoader(hf_tokenized[\"test\"], batch_size=8)\n",
    "\n",
    "\n",
    "print(f\"Inizio generazione su {device}\")\n",
    "print(\"=============================\")\n",
    "\n",
    "for batch in tqdm(loader):\n",
    "    # Sposta l'intero batch sul device\n",
    "    # Nota: DataLoader restituisce un batch come dizionario di tensori\n",
    "    \n",
    "    batch[\"input_ids\"] = batch[\"input_ids\"].to(device)\n",
    "    batch[\"attention_mask\"] = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "    # Genera l'output per il batch\n",
    "    # Input al generate devono essere input_ids e attention_mask\n",
    "    result_ids = model.generate(\n",
    "        input_ids=batch[\"input_ids\"],\n",
    "        attention_mask=batch[\"attention_mask\"],\n",
    "        max_new_tokens=max_length,\n",
    "    )\n",
    "\n",
    "    # Decodifica *separatamente* ogni prompt e ogni risultato generato\n",
    "    # Iteriamo sul batch per decodificare uno per uno\n",
    "    # batch[\"input_ids\"] ha forma (batch_size, seq_len_input)\n",
    "    # result_ids ha forma (batch_size, seq_len_output)\n",
    "    \n",
    "    # Decodifica i prompt originali\n",
    "    decoded_prompts = tokenizer.batch_decode(batch[\"input_ids\"], skip_special_tokens=True)\n",
    "    \n",
    "    # Decodifica i risultati generati\n",
    "    decoded_results = tokenizer.batch_decode(result_ids, skip_special_tokens=True)\n",
    "\n",
    "    # Stampa i risultati per ogni elemento del batch\n",
    "    with open(OUT_DIR + \"/output_chat.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for i in range(len(decoded_prompts)):\n",
    "            print(f\"===:{i}-(Model for Prompt)===\")\n",
    "            print(f\"{decoded_prompts[i]}\")\n",
    "            print(\"=========================\")\n",
    "            \n",
    "            print(f\"===:(model {network}):===\")\n",
    "            print(decoded_results[i])\n",
    "            print(\"=================================\")\n",
    "            \n",
    "            ############################################\n",
    "\n",
    "\n",
    "            f.write(\"f===:({i}Model for Prompt)===\")\n",
    "            f.write(f\"{decoded_prompts[i]}\")\n",
    "            f.write(\"=========================\")\n",
    "\n",
    "            f.write(f\"===:(model {network}):===\")\n",
    "            f.write(decoded_results[i])\n",
    "            f.write(\"==========================\")\n",
    "\n",
    "print(\"\\nGenerazione completata.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MNLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
