{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asJxkfJNJ6Ab"
      },
      "source": [
        "# Fine-tuning for Translation from Ancient to Modern Italian"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZXyfYF5KTyH"
      },
      "source": [
        "# System Setup 🖥️"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWshcyakjQV8"
      },
      "source": [
        "### Drive Interface 📁"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bn2BVRjjKKUZ",
        "outputId": "95a55d51-b204-47e2-85d1-008a0984ccf5"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "%cp /content/drive/MyDrive/MNLP_HW_2/Many_Naps_Little_Progress/*.* .\n",
        "%ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgbPZ6tqKLDD"
      },
      "source": [
        "### Additional Dependencies 🐍"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNhSDgXeKKBS",
        "outputId": "a0f02e11-0497-4e18-e78b-45ee5a4dd38f"
      },
      "outputs": [],
      "source": [
        "!bash ../install_colab.sh >> /dev/null"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_q2wvHwFKt5F"
      },
      "source": [
        "### Hugging Face 🤗"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ccb4L42zKuVB"
      },
      "outputs": [],
      "source": [
        "import huggingface_hub\n",
        "TOKEN = \"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\"\n",
        "huggingface_hub.login(token=TOKEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LN3PEzDWJ6Ai"
      },
      "outputs": [],
      "source": [
        "# Import Datases to work with Transformers by Hugging-Face\n",
        "import torch\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "# Imports for Transformers\n",
        "from transformers import AutoTokenizer  # Datasets\n",
        "from datasets import Dataset, DatasetDict\n",
        "from utils import Report\n",
        "import numpy as np\n",
        "import evaluate\n",
        "from transformers import DataCollatorForLanguageModeling, Trainer, TrainingArguments, AutoModelForCausalLM          # imports for causal Learning\n",
        "from transformers import DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer, AutoModelForSeq2SeqLM    # imports for Seq2Seq models\n",
        "from peft import LoraConfig, TaskType, LoftQConfig, PeftModelForSeq2SeqLM, PeftModelForCausalLM, get_peft_model     # imports for quantization methods (LoRA etc...)\n",
        "from transformers import EarlyStoppingCallback"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8kgPZC0J6Ak"
      },
      "source": [
        "# Fine-Tuned Models\n",
        "\n",
        "* google/mt5-base (Machine Translation)\n",
        "* sapienzanlp/Minerva-1B-base-v1.0 🇮🇹 (LLM)\n",
        "* sapienzanlp/Minerva-3B-base-v1.0 🇮🇹 (LLM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QSAlloAIJ6Am"
      },
      "outputs": [],
      "source": [
        "device = ('cuda' if torch.cuda.is_available() else \"cpu\")\n",
        "DATASET = \"dataset_ann.csv\"\n",
        "SRC_L = \"Sentence\"\n",
        "TRG_L = \"Target\"\n",
        "network = \"sapienzanlp/Minerva-3B-base-v1.0\"\n",
        "tokenization_method = \"minerva_base\"\n",
        "OUT_DIR = network.split(\"/\")[-1]\n",
        "EPOCHS = 42\n",
        "BATCH_SIZE = 8\n",
        "max_length = 80"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jEpmxmqJ6Am"
      },
      "source": [
        "# Dataset Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dsfHlEQ8J6An"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(DATASET, sep=\",\", index_col=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aMPFWURdJ6Ao",
        "outputId": "696c4127-f9fb-4b6b-d6c3-e5872a57b404"
      },
      "outputs": [],
      "source": [
        "print(f\"length mean {SRC_L} text: {df[SRC_L].apply(lambda x: len(x.split())).mean()}\")\n",
        "print(f\"length mean {TRG_L} text: {df[TRG_L].apply(lambda x: len(x.split())).mean()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nejry195J6Ap",
        "outputId": "cad8aacb-1bb2-4ddf-f05e-1cea33886c67"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8ivbkfnJ6Aq"
      },
      "source": [
        "## Model Selection\n",
        "\n",
        "Configure Pipline for model select for fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "c802734e29674453b4553504f77407da"
          ]
        },
        "id": "zw802PcHJ6Ar",
        "outputId": "b7bf3d15-5c77-4c74-ada1-5a78fc0b0e3a"
      },
      "outputs": [],
      "source": [
        "# Switch to select the network and load the appropriate model and tokenizer\n",
        "match network:\n",
        "\n",
        "    case \"sapienzanlp/Minerva-3B-base-v1.0\":\n",
        "        from transformers import BitsAndBytesConfig\n",
        "        bnb_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_compute_dtype=torch.bfloat16,  # or torch.float16 if bfloat16 not supported\n",
        "        )\n",
        "\n",
        "        tokenizer = AutoTokenizer.from_pretrained(network)\n",
        "        if tokenizer.pad_token is None:\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "        model = AutoModelForCausalLM.from_pretrained(network, device_map=\"auto\", quantization_config=bnb_config)\n",
        "        data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        "\n",
        "        lora_config = LoraConfig(\n",
        "            task_type=TaskType.CAUSAL_LM,\n",
        "            inference_mode=False,\n",
        "            r=128,\n",
        "            lora_alpha=128,\n",
        "            lora_dropout=0.50\n",
        "            )\n",
        "\n",
        "        qlora_config = LoraConfig(\n",
        "            init_lora_weights=\"loftq\",\n",
        "            loftq_config=LoftQConfig(loftq_bits=4),\n",
        "            r=8,\n",
        "            lora_alpha=16,\n",
        "            target_modules=\"all-linear\",\n",
        "            lora_dropout=0.5,\n",
        "            bias=\"none\",\n",
        "            task_type=\"CAUSAL_LM\"\n",
        "        )\n",
        "\n",
        "        model = get_peft_model(model, lora_config)\n",
        "        model.print_trainable_parameters()\n",
        "\n",
        "        early_stopping_patience = 10\n",
        "        early_stopping_threshold = 0.01\n",
        "\n",
        "        early_callback = EarlyStoppingCallback(\n",
        "            early_stopping_patience=early_stopping_patience, # If validation loss does not improve for 3 consecutive epochs\n",
        "            early_stopping_threshold=early_stopping_threshold # Ignores improvements smaller than 0.001\n",
        "        )\n",
        "\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=OUT_DIR,\n",
        "            learning_rate=3e-5,\n",
        "            weight_decay=1e-2,\n",
        "            warmup_steps=500,\n",
        "            per_device_train_batch_size=BATCH_SIZE,\n",
        "            per_device_eval_batch_size=BATCH_SIZE,\n",
        "            num_train_epochs=EPOCHS,\n",
        "            eval_strategy=\"epoch\",\n",
        "            save_strategy=\"epoch\",\n",
        "            logging_strategy=\"epoch\",\n",
        "            load_best_model_at_end=True,\n",
        "            report_to=\"none\",\n",
        "            save_total_limit=3,\n",
        "            lr_scheduler_type=\"cosine\",\n",
        "            logging_dir=OUT_DIR,\n",
        "            logging_steps=10,\n",
        "            label_names=['labels'],\n",
        "            metric_for_best_model=\"eval_chrf++\",\n",
        "            greater_is_better=True,\n",
        "        )\n",
        "\n",
        "        params = {\n",
        "            #\"max_new_tokens\": max_length, # max number of new tokens to generate\n",
        "            #\"do_sample\":True,      # enables sampling for more diverse outputs\n",
        "            #\"top_k\":100,            # diversity increase by controlling the candidate words\n",
        "            #\"top_p\":0.95,          # nucleus sampling for further control over variety\n",
        "            #\"temperature\":1.0,     # reduces randomness and increases coherence\n",
        "            #\"repetition_penalty\":1.0,  # penalizes repetitions\n",
        "            #\"num_return_sequences\":10,  # number of generated responses\n",
        "            \"pad_token_id\":tokenizer.eos_token_id  # avoids warning if padding token is missing\n",
        "        }\n",
        "        \n",
        "    case \"google/mt5-base\" | \"google/mt5-large\":\n",
        "        tokenizer = AutoTokenizer.from_pretrained(network)\n",
        "        if tokenizer.pad_token is None:\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "        model = AutoModelForSeq2SeqLM.from_pretrained(network, device_map=device, torch_dtype=torch.float32)\n",
        "        data_collator = DataCollatorForSeq2Seq(tokenizer, network)\n",
        "\n",
        "        lora_config = LoraConfig(\n",
        "            task_type=TaskType.SEQ_2_SEQ_LM,\n",
        "            inference_mode=False,\n",
        "            r=64,\n",
        "            lora_alpha=64,\n",
        "            lora_dropout=0.3\n",
        "            )\n",
        "\n",
        "        qlora_config = LoraConfig(\n",
        "            init_lora_weights=\"loftq\",\n",
        "            loftq_config=LoftQConfig(loftq_bits=4),\n",
        "            r=128,\n",
        "            lora_alpha=128*2,\n",
        "            target_modules=\"all-linear\",\n",
        "            lora_dropout=0.3,\n",
        "            bias=\"none\",\n",
        "            task_type=TaskType.SEQ_2_SEQ_LM\n",
        "        )\n",
        "\n",
        "        model = get_peft_model(model, lora_config)\n",
        "        model.print_trainable_parameters()\n",
        "\n",
        "        early_stopping_patience = 3\n",
        "        early_stopping_threshold = 0.01\n",
        "\n",
        "        early_callback = EarlyStoppingCallback(\n",
        "            early_stopping_patience=early_stopping_patience, # If validation loss does not improve for 3 consecutive epochs\n",
        "            early_stopping_threshold=early_stopping_threshold # Ignores improvements smaller than 0.001\n",
        "        )\n",
        "\n",
        "        training_args = Seq2SeqTrainingArguments(\n",
        "            output_dir=OUT_DIR,\n",
        "            learning_rate=4e-4,\n",
        "            weight_decay=3e-4,\n",
        "            warmup_steps=500,\n",
        "            per_device_train_batch_size=BATCH_SIZE,\n",
        "            per_device_eval_batch_size=BATCH_SIZE,\n",
        "            num_train_epochs=EPOCHS,\n",
        "            eval_strategy=\"epoch\",\n",
        "            save_strategy=\"epoch\",\n",
        "            logging_strategy=\"epoch\",\n",
        "            load_best_model_at_end=True,\n",
        "            report_to=\"none\",\n",
        "            save_total_limit=3,\n",
        "            lr_scheduler_type=\"linear\",\n",
        "            logging_dir=OUT_DIR,\n",
        "            logging_steps=10,\n",
        "            label_names=['labels'],\n",
        "            metric_for_best_model=\"eval_chrf++\",\n",
        "            greater_is_better=True,\n",
        "        )\n",
        "\n",
        "        params = {\n",
        "            \"max_new_tokens\": max_length, # max number of new tokens to generate\n",
        "            \"do_sample\":True,      # enables sampling for more diverse outputs\n",
        "            #\"top_k\":100,            # diversity increase by controlling the candidate words\n",
        "            #\"top_p\":0.95,          # nucleus sampling for further control over variety\n",
        "            #\"temperature\":1.0,     # reduces randomness and increases coherence\n",
        "            #\"repetition_penalty\":1.0,  # penalizes repetitions\n",
        "            #\"num_return_sequences\":10,  # number of generated responses\n",
        "            \"pad_token_id\":tokenizer.eos_token_id  # avoids warning if padding token is missing\n",
        "        }\n",
        "\n",
        "    case _:\n",
        "        raise Exception(f\"Rete {network} non testabile\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xFz9vXsgJ6As"
      },
      "outputs": [],
      "source": [
        "class MyTrainerSeq2Seq(Seq2SeqTrainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
        "        if 'num_items_in_batch' in inputs:\n",
        "            inputs = {k: v for k, v in inputs.items() if k != 'num_items_in_batch'}\n",
        "        return super().compute_loss(model, inputs, return_outputs=return_outputs)\n",
        "\n",
        "class MyTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
        "        if 'num_items_in_batch' in inputs:\n",
        "            inputs = {k: v for k, v in inputs.items() if k != 'num_items_in_batch'}\n",
        "        return super().compute_loss(model, inputs, return_outputs=return_outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iPt0wW7VJ6At",
        "outputId": "714c0a63-d816-4373-dac9-bbb95acfb8b5"
      },
      "outputs": [],
      "source": [
        "sacrebleu_metric = evaluate.load(\"sacrebleu\")\n",
        "rouge_metric = evaluate.load(\"rouge\")\n",
        "meteor_metric = evaluate.load(\"meteor\")\n",
        "chrf_metric = evaluate.load(\"chrf\")\n",
        "ter_metric = evaluate.load(\"ter\")\n",
        "\n",
        "\n",
        "def postprocess_text(preds, labels):\n",
        "    preds = [pred.strip() for pred in preds]\n",
        "    labels = [[label.strip()] for label in labels] # Specific format for SacreBLEU\n",
        "    return preds, labels\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    preds_input, label_ids = eval_preds\n",
        "\n",
        "    # Dealing with logits or token IDs for predictions\n",
        "    # If preds_input are logits (es. direct output of training modello)\n",
        "    current_preds = preds_input\n",
        "    if isinstance(current_preds, tuple): # Common in HF Trainer, es. (logits, hidden_states)\n",
        "        current_preds = current_preds[0]\n",
        "\n",
        "    if hasattr(current_preds, \"ndim\") and current_preds.ndim == 3: # Array of logits (batch_size, seq_len, vocab_size)\n",
        "        current_preds_ids = np.argmax(current_preds, axis=-1)\n",
        "    else: # Otherwise, assumed to be token ID (batch_size, seq_len)\n",
        "        current_preds_ids = current_preds\n",
        "\n",
        "    # Decode predictions and labels\n",
        "    decoded_preds_raw = tokenizer.batch_decode(current_preds_ids, skip_special_tokens=True)\n",
        "\n",
        "    # Replace -100 in labels (common for token to be ignored) with pad_token_id for decoding\n",
        "    processed_label_ids = np.where(label_ids != -100, label_ids, tokenizer.pad_token_id)\n",
        "    decoded_labels_raw = tokenizer.batch_decode(processed_label_ids, skip_special_tokens=True)\n",
        "\n",
        "    processed_preds, processed_labels_for_sacrebleu = postprocess_text(decoded_preds_raw, decoded_labels_raw)\n",
        "\n",
        "    # For other metrics (ROUGE, METEOR, CHRF, TER), usually expects a flat list of reference strings\n",
        "    flat_references = [ref[0] for ref in processed_labels_for_sacrebleu]\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    # 1. SacreBLEU\n",
        "    sacrebleu_output = sacrebleu_metric.compute(predictions=processed_preds, references=processed_labels_for_sacrebleu)\n",
        "    if sacrebleu_output and \"score\" in sacrebleu_output:\n",
        "        results[\"bleu\"] = sacrebleu_output[\"score\"]\n",
        "    else:\n",
        "        results[\"bleu\"] = 0.0 # Fallback\n",
        "\n",
        "    # 2. ROUGE (rouge1, rouge2, rougeL, rougeLsum)\n",
        "    rouge_output = rouge_metric.compute(predictions=processed_preds, references=flat_references, use_stemmer=True)\n",
        "    if rouge_output:\n",
        "        results[\"rouge1\"] = rouge_output.get(\"rouge1\", 0.0)\n",
        "        results[\"rouge2\"] = rouge_output.get(\"rouge2\", 0.0)\n",
        "        results[\"rougeL\"] = rouge_output.get(\"rougeL\", 0.0)\n",
        "        results[\"rougeLsum\"] = rouge_output.get(\"rougeLsum\", 0.0) # Spesso più robusto per sommario\n",
        "    else:\n",
        "        results.update({\"rouge1\": 0.0, \"rouge2\": 0.0, \"rougeL\": 0.0, \"rougeLsum\": 0.0})\n",
        "\n",
        "    # 3. METEOR\n",
        "    meteor_output = meteor_metric.compute(predictions=processed_preds, references=flat_references)\n",
        "    if meteor_output and \"meteor\" in meteor_output:\n",
        "        results[\"meteor\"] = meteor_output[\"meteor\"]\n",
        "    else:\n",
        "        results[\"meteor\"] = 0.0\n",
        "\n",
        "    # 4. CHRF++ (CHRF with n-grams of words)\n",
        "    # For CHRF++, word_order (or word_n) is > 0. Default of evaluate.load('chrf') are word_order=0 (CHRF standard).\n",
        "    # Common parameters for CHRF++: word_order=2, beta=2 (beta=2 default)\n",
        "    chrf_output = chrf_metric.compute(predictions=processed_preds, references=flat_references, word_order=2, beta=2)\n",
        "    if chrf_output and \"score\" in chrf_output:\n",
        "        results[\"chrf++\"] = chrf_output[\"score\"] # CHRF++ score\n",
        "    else:\n",
        "        results[\"chrf++\"] = 0.0\n",
        "\n",
        "    # (Optional) CHRF standard (only characters)\n",
        "    # chrf_std_output = chrf_metric.compute(predictions=processed_preds, references=flat_references, word_order=0)\n",
        "    # if chrf_std_output and \"score\" in chrf_std_output:\n",
        "    #     results[\"chrf\"] = chrf_std_output[\"score\"]\n",
        "    # else:\n",
        "    #     results[\"chrf\"] = 0.0\n",
        "\n",
        "    # 5. TER (Translation Edit Rate) - the smaller, the better\n",
        "    ter_output = ter_metric.compute(predictions=processed_preds, references=flat_references)\n",
        "    if ter_output and \"score\" in ter_output:\n",
        "        results[\"ter\"] = ter_output[\"score\"]\n",
        "    else:\n",
        "        results[\"ter\"] = 1.0 # Fallback on worst score TER possible\n",
        "\n",
        "    # Mean length of generated predictions (excluding padding tokens)\n",
        "    # 'current_preds_ids' are ID token of the predictions\n",
        "    prediction_lengths = [np.count_nonzero(pid_seq != tokenizer.pad_token_id) for pid_seq in current_preds_ids]\n",
        "    results[\"gen_len\"] = np.mean(prediction_lengths) if prediction_lengths else 0.0\n",
        "\n",
        "    # Rounding of all numerical results\n",
        "    final_results = {k: round(v, 4) for k, v in results.items() if isinstance(v, (int, float))}\n",
        "\n",
        "    return final_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ui4ZU02DJ6Au"
      },
      "outputs": [],
      "source": [
        "from datasets.features import Value, Features\n",
        "hf = Dataset.from_csv(DATASET, features=\n",
        "    Features({\n",
        "        SRC_L : Value(\"string\"),\n",
        "        TRG_L : Value(\"string\"),\n",
        "        \"Date\": Value(\"string\"),\n",
        "        \"Author\":Value(\"string\"),\n",
        "        \"Region\":Value(\"string\")\n",
        "    })\n",
        "    ).shuffle(2025).train_test_split(test_size=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCtZMnYMJ6Au"
      },
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtW9sFCaJ6Av",
        "outputId": "5856bc07-74c2-4176-adbc-861353c16240"
      },
      "outputs": [],
      "source": [
        "print(f\"model max length: {max_length}\")\n",
        "\n",
        "def noprompt_it_it(examples):\n",
        "    inputs = [example for example in examples[SRC_L]]\n",
        "    targets = [example for example in examples[TRG_L]]\n",
        "\n",
        "    model_inputs = tokenizer(inputs, text_target=targets, max_length=max_length, truncation=True, padding=\"max_length\")\n",
        "    return model_inputs\n",
        "\n",
        "# I. Rewrite\n",
        "# II. Translate\n",
        "# III. Correct\n",
        "def minerva_base_prompt_it_it_train(examples):\n",
        "\n",
        "    prompts = [\n",
        "        f\"\"\"riscrivi la seguente frase '{src}' scritta in italiano arcaico in Italiano moderno: {dst}\"\"\"\n",
        "        for src, dst, dat, dia in zip(examples[SRC_L], examples[TRG_L], examples[\"Date\"], examples[\"Region\"])\n",
        "    ]\n",
        "\n",
        "    # Tokenizes input+target and creates label with same tokens\n",
        "    model_inputs = tokenizer(\n",
        "        prompts,\n",
        "        max_length=max_length,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\"\n",
        "    )\n",
        "\n",
        "    model_inputs[\"labels\"] = [\n",
        "        [(label if label != tokenizer.pad_token_id else -100) for label in input_ids]\n",
        "        for input_ids in model_inputs[\"input_ids\"]\n",
        "    ]\n",
        "    return model_inputs\n",
        "\n",
        "def minerva_base_prompt_it_it_eval(examples):\n",
        "    prompts = [\n",
        "\n",
        "        f\"\"\"riscrivi la seguente frase '{src}' scritta in italiano arcaico in Italiano moderno: \"\"\"\n",
        "        for src in examples[SRC_L]\n",
        "    ]\n",
        "\n",
        "    # Tokenizes input+target and creates label with same tokens\n",
        "    model_inputs = tokenizer(\n",
        "        prompts,\n",
        "        max_length=max_length,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\"\n",
        "    )\n",
        "\n",
        "    return model_inputs\n",
        "\n",
        "def base_prompt_en(examples):\n",
        "    inputs = [\"translate from Ancient Italian to Modern Italian: \" + example for example in examples[SRC_L]]\n",
        "\n",
        "    # Tokenizes only inputs\n",
        "    model_inputs = tokenizer(inputs, max_length=max_length, truncation=True, padding=\"max_length\")\n",
        "    targets = [example for example in examples[TRG_L]]\n",
        "\n",
        "    model_inputs = tokenizer(inputs, text_target=targets, max_length=max_length, truncation=True, padding=\"longest\")\n",
        "\n",
        "    return model_inputs\n",
        "\n",
        "def base_prompt_it_it(examples):\n",
        "    inputs = [\"Riscrivi dall'Italiano Antico a l'Italiano Moderno: \" + example for example in examples[SRC_L]]\n",
        "\n",
        "    # Tokenizes only inputs\n",
        "    model_inputs = tokenizer(inputs, max_length=max_length, truncation=True, padding=\"max_length\")\n",
        "    targets = [example for example in examples[TRG_L]]\n",
        "\n",
        "    model_inputs = tokenizer(inputs, text_target=targets, max_length=max_length, truncation=True, padding=\"longest\")\n",
        "\n",
        "    return model_inputs\n",
        "\n",
        "def parafrasi_prompt_it_it(examples):\n",
        "    inputs = [\"Scrivi la parafrasi di questo testo: \" + example for example in examples[SRC_L]]\n",
        "    targets = [example for example in examples[TRG_L]]\n",
        "\n",
        "    model_inputs = tokenizer(inputs, text_target=targets, max_length=max_length, truncation=True, padding=\"longest\")\n",
        "    return model_inputs\n",
        "\n",
        "def informative_prompt_it_it(examples):\n",
        "    inputs = [f\"Riscrivi in uno stile più moderno il testo del seguente Autore: '{author}', anno di scrittura: {date}, luogo: Italia, dialetto: '{region}', testo: '{text}'.\" for text, date, region, author in zip(examples[SRC_L], examples[\"Date\"], examples[\"Region\"], examples[\"Author\"]) ]\n",
        "    targets = [example for example in examples[TRG_L]]\n",
        "\n",
        "    model_inputs = tokenizer(inputs, text_target=targets, max_length=max_length, truncation=True, padding=\"longest\")\n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BFXkm0YJ6Av"
      },
      "source": [
        "## Tokenizer Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "e439f25244e041b8b00b62a4e79edd36",
            "86c58244034945728ce237f1d28e215d"
          ]
        },
        "id": "luEQkXHIJ6Aw",
        "outputId": "297d2697-d874-441c-85ef-bf3f45f1cfde"
      },
      "outputs": [],
      "source": [
        "match tokenization_method:\n",
        "    case \"minerva_base\":\n",
        "        map_callback_train = minerva_base_prompt_it_it_train\n",
        "        map_callback_eval   = minerva_base_prompt_it_it_eval\n",
        "\n",
        "        hf_tokenized = DatasetDict({\n",
        "            \"train\": hf[\"train\"].map(map_callback_train, batched=True),\n",
        "            \"test\":  hf[\"test\"].map(map_callback_eval, batched=True)\n",
        "        })\n",
        "\n",
        "        hf_tokenized.set_format(type=\"torch\", columns=['input_ids', 'attention_mask'])\n",
        "\n",
        "    case \"base_prompt_en\":\n",
        "        map_callback = base_prompt_en\n",
        "        hf_tokenized = hf.map(map_callback, batched=True)\n",
        "\n",
        "    case \"base_prompt_it_it\":\n",
        "        map_callback = base_prompt_it_it\n",
        "        hf_tokenized = hf.map(map_callback, batched=True)\n",
        "        \n",
        "    case _:\n",
        "        raise ValueError(\"Tokenization method not avaiable\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8_EGew_J6Aw",
        "outputId": "c5da6260-613d-4cb6-e04f-c3b475fa36b0"
      },
      "outputs": [],
      "source": [
        "print(hf_tokenized.column_names)\n",
        "print(hf_tokenized.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eC3T3SlkJ6Aw",
        "outputId": "32aa2760-ca3b-4bb7-82ec-a67f56d5d680"
      },
      "outputs": [],
      "source": [
        "for idx, s in enumerate(hf_tokenized[\"train\"].take(5), 1):\n",
        "    print(f\"===:(sentence n°{idx}):===\")\n",
        "    print(f\"{SRC_L}:{tokenizer.decode(s['input_ids'], attention_mask=s['attention_mask'], skip_special_tokens=True)}\" )\n",
        "    #print(f\"{TRG_L}:{tokenizer.decode(s[\"labels\"], skip_special_tokens=True)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WB1tROKuJ6Ax",
        "outputId": "82cde9f8-a3c9-454a-ed9b-a6e497b7c4dd"
      },
      "outputs": [],
      "source": [
        "for idx, s in enumerate(hf_tokenized[\"test\"].take(5), 1):\n",
        "    print(f\"===:(sentence n°{idx}):===\")\n",
        "    print(f\"{SRC_L}:{tokenizer.decode(s['input_ids'], attention_mask=s['attention_mask'], skip_special_tokens=True)}\" )\n",
        "    #print(f\"{TRG_L}:{tokenizer.decode(s[\"labels\"], skip_special_tokens=True)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLsr_a4RJ6Ax"
      },
      "source": [
        "## Models & Traning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aNXSOX0J6Ay"
      },
      "source": [
        "### PEFT Fine-Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KGZ5c0DcJ6Ay",
        "outputId": "67462809-6c0c-40af-a623-f77612bdd67c"
      },
      "outputs": [],
      "source": [
        "if isinstance(model, PeftModelForSeq2SeqLM):\n",
        "    print(\"[SEQ2SEQ Generation]\")\n",
        "    trainer = MyTrainerSeq2Seq(\n",
        "                model=model,\n",
        "                args=training_args,\n",
        "                train_dataset=hf_tokenized[\"train\"],\n",
        "                eval_dataset=hf_tokenized[\"test\"],\n",
        "                processing_class=tokenizer,\n",
        "                data_collator=data_collator,\n",
        "                compute_metrics=compute_metrics,\n",
        "                callbacks=[Report(OUT_DIR)]\n",
        "            )\n",
        "else:\n",
        "    print(\"CAUSAL Generation\")\n",
        "    trainer = MyTrainer(\n",
        "                model=model,\n",
        "                args=training_args,\n",
        "                train_dataset=hf_tokenized[\"train\"],\n",
        "                eval_dataset=hf_tokenized[\"test\"],\n",
        "                processing_class=tokenizer,\n",
        "                data_collator=data_collator,\n",
        "                compute_metrics=compute_metrics,\n",
        "                callbacks=[Report(OUT_DIR), early_callback]\n",
        "            )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UojACT4JJ6Ay",
        "outputId": "bda716fb-9292-48bc-f5ee-d9464f454b6f"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "5ba49f79e3d2462baf2deaf3898bea07"
          ]
        },
        "id": "Mvm49C6vJ6Az",
        "outputId": "dbb38e13-993c-4b56-8056-d4e1557a9628"
      },
      "outputs": [],
      "source": [
        "# Sets model in evaluation mode and moves it on device\n",
        "model = model.eval()\n",
        "model = model.to(device)\n",
        "\n",
        "# Creates DataLoader\n",
        "hf_tokenized.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
        "loader = torch.utils.data.DataLoader(hf_tokenized[\"test\"], batch_size=8)\n",
        "\n",
        "\n",
        "print(f\"Start generation on {device}\")\n",
        "print(\"=============================\")\n",
        "\n",
        "for batch in tqdm(loader):\n",
        "    # Move the entire batch to the device\n",
        "    # Note: DataLoader returns a batch as a dictionary of tensors\n",
        "\n",
        "    batch[\"input_ids\"] = batch[\"input_ids\"].to(device)\n",
        "    batch[\"attention_mask\"] = batch[\"attention_mask\"].to(device)\n",
        "\n",
        "    # Generate output for the batch\n",
        "    # Note: The model.generate method expects input_ids and attention_mask\n",
        "    result_ids = model.generate(\n",
        "        input_ids=batch[\"input_ids\"],\n",
        "        attention_mask=batch[\"attention_mask\"],\n",
        "        max_new_tokens=max_length,\n",
        "    )\n",
        "\n",
        "    # Decodes separately each prompt and each generated result\n",
        "    # Note: result_ids has shape (batch_size, seq_len_output)\n",
        "    # Iterate on batch to decode one by one\n",
        "    # batch[\"input_ids\"] has shape (batch_size, seq_len_input)\n",
        "\n",
        "    # Decode original prompts\n",
        "    decoded_prompts = tokenizer.batch_decode(batch[\"input_ids\"], skip_special_tokens=True)\n",
        "\n",
        "    # Decode gnerated results\n",
        "    decoded_results = tokenizer.batch_decode(result_ids, skip_special_tokens=True)\n",
        "\n",
        "    # Prints results for each batch element\n",
        "    with open(OUT_DIR + \"/output_chat.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "        for i in range(len(decoded_prompts)):\n",
        "            print(f\"===:{i}-(Model for Prompt)===\")\n",
        "            print(f\"{decoded_prompts[i]}\")\n",
        "            print(\"=========================\")\n",
        "\n",
        "            print(f\"===:(model {network}):===\")\n",
        "            print(decoded_results[i])\n",
        "            print(\"=================================\")\n",
        "\n",
        "            ############################################\n",
        "\n",
        "\n",
        "            f.write(\"f===:({i}Model for Prompt)===\")\n",
        "            f.write(f\"{decoded_prompts[i]}\")\n",
        "            f.write(\"=========================\")\n",
        "\n",
        "            f.write(f\"===:(model {network}):===\")\n",
        "            f.write(decoded_results[i])\n",
        "            f.write(\"==========================\")\n",
        "\n",
        "print(\"\\nGeneration completed.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "MNLP",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
