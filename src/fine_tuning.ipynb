{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translation from Ancient to Modern Italian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import Datases to work with Transformers by Hugging-Face\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "# Imports for Transformers\n",
    "from transformers import AutoTokenizer  # Datasets\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "import numpy as np  # Evaluation\n",
    "import evaluate\n",
    "from datasets import Dataset, load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from transformers import TrainerCallback\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType, PeftModelForSeq2SeqLM # Optimize traning for big models! (more than 1B parameters)\n",
    "import numpy as np\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Report(TrainerCallback):\n",
    "    \"\"\"\n",
    "    Personalized callback to draw loss and metrics graphs.\n",
    "    \"\"\"\n",
    "    def __init__(self, plotting_dir=\"./training_plots\"):\n",
    "        self.plotting_dir = plotting_dir\n",
    "        self.log_history = []\n",
    "        os.makedirs(self.plotting_dir, exist_ok=True)\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Event called after logging the last metrics.\n",
    "        Collects loss and metrics data.\n",
    "        \"\"\"\n",
    "        if logs is not None:\n",
    "            self.log_history.append(logs)\n",
    "\n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        \"\"\"\n",
    "        Event called at the end of training.\n",
    "        Draws and saves the graphs.\n",
    "        \"\"\"\n",
    "        print(\"Training done. Generating graphs...\")\n",
    "\n",
    "        train_losses = []\n",
    "        eval_losses = []\n",
    "        eval_metrics = {}\n",
    "        global_steps = []\n",
    "        epochs = []\n",
    "\n",
    "        for log in self.log_history:\n",
    "            # Collect training loss (recorded at logging_steps)\n",
    "            if 'loss' in log:\n",
    "                train_losses.append(log['loss'])\n",
    "                global_steps.append(log.get('step', None)) # Use 'step' if available\n",
    "            # Collect evaluation metrics (recorded at evaluation_strategy)\n",
    "            elif 'eval_loss' in log:\n",
    "                eval_losses.append(log['eval_loss'])\n",
    "                epochs.append(log.get('epoch', None)) # Use 'epoch' if available\n",
    "                for key, value in log.items():\n",
    "                    if key.startswith('eval_') and key != 'eval_loss' and isinstance(value, (int, float)):\n",
    "                        if key not in eval_metrics:\n",
    "                            eval_metrics[key] = []\n",
    "                        eval_metrics[key].append(value)\n",
    "\n",
    "        # Remove None from global_steps if not uniformely available\n",
    "        if not all(step is None for step in global_steps):\n",
    "            # Only filters log containing step for training loss\n",
    "            train_logs_with_step = [(log['loss'], log['step']) for log in self.log_history if 'loss' in log and 'step' in log]\n",
    "            train_losses = [log[0] for log in train_logs_with_step]\n",
    "            global_steps = [log[1] for log in train_logs_with_step]\n",
    "        else:\n",
    "            global_steps = list(range(len(train_losses))) # Use range if steps are not logged\n",
    "\n",
    "        # Remove None from epochs if not uniformely available\n",
    "        if not all(epoch is None for epoch in epochs):\n",
    "            # Only filter log containing epoch for eval metrics\n",
    "            eval_logs_with_epoch = [(log['eval_loss'], log['epoch'], {k:v for k,v in log.items() if k.startswith('eval_') and k != 'eval_loss'}) for log in self.log_history if 'eval_loss' in log and 'epoch' in log]\n",
    "            eval_losses = [log[0] for log in eval_logs_with_epoch]\n",
    "            epochs = [log[1] for log in eval_logs_with_epoch]\n",
    "            eval_metrics = {k: [log[2][k] for log in eval_logs_with_epoch if k in log[2]] for k in eval_metrics.keys()}\n",
    "\n",
    "        else:\n",
    "             epochs = list(range(len(eval_losses))) # Use range if epochs are not logged\n",
    "             # Ensure metrics have same length\n",
    "             for key in eval_metrics:\n",
    "                 eval_metrics[key] = eval_metrics[key][:len(epochs)]\n",
    "\n",
    "\n",
    "        # Plot loss\n",
    "        if train_losses or eval_losses:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            if train_losses:\n",
    "                plt.plot(global_steps, train_losses, label='Training Loss')\n",
    "            if eval_losses:\n",
    "                plt.plot(epochs, eval_losses, label='Validation Loss')\n",
    "            plt.xlabel('Step (Training Loss) / Epoch (Validation Loss)')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.title('Training and Validation Loss')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.savefig(os.path.join(self.plotting_dir, \"loss_graph.png\"))\n",
    "            # plt.show()\n",
    "            plt.close()\n",
    "\n",
    "        # Plot evaluation metrics\n",
    "        for metric_name, metric_values in eval_metrics.items():\n",
    "            if metric_values:\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                plt.plot(epochs, metric_values, label=metric_name)\n",
    "                plt.xlabel('Epoch')\n",
    "                plt.ylabel(metric_name.replace('eval_', '').capitalize())\n",
    "                plt.title(f'Validation Metric: {metric_name.replace(\"eval_\", \"\").capitalize()}')\n",
    "                plt.legend()\n",
    "                plt.grid(True)\n",
    "                plt.savefig(os.path.join(self.plotting_dir, f\"{metric_name}.png\"))\n",
    "                # plt.show()\n",
    "                plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = ('cuda' if torch.cuda.is_available() else \"cpu\")\n",
    "DATASET = \"the_old_english_dataset.csv\"\n",
    "PROMPT = \"translate OldEnglish to English: \"\n",
    "SRC_L = \"ang\"\n",
    "TRG_L = \"en\"\n",
    "network = \"google-t5/t5-base\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATASET, sep=\",\", index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length mean ang text: 58.496155585707825\n",
      "length mean en text: 81.98507462686567\n"
     ]
    }
   ],
   "source": [
    "print(f\"length mean {SRC_L} text: {df[SRC_L].apply(lambda x: len(x.split())).mean()}\")\n",
    "print(f\"length mean {TRG_L} text: {df[TRG_L].apply(lambda x: len(x.split())).mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "start",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "end",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "text_name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "new_match",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "original_match",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "en",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "ang",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "len_translation",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "len_original",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "len_diff",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "abd6fafd-0a02-4a25-8f3f-d1073a05f9e8",
       "rows": [
        [
         "0",
         "0",
         "0",
         "alms_giving.txt",
         "(0-0)",
         null,
         "It will be well for that earl who keeps inside himself, the right-thinking man, a roomy heart — so that the most of honorable intentions will be the greatest glory for the world and for our Lord. Even so this man extinguishes the flame with the welling waters, so that he cannot for long be injured in the cities with the burning brightness so he with almsdeeds shoves away entirely the wounds of sinfulness, healing the soul.",
         " Wel bið þam eorle þe him on innan hafað, reþehygdig wer, rume heortan; þæt him biþ for worulde weorðmynda mæst, ond for ussum dryhtne doma selast. Efne swa he mid wætre þone weallendan leg adwæsce, þæt he leng ne mæg blac byrnende burgum sceððan, swa he mid ælmessan ealle toscufeð synna wunde, sawla lacnað. ",
         "77",
         "54",
         "23"
        ],
        [
         "1",
         "628",
         "631",
         "andreas.txt",
         "(628-631)",
         "(628-31)",
         "And so Andrew gave answer: “What are you asking me, most beloved lord, in elaborate words, when you perceive the truth of each word by the skill of the wise?”",
         "Him þa Andreas ondsware agef: \"Hwæt frinest ðu me, frea leofesta, wordum wrætlicum, ond þe wyrda gehwære þurh snyttra cræft soð oncnawest?\"",
         "30",
         "22",
         "8"
        ],
        [
         "2",
         "977",
         "980",
         "andreas.txt",
         "(977-980)",
         "(977-80)",
         "Then the holy one departed from him, seeking the heavens, the King of All Kings, that pure home, with humility upwards, where there is mercy belonging to every man, to those who know how to find it.",
         "Gewat him þa se halga heofonas secan, eallra cyninga cining, þone clænan ham, eaðmedum upp, þær is ar gelang fira gehwylcum, þam þe hie findan cann.",
         "37",
         "26",
         "11"
        ],
        [
         "3",
         "981",
         "996a",
         "andreas.txt",
         "(981-996a)",
         "(981-96a)",
         "Then Andrew, soul-patient and mindful, a warrior hard for battle, was bolstered in his courage— he went quickly into the city, a single-minded contestant. Powerful and stout of mind and true to his creator, he stepped down the street, the path guiding him— so no man could recognize him nor the sinful see him. The Guardian of Victories had prudently concealed the beloved folk-prince from sight with his hand inside the city. When noble Andrew had pressed inwards, Christ’s champion, near to the prison, he saw a heap of heathens together, herdsmen standing before the grated door, seven at once. Death seized them suddenly, they fell ingloriously— the fatal rush grasping the sword-bloody warriors.",
         "ða wæs gemyndig modgeþyldig, beorn beaduwe heard, eode in burh hraðe, anræd oretta, elne gefyrðred, maga mode rof, meotude getreowe, stop on stræte, (stig wisode), swa him nænig gumena ongitan ne mihte, synfulra geseon. Hæfde sigora weard on þam wangstede wære betolden leofne leodfruman mid lofe sinum. Hæfde þa se æðeling in geþrungen, Cristes cempa, carcerne neh. Geseh he hæðenra hloð ætgædere, fore hlindura hyrdas standan, seofone ætsomne. Ealle swylt fornam, druron domlease. Deaðræs forfeng hæleð heorodreorige.",
         "114",
         "77",
         "37"
        ],
        [
         "4",
         "996b",
         "1003",
         "andreas.txt",
         "(996b-1003)",
         "(996b-1003)",
         "Then the holy one prayed to the merciful father from his inmost thoughts, praising the Heaven-King’s Majesty on high, God’s sovereignty. The prison door buckled at once through the hand-grip of the Holy Ghost, and there he went in, mindful of courage, the battle-brave man. The heathens slept, drunk in blood, reddening the death-hall.",
         "ða se halga gebæd bilwytne fæder, breostgehygdum herede on hehðo heofoncyninges þrym, godes dryhtendom. Duru sona onarn þurh handhrine haliges gastes, ond þær in eode, elnes gemyndig, hæle hildedeor. Hæðene swæfon, dreore druncne, deaðwang rudon.",
         "54",
         "35",
         "19"
        ]
       ],
       "shape": {
        "columns": 10,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>text_name</th>\n",
       "      <th>new_match</th>\n",
       "      <th>original_match</th>\n",
       "      <th>en</th>\n",
       "      <th>ang</th>\n",
       "      <th>len_translation</th>\n",
       "      <th>len_original</th>\n",
       "      <th>len_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>alms_giving.txt</td>\n",
       "      <td>(0-0)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>It will be well for that earl who keeps inside...</td>\n",
       "      <td>Wel bið þam eorle þe him on innan hafað, reþe...</td>\n",
       "      <td>77</td>\n",
       "      <td>54</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>628</td>\n",
       "      <td>631</td>\n",
       "      <td>andreas.txt</td>\n",
       "      <td>(628-631)</td>\n",
       "      <td>(628-31)</td>\n",
       "      <td>And so Andrew gave answer: “What are you askin...</td>\n",
       "      <td>Him þa Andreas ondsware agef: \"Hwæt frinest ðu...</td>\n",
       "      <td>30</td>\n",
       "      <td>22</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>977</td>\n",
       "      <td>980</td>\n",
       "      <td>andreas.txt</td>\n",
       "      <td>(977-980)</td>\n",
       "      <td>(977-80)</td>\n",
       "      <td>Then the holy one departed from him, seeking t...</td>\n",
       "      <td>Gewat him þa se halga heofonas secan, eallra c...</td>\n",
       "      <td>37</td>\n",
       "      <td>26</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>981</td>\n",
       "      <td>996a</td>\n",
       "      <td>andreas.txt</td>\n",
       "      <td>(981-996a)</td>\n",
       "      <td>(981-96a)</td>\n",
       "      <td>Then Andrew, soul-patient and mindful, a warri...</td>\n",
       "      <td>ða wæs gemyndig modgeþyldig, beorn beaduwe hea...</td>\n",
       "      <td>114</td>\n",
       "      <td>77</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>996b</td>\n",
       "      <td>1003</td>\n",
       "      <td>andreas.txt</td>\n",
       "      <td>(996b-1003)</td>\n",
       "      <td>(996b-1003)</td>\n",
       "      <td>Then the holy one prayed to the merciful fathe...</td>\n",
       "      <td>ða se halga gebæd bilwytne fæder, breostgehygd...</td>\n",
       "      <td>54</td>\n",
       "      <td>35</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  start   end        text_name    new_match original_match  \\\n",
       "0     0     0  alms_giving.txt        (0-0)            NaN   \n",
       "1   628   631      andreas.txt    (628-631)       (628-31)   \n",
       "2   977   980      andreas.txt    (977-980)       (977-80)   \n",
       "3   981  996a      andreas.txt   (981-996a)      (981-96a)   \n",
       "4  996b  1003      andreas.txt  (996b-1003)    (996b-1003)   \n",
       "\n",
       "                                                  en  \\\n",
       "0  It will be well for that earl who keeps inside...   \n",
       "1  And so Andrew gave answer: “What are you askin...   \n",
       "2  Then the holy one departed from him, seeking t...   \n",
       "3  Then Andrew, soul-patient and mindful, a warri...   \n",
       "4  Then the holy one prayed to the merciful fathe...   \n",
       "\n",
       "                                                 ang  len_translation  \\\n",
       "0   Wel bið þam eorle þe him on innan hafað, reþe...               77   \n",
       "1  Him þa Andreas ondsware agef: \"Hwæt frinest ðu...               30   \n",
       "2  Gewat him þa se halga heofonas secan, eallra c...               37   \n",
       "3  ða wæs gemyndig modgeþyldig, beorn beaduwe hea...              114   \n",
       "4  ða se halga gebæd bilwytne fæder, breostgehygd...               54   \n",
       "\n",
       "   len_original  len_diff  \n",
       "0            54        23  \n",
       "1            22         8  \n",
       "2            26        11  \n",
       "3            77        37  \n",
       "4            35        19  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Models\n",
    "* RNN (GRU-cell + attention) : [related paper](https://arxiv.org/pdf/1704.08430)\n",
    "* Text-Generator(prompt2text) : **google/flan-t5-base**\n",
    "* Machine Translator : **google-t5/t5-base**\n",
    "* LLM : **openai-community/gpt2-medium**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Env Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install additional libs required for traning/testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Login on Hugging-Face (to download pre-trained network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ang': ['Gehyge þu, frea min, fæstlicne ræd. Syle ælmyssan, wes earmra hleo, þinga for ðeodne, ær ðam seo þrah cyme þæt he þec aworpe of woruldrice. Oft metod alæt monige ðeode wyrcan bote, þonne hie woldon sylfe, fyrene fæstan, ær him fær godes þurh egesan gryre aldre gesceode.\"',\n",
       "  'Geseah ic þæt fuse beacen wendan wædum ond bleom; hwilum hit wæs mid wætan bestemed, beswyled mid swates gange, hwilum mid since gegyrwed.',\n",
       "  'We bi sumum hyrdon wrætlice gecynd wildra secgan firum freamærne feorlondum on',\n",
       "  'þæt fram ham gefrægn Higelaces þegn, god mid Geatum, Grendles dæda; se wæs moncynnes mægenes strengest on þæm dæge þysses lifes, æþele ond eacen. Het him yðlidan godne gegyrwan, cwæð, he guðcyning ofer swanrade secean wolde, mærne þeoden, þa him wæs manna þearf. ðone siðfæt him snotere ceorlas lythwon logon, þeah he him leof wære; hwetton higerofne, hæl sceawedon.',\n",
       "  'Mæg ic be me sylfum soðgied wrecan, siþas secgan, hu ic geswincdagum earfoðhwile oft þrowade, bitre breostceare gebiden hæbbe, gecunnad in ceole cearselda fela, atol yþa gewealc, þær mec oft bigeat nearo nihtwaco æt nacan stefnan, þonne he be clifum cnossað.'],\n",
       " 'en': ['“Consider, my lord, this steadfast advice. Give out alms, become a shelter to the wretched, make entreaty before the Lord, before the time comes that he should cast you down from worldly rule. Often the Measurer pardons many peoples that perform their cure, when they are willing themselves, repenting their crimes before the onslaught of God through his terrible fear, should scathe their lives.”',\n",
       "  'I witnessed the change, the streaking beacon, warping its own in clad & color: sometimes it was blood steaming, swilling in trills & rills of ruddy sweat; sometimes it was bedazzled with richness.',\n",
       "  'We have heard spoken by certain men a wondrous species of wild beast, illustrious to men, in distant lands',\n",
       "  'Among his own Hygelac’s thane had heard, good among the Geats, about the deeds of Grendel. He was the strongest of power among mankind in those days of this life, noble and well-grown. He ordered an excellent wave-glider readied for himself—he stated he wished to seek the war-king across the swan-road, the famous prince who stood in need of men. Wise retainers reproached him but little about that mission, though he was loved by them, whetting his mighty spirit and peering at the portents.',\n",
       "  'I can talk treks, truth-tales twisting — every word my wracking — how I buckle underneath these hours of fuckeries, long dredging days — grudged to the gall, the stew of my guts — tracking down plenty: seats of sorrow-bound ships — the roiling awful of waves, where clutching night-watches poured over me, catching at dromund prow, dashed smashed upon stone.']}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets.features import Value, Features\n",
    "hf = Dataset.from_csv(DATASET, features=\n",
    "    Features({\n",
    "        SRC_L : Value(\"string\"),\n",
    "        TRG_L : Value(\"string\")\n",
    "    })          \n",
    "                      \n",
    "    ).shuffle(2025).train_test_split(test_size=0.15)\n",
    "\n",
    "hf[\"train\"].take(5)[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "#hf = load_dataset(\"grosenthal/latin_english_translation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(network)\n",
    "\n",
    "class Preprocessor:\n",
    "    def __init__(self, PREFIX, src_lan, dest_lan, max_length):\n",
    "        self.PREFIX = PREFIX\n",
    "        self.src_lan = src_lan\n",
    "        self.dest_lan = dest_lan\n",
    "        self.ml = max_length\n",
    "    \n",
    "    def __call__(self, examples):\n",
    "        inputs = [self.PREFIX + example for example in examples[self.src_lan]]\n",
    "        targets = [example for example in examples[self.dest_lan]]\n",
    "        \n",
    "        model_inputs = tokenizer(inputs, text_target=targets, max_length=self.ml, truncation=True)\n",
    "        \n",
    "        return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prc = Preprocessor(PROMPT, SRC_L, TRG_L, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea8587973f0e4a2a9d8c71598bfd0164",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1879 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5adc39b4e8e74ed4924e049daf18fa64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/332 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['ang', 'en', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 1879\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['ang', 'en', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 332\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "hf_tokenized = hf.map(prc, batched=True)\n",
    "print(hf_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = hf_tokenized[\"train\"]['input_ids'][1]\n",
    "target = hf_tokenized[\"train\"]['labels'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "translate OldEnglish to English: Geseah ic t fuse beacen wendan wdum ond bleom; hwilum hit ws mid wtan bestemed, beswyled mid s\n",
      "I witnessed the change, the streaking beacon, warping its own in clad & color: sometimes it was blood steaming, swilling in trills & rills of ruddy sweat; sometimes it was bedazzled with richness.\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(src, skip_special_tokens=True))\n",
    "print(tokenizer.decode(target, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/andrea/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/andrea/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/andrea/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "sacrebleu_metric = evaluate.load(\"sacrebleu\")\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "meteor_metric = evaluate.load(\"meteor\")\n",
    "chrf_metric = evaluate.load(\"chrf\")\n",
    "ter_metric = evaluate.load(\"ter\")\n",
    "\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels] # Specific format for SacreBLEU\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds_input, label_ids = eval_preds\n",
    "\n",
    "    # Dealing with logits or token IDs for predictions\n",
    "    # If preds_input are logits (es. direct output of training modello)\n",
    "    current_preds = preds_input\n",
    "    if isinstance(current_preds, tuple): # Common in HF Trainer, es. (logits, hidden_states)\n",
    "        current_preds = current_preds[0]\n",
    "    \n",
    "    if hasattr(current_preds, \"ndim\") and current_preds.ndim == 3: # Array of logits (batch_size, seq_len, vocab_size)\n",
    "        current_preds_ids = np.argmax(current_preds, axis=-1)\n",
    "    else: # Otherwise, assumed to be token ID (batch_size, seq_len)\n",
    "        current_preds_ids = current_preds\n",
    "\n",
    "    # Decode predictions and labels\n",
    "    decoded_preds_raw = tokenizer.batch_decode(current_preds_ids, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100 in labels (common for token to be ignored) with pad_token_id for decoding\n",
    "    processed_label_ids = np.where(label_ids != -100, label_ids, tokenizer.pad_token_id)\n",
    "    decoded_labels_raw = tokenizer.batch_decode(processed_label_ids, skip_special_tokens=True)\n",
    "    \n",
    "    processed_preds, processed_labels_for_sacrebleu = postprocess_text(decoded_preds_raw, decoded_labels_raw)\n",
    "\n",
    "    # For other metrics (ROUGE, METEOR, CHRF, TER), usually expects a flat list of reference strings\n",
    "    flat_references = [ref[0] for ref in processed_labels_for_sacrebleu]\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # 1. SacreBLEU\n",
    "    sacrebleu_output = sacrebleu_metric.compute(predictions=processed_preds, references=processed_labels_for_sacrebleu)\n",
    "    if sacrebleu_output and \"score\" in sacrebleu_output:\n",
    "        results[\"bleu\"] = sacrebleu_output[\"score\"]\n",
    "    else:\n",
    "        results[\"bleu\"] = 0.0 # Fallback\n",
    "\n",
    "    # 2. ROUGE (rouge1, rouge2, rougeL, rougeLsum)\n",
    "    rouge_output = rouge_metric.compute(predictions=processed_preds, references=flat_references, use_stemmer=True)\n",
    "    if rouge_output:\n",
    "        results[\"rouge1\"] = rouge_output.get(\"rouge1\", 0.0)\n",
    "        results[\"rouge2\"] = rouge_output.get(\"rouge2\", 0.0)\n",
    "        results[\"rougeL\"] = rouge_output.get(\"rougeL\", 0.0)\n",
    "        results[\"rougeLsum\"] = rouge_output.get(\"rougeLsum\", 0.0) # Spesso più robusto per sommario\n",
    "    else:\n",
    "        results.update({\"rouge1\": 0.0, \"rouge2\": 0.0, \"rougeL\": 0.0, \"rougeLsum\": 0.0})\n",
    "\n",
    "    # 3. METEOR\n",
    "    meteor_output = meteor_metric.compute(predictions=processed_preds, references=flat_references)\n",
    "    if meteor_output and \"meteor\" in meteor_output:\n",
    "        results[\"meteor\"] = meteor_output[\"meteor\"]\n",
    "    else:\n",
    "        results[\"meteor\"] = 0.0\n",
    "\n",
    "    # 4. CHRF++ (CHRF with n-grams of words)\n",
    "    # For CHRF++, word_order (or word_n) is > 0. Default of evaluate.load('chrf') are word_order=0 (CHRF standard).\n",
    "    # Common parameters for CHRF++: word_order=2, beta=2 (beta=2 default)\n",
    "    chrf_output = chrf_metric.compute(predictions=processed_preds, references=flat_references, word_order=2, beta=2)\n",
    "    if chrf_output and \"score\" in chrf_output:\n",
    "        results[\"chrf++\"] = chrf_output[\"score\"] # CHRF++ score\n",
    "    else:\n",
    "        results[\"chrf++\"] = 0.0\n",
    "        \n",
    "    # (Optional) CHRF standard (only characters)\n",
    "    # chrf_std_output = chrf_metric.compute(predictions=processed_preds, references=flat_references, word_order=0)\n",
    "    # if chrf_std_output and \"score\" in chrf_std_output:\n",
    "    #     results[\"chrf\"] = chrf_std_output[\"score\"]\n",
    "    # else:\n",
    "    #     results[\"chrf\"] = 0.0\n",
    "\n",
    "    # 5. TER (Translation Edit Rate) - the smaller, the better\n",
    "    ter_output = ter_metric.compute(predictions=processed_preds, references=flat_references)\n",
    "    if ter_output and \"score\" in ter_output:\n",
    "        results[\"ter\"] = ter_output[\"score\"]\n",
    "    else:\n",
    "        results[\"ter\"] = 1.0 # Fallback on worst score TER possible\n",
    "\n",
    "    # Mean length of generated predictions (excluding padding tokens)\n",
    "    # 'current_preds_ids' are ID token of the predictions\n",
    "    prediction_lengths = [np.count_nonzero(pid_seq != tokenizer.pad_token_id) for pid_seq in current_preds_ids]\n",
    "    results[\"gen_len\"] = np.mean(prediction_lengths) if prediction_lengths else 0.0\n",
    "\n",
    "    # Rounding of all numerical results\n",
    "    final_results = {k: round(v, 4) for k, v in results.items() if isinstance(v, (int, float))}\n",
    "    \n",
    "    return final_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(network)\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PEFT Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 884,736 || all params: 223,788,288 || trainable%: 0.3953\n"
     ]
    }
   ],
   "source": [
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_DIR = network.split(\"/\")[-1]\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "class MyTrainer(Seq2SeqTrainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        if 'num_items_in_batch' in inputs:\n",
    "            inputs = {k: v for k, v in inputs.items() if k != 'num_items_in_batch'}\n",
    "        return super().compute_loss(model, inputs, return_outputs=return_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForSeq2SeqLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=OUT_DIR,\n",
    "    learning_rate=1e-3,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    load_best_model_at_end=False,\n",
    "    report_to=\"none\",\n",
    "    logging_dir=OUT_DIR,\n",
    "    logging_steps=10\n",
    ")\n",
    "\n",
    "trainer = MyTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=hf_tokenized[\"train\"],\n",
    "    eval_dataset=hf_tokenized[\"test\"],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[Report(OUT_DIR)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1180' max='1180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1180/1180 05:49, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Meteor</th>\n",
       "      <th>Chrf++</th>\n",
       "      <th>Ter</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.139600</td>\n",
       "      <td>3.800684</td>\n",
       "      <td>2.188600</td>\n",
       "      <td>0.314300</td>\n",
       "      <td>0.044200</td>\n",
       "      <td>0.251100</td>\n",
       "      <td>0.251100</td>\n",
       "      <td>0.202400</td>\n",
       "      <td>22.351300</td>\n",
       "      <td>84.735000</td>\n",
       "      <td>64.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.880100</td>\n",
       "      <td>3.687197</td>\n",
       "      <td>2.772500</td>\n",
       "      <td>0.320200</td>\n",
       "      <td>0.049800</td>\n",
       "      <td>0.261900</td>\n",
       "      <td>0.261800</td>\n",
       "      <td>0.207900</td>\n",
       "      <td>22.945200</td>\n",
       "      <td>83.655800</td>\n",
       "      <td>64.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.768000</td>\n",
       "      <td>3.629912</td>\n",
       "      <td>3.012900</td>\n",
       "      <td>0.331500</td>\n",
       "      <td>0.054200</td>\n",
       "      <td>0.269100</td>\n",
       "      <td>0.269300</td>\n",
       "      <td>0.217200</td>\n",
       "      <td>23.438800</td>\n",
       "      <td>83.075300</td>\n",
       "      <td>64.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.689800</td>\n",
       "      <td>3.588008</td>\n",
       "      <td>3.571700</td>\n",
       "      <td>0.335100</td>\n",
       "      <td>0.060400</td>\n",
       "      <td>0.273200</td>\n",
       "      <td>0.273200</td>\n",
       "      <td>0.224900</td>\n",
       "      <td>24.669600</td>\n",
       "      <td>83.283700</td>\n",
       "      <td>64.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.623800</td>\n",
       "      <td>3.548133</td>\n",
       "      <td>4.033200</td>\n",
       "      <td>0.343100</td>\n",
       "      <td>0.064500</td>\n",
       "      <td>0.280100</td>\n",
       "      <td>0.280400</td>\n",
       "      <td>0.228300</td>\n",
       "      <td>24.929900</td>\n",
       "      <td>83.023200</td>\n",
       "      <td>64.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.569900</td>\n",
       "      <td>3.521148</td>\n",
       "      <td>4.010800</td>\n",
       "      <td>0.342800</td>\n",
       "      <td>0.065200</td>\n",
       "      <td>0.281000</td>\n",
       "      <td>0.281000</td>\n",
       "      <td>0.231000</td>\n",
       "      <td>25.074600</td>\n",
       "      <td>82.807400</td>\n",
       "      <td>64.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.533000</td>\n",
       "      <td>3.509242</td>\n",
       "      <td>4.405300</td>\n",
       "      <td>0.347100</td>\n",
       "      <td>0.067900</td>\n",
       "      <td>0.284900</td>\n",
       "      <td>0.285100</td>\n",
       "      <td>0.234000</td>\n",
       "      <td>25.720100</td>\n",
       "      <td>82.524600</td>\n",
       "      <td>64.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.495300</td>\n",
       "      <td>3.495327</td>\n",
       "      <td>4.407100</td>\n",
       "      <td>0.350500</td>\n",
       "      <td>0.069100</td>\n",
       "      <td>0.288100</td>\n",
       "      <td>0.288300</td>\n",
       "      <td>0.236400</td>\n",
       "      <td>25.667200</td>\n",
       "      <td>82.107800</td>\n",
       "      <td>64.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.469200</td>\n",
       "      <td>3.488515</td>\n",
       "      <td>4.470600</td>\n",
       "      <td>0.352100</td>\n",
       "      <td>0.070300</td>\n",
       "      <td>0.288400</td>\n",
       "      <td>0.288700</td>\n",
       "      <td>0.237300</td>\n",
       "      <td>25.930000</td>\n",
       "      <td>81.929100</td>\n",
       "      <td>64.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.449900</td>\n",
       "      <td>3.486037</td>\n",
       "      <td>4.633400</td>\n",
       "      <td>0.352300</td>\n",
       "      <td>0.071000</td>\n",
       "      <td>0.289500</td>\n",
       "      <td>0.289700</td>\n",
       "      <td>0.237200</td>\n",
       "      <td>25.935300</td>\n",
       "      <td>81.869600</td>\n",
       "      <td>64.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Addestramento terminato. Generazione dei grafici...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1180, training_loss=3.6618559239274364, metrics={'train_runtime': 349.1349, 'train_samples_per_second': 53.819, 'train_steps_per_second': 3.38, 'total_flos': 1436673534197760.0, 'train_loss': 3.6618559239274364, 'epoch': 10.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.eval()\n",
    "text = hf_tokenized['test'][2][SRC_L]\n",
    "target = hf_tokenized['test'][2][TRG_L]\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "ids = inputs.input_ids.to(device)\n",
    "attention = inputs.attention_mask.to(device)\n",
    "\n",
    "output = model.generate(input_ids=ids,  attention_mask=attention, max_new_tokens=120, do_sample=True, top_k=10, top_p=0.95)\n",
    "output = tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence:\n",
      " Wundor is to secganne hu mihtig god manna cynne þurh sidne sefan snyttru bryttað, eard ond eorlscipe; he ah ealra geweald. Hwilum he on lufan læteð hworfan monnes modgeþonc mæran cynnes, seleð him on eþle eorþan wynne to healdanne, hleoburh wera, gedeð him swa gewealdene worolde dælas, side rice, þæt he his selfa ne mæg for his unsnyttrum ende geþencean. Wunað he on wiste; no hine wiht dweleð adl ne yldo, ne him inwitsorh on sefan sweorceð, ne gesacu ohwær ecghete eoweð, ac him eal worold wendeð on willan (he þæt wyrse ne con),\n",
      "Target sentence:\n",
      " It is a wonder to speak how Mighty God dispenses wisdom to the kindred of men through a spacious soul, a home to command. He owns the power over all creatures. Sometimes he allows the mind-thoughts of man to rove in love of his famous kinsmen, giving him joy on earth — in order to keep well the sheltering stronghold of mortals, lending him such authority over his worldly share, this broad realm, so that he imagines no end for himself in a lapse of wisdom. He lives well at the feast. Nothing stands in his way, not disease or old age, nor do wicked preoccupations darken his soul, nor does conflict or sword-hate show itself anywhere—all the world turns towards his pleasure. He knows not the worst—\n",
      "Tranlated sentence;\n",
      " a wretched savage. Thence he must seek out the battle-dweller, the sword-terrorer, and he must find his way. He can never find any retribution for his sins. He must seek the wrathful lord, a\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original Sentence:\\n {text}\")\n",
    "print(f\"Target Sentence:\\n {target}\")\n",
    "print(f\"Translated Sentence:\\n {output}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MNLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
