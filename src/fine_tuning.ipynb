{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asJxkfJNJ6Ab"
      },
      "source": [
        "# Fine-tuning for Translation from Ancient to Modern Italian"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZXyfYF5KTyH"
      },
      "source": [
        "# System Setup ðŸ–¥ï¸"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWshcyakjQV8"
      },
      "source": [
        "### Drive Interface ðŸ“"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bn2BVRjjKKUZ",
        "outputId": "95a55d51-b204-47e2-85d1-008a0984ccf5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "dataset_ann.csv  fine_tuning.ipynb  PrometheusAPI.py  utils.py\n",
            "dataset.csv      install_colab.sh   prompting.ipynb\n",
            "\u001b[0m\u001b[01;34mdrive\u001b[0m/           Judge.py           \u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "%cp /content/drive/MyDrive/MNLP_HW_2/Many_Naps_Little_Progress/*.* .\n",
        "%ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgbPZ6tqKLDD"
      },
      "source": [
        "### Additional Dependencies ðŸ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNhSDgXeKKBS",
        "outputId": "a0f02e11-0497-4e18-e78b-45ee5a4dd38f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cublas-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cublas-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cublas-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cublas-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cublas-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cublas-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cublas-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!bash install_colab.sh >> /dev/null"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_q2wvHwFKt5F"
      },
      "source": [
        "### Hugging Face ðŸ¤—"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ccb4L42zKuVB"
      },
      "outputs": [],
      "source": [
        "import huggingface_hub\n",
        "TOKEN = \"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\"\n",
        "huggingface_hub.login(token=TOKEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LN3PEzDWJ6Ai"
      },
      "outputs": [],
      "source": [
        "# Import Datases to work with Transformers by Hugging-Face\n",
        "import torch\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "# Imports for Transformers\n",
        "from transformers import AutoTokenizer  # Datasets\n",
        "from datasets import Dataset, DatasetDict\n",
        "from utils import Report\n",
        "import numpy as np\n",
        "import evaluate\n",
        "\n",
        "from transformers import DataCollatorForLanguageModeling, Trainer, TrainingArguments, AutoModelForCausalLM          # imports for causal Learning\n",
        "from transformers import DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer, AutoModelForSeq2SeqLM    # imports for Seq2Seq models\n",
        "from peft import LoraConfig, TaskType, LoftQConfig, PeftModelForSeq2SeqLM, PeftModelForCausalLM, get_peft_model     # imports for quantization methods (LoRA etc...)\n",
        "from transformers import EarlyStoppingCallback\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8kgPZC0J6Ak"
      },
      "source": [
        "# Fine-Tuned Models\n",
        "\n",
        "* google/mt5-base (Machine Translation)\n",
        "* sapienzanlp/Minerva-1B-base-v1.0 ðŸ‡®ðŸ‡¹ (LMM)\n",
        "* sapienzanlp/Minerva-3B-base-v1.0 ðŸ‡®ðŸ‡¹ (LMM)\n",
        "*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QSAlloAIJ6Am"
      },
      "outputs": [],
      "source": [
        "device = ('cuda' if torch.cuda.is_available() else \"cpu\")\n",
        "DATASET = \"dataset_ann.csv\"\n",
        "SRC_L = \"Sentence\"\n",
        "TRG_L = \"Target\"\n",
        "network = \"sapienzanlp/Minerva-3B-base-v1.0\"\n",
        "tokenization_method = \"minerva_base\"\n",
        "OUT_DIR = network.split(\"/\")[-1]\n",
        "EPOCHS = 42\n",
        "BATCH_SIZE = 8\n",
        "max_length = 80"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jEpmxmqJ6Am"
      },
      "source": [
        "# Dataset Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dsfHlEQ8J6An"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(DATASET, sep=\",\", index_col=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aMPFWURdJ6Ao",
        "outputId": "696c4127-f9fb-4b6b-d6c3-e5872a57b404"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "length mean Sentence text: 20.04123711340206\n",
            "length mean Target text: 20.690721649484537\n"
          ]
        }
      ],
      "source": [
        "print(f\"length mean {SRC_L} text: {df[SRC_L].apply(lambda x: len(x.split())).mean()}\")\n",
        "print(f\"length mean {TRG_L} text: {df[TRG_L].apply(lambda x: len(x.split())).mean()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nejry195J6Ap",
        "outputId": "cad8aacb-1bb2-4ddf-f05e-1cea33886c67"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.microsoft.datawrangler.viewer.v0+json": {
              "columns": [
                {
                  "name": "index",
                  "rawType": "int64",
                  "type": "integer"
                },
                {
                  "name": "Author",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "Date",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "Region",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "Sentence",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "Target",
                  "rawType": "object",
                  "type": "string"
                }
              ],
              "ref": "11c6ca48-80de-4e5a-911e-ed1f036a6143",
              "rows": [
                [
                  "0",
                  "Brunetto Latini",
                  "1260-61",
                  "fior.",
                  "quella guerra ben fatta l' opera perchÃ© etc. Et dall' altra parte Aiaces era uno cavaliere franco e prode all' arme, di gran guisa, ma non era pieno di grande senno",
                  "Quella guerra fu ben condotta per via delle azioni compiute, eccetera. Dallâ€™altra parte, Aiace era un cavaliere valoroso e coraggioso nelle armi, di grande fama, ma non era molto saggio nÃ© dotato di grande intelligenza."
                ],
                [
                  "1",
                  "Bono Giamboni",
                  "1292",
                  "fior.",
                  "crudele, e di tutte le colpe pigli vendetta, come dice la legge, ed a neuno cavaliere perdoni che pecchi.",
                  "Ãˆ severo, e punisce tutte le colpe come prescrive la legge, e non perdona nessun cavaliere che commetta errori."
                ],
                [
                  "2",
                  "Valerio Massimo (red. V1",
                  "1336",
                  "fior.",
                  "Non d' altra forza d' animo fue ornato Ponzio Aufidiano, romano cavaliere.",
                  "Ponzio Aufidiano, cavaliere romano, fu dotato dello stesso coraggio dâ€™animo."
                ],
                [
                  "3",
                  "Lucano volg. (ed. Marinoni)",
                  "1330/40",
                  "prat.",
                  "Se questo piace a tutti e se 'l tempo hae bisogno d'avere Pompeio per cavaliere e non per compagno, non riterrÃ² piÃ¹ i fati.",
                  "Se questo Ã¨ quello che tutti desiderano e se l'attuale situazione richiede che Pompeo sia un leader e non solo un alleato, allora non cercherÃ² piÃ¹ di oppormi al destino."
                ],
                [
                  "4",
                  "Brunetto Latini",
                  "1260-61",
                  "fior.",
                  "Officio di questa arte pare che sia dicere appostatamente per fare credere, fine Ã¨ far credere per lo dire.",
                  "Il compito di questâ€™arte sembra essere quello di parlare in modo appropriato per convincere, e di far credere qualcosa attraverso le parole."
                ]
              ],
              "shape": {
                "columns": 5,
                "rows": 5
              }
            },
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Author</th>\n",
              "      <th>Date</th>\n",
              "      <th>Region</th>\n",
              "      <th>Sentence</th>\n",
              "      <th>Target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Brunetto Latini</td>\n",
              "      <td>1260-61</td>\n",
              "      <td>fior.</td>\n",
              "      <td>quella guerra ben fatta l' opera perchÃ© etc. E...</td>\n",
              "      <td>Quella guerra fu ben condotta per via delle az...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Bono Giamboni</td>\n",
              "      <td>1292</td>\n",
              "      <td>fior.</td>\n",
              "      <td>crudele, e di tutte le colpe pigli vendetta, c...</td>\n",
              "      <td>Ãˆ severo, e punisce tutte le colpe come prescr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Valerio Massimo (red. V1</td>\n",
              "      <td>1336</td>\n",
              "      <td>fior.</td>\n",
              "      <td>Non d' altra forza d' animo fue ornato Ponzio ...</td>\n",
              "      <td>Ponzio Aufidiano, cavaliere romano, fu dotato ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Lucano volg. (ed. Marinoni)</td>\n",
              "      <td>1330/40</td>\n",
              "      <td>prat.</td>\n",
              "      <td>Se questo piace a tutti e se 'l tempo hae biso...</td>\n",
              "      <td>Se questo Ã¨ quello che tutti desiderano e se l...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Brunetto Latini</td>\n",
              "      <td>1260-61</td>\n",
              "      <td>fior.</td>\n",
              "      <td>Officio di questa arte pare che sia dicere app...</td>\n",
              "      <td>Il compito di questâ€™arte sembra essere quello ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                        Author     Date Region  \\\n",
              "0              Brunetto Latini  1260-61  fior.   \n",
              "1                Bono Giamboni     1292  fior.   \n",
              "2     Valerio Massimo (red. V1     1336  fior.   \n",
              "3  Lucano volg. (ed. Marinoni)  1330/40  prat.   \n",
              "4              Brunetto Latini  1260-61  fior.   \n",
              "\n",
              "                                            Sentence  \\\n",
              "0  quella guerra ben fatta l' opera perchÃ© etc. E...   \n",
              "1  crudele, e di tutte le colpe pigli vendetta, c...   \n",
              "2  Non d' altra forza d' animo fue ornato Ponzio ...   \n",
              "3  Se questo piace a tutti e se 'l tempo hae biso...   \n",
              "4  Officio di questa arte pare che sia dicere app...   \n",
              "\n",
              "                                              Target  \n",
              "0  Quella guerra fu ben condotta per via delle az...  \n",
              "1  Ãˆ severo, e punisce tutte le colpe come prescr...  \n",
              "2  Ponzio Aufidiano, cavaliere romano, fu dotato ...  \n",
              "3  Se questo Ã¨ quello che tutti desiderano e se l...  \n",
              "4  Il compito di questâ€™arte sembra essere quello ...  "
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8ivbkfnJ6Aq"
      },
      "source": [
        "## Model Selection\n",
        "\n",
        "Configure Pipline for model select for fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "c802734e29674453b4553504f77407da"
          ]
        },
        "id": "zw802PcHJ6Ar",
        "outputId": "b7bf3d15-5c77-4c74-ada1-5a78fc0b0e3a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c802734e29674453b4553504f77407da",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 34,078,720 || all params: 2,928,314,880 || trainable%: 1.1638\n"
          ]
        }
      ],
      "source": [
        "# Switch to select the network and load the appropriate model and tokenizer\n",
        "match network:\n",
        "\n",
        "    case \"sapienzanlp/Minerva-3B-base-v1.0\":\n",
        "        from transformers import BitsAndBytesConfig\n",
        "        bnb_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_compute_dtype=torch.bfloat16,  # o torch.float16 se bfloat16 non Ã¨ supportato\n",
        "        )\n",
        "        tokenizer = AutoTokenizer.from_pretrained(network)\n",
        "        if tokenizer.pad_token is None:\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "        model = AutoModelForCausalLM.from_pretrained(network, device_map=\"auto\", quantization_config=bnb_config)\n",
        "        data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        "\n",
        "        lora_config = LoraConfig(\n",
        "            task_type=TaskType.CAUSAL_LM,\n",
        "            inference_mode=False,\n",
        "            r=128,\n",
        "            lora_alpha=128,\n",
        "            lora_dropout=0.50\n",
        "            )\n",
        "\n",
        "        qlora_config = LoraConfig(\n",
        "            init_lora_weights=\"loftq\",\n",
        "            loftq_config=LoftQConfig(loftq_bits=4),\n",
        "            r=8,\n",
        "            lora_alpha=16,\n",
        "            target_modules=\"all-linear\",\n",
        "            lora_dropout=0.5,\n",
        "            bias=\"none\",\n",
        "            task_type=\"CAUSAL_LM\"\n",
        "        )\n",
        "\n",
        "        model = get_peft_model(model, lora_config)\n",
        "        model.print_trainable_parameters()\n",
        "\n",
        "\n",
        "\n",
        "        early_stopping_patience = 10\n",
        "        early_stopping_threshold = 0.01\n",
        "\n",
        "        early_callback = EarlyStoppingCallback(\n",
        "            early_stopping_patience=early_stopping_patience, # Se la loss di valutazione non migliora per 3 epoche consecutive\n",
        "            early_stopping_threshold=early_stopping_threshold # Ignora miglioramenti inferiori a 0.001\n",
        "        )\n",
        "\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=OUT_DIR,\n",
        "            learning_rate=3e-5,\n",
        "            weight_decay=1e-2,\n",
        "            warmup_steps=500,\n",
        "            per_device_train_batch_size=BATCH_SIZE,\n",
        "            per_device_eval_batch_size=BATCH_SIZE,\n",
        "            num_train_epochs=EPOCHS,\n",
        "            eval_strategy=\"epoch\",\n",
        "            save_strategy=\"epoch\",\n",
        "            logging_strategy=\"epoch\",\n",
        "            load_best_model_at_end=True,\n",
        "            report_to=\"none\",\n",
        "            save_total_limit=3,\n",
        "            lr_scheduler_type=\"cosine\",\n",
        "            logging_dir=OUT_DIR,\n",
        "            logging_steps=10,\n",
        "            label_names=['labels'],\n",
        "            metric_for_best_model=\"eval_chrf++\",\n",
        "            greater_is_better=True,\n",
        "        )\n",
        "\n",
        "        params = {\n",
        "\n",
        "            #\"max_new_tokens\": max_length, # max number of new tokens to generate\n",
        "            #\"do_sample\":True,      # enables sampling for more diverse outputs\n",
        "            #\"top_k\":100,            # diversity increase by controlling the candidate words\n",
        "            #\"top_p\":0.95,          # nucleus sampling for further control over variety\n",
        "            #\"temperature\":1.0,     # reduces randomness and increases coherence\n",
        "            #\"repetition_penalty\":1.0,  # penalizza ripetizioni\n",
        "            #\"num_return_sequences\":10,  # number of generated responses\n",
        "            \"pad_token_id\":tokenizer.eos_token_id  # avoids warning if padding token is missing\n",
        "        }\n",
        "    case \"google/mt5-base\" | \"google/mt5-large\":\n",
        "        tokenizer = AutoTokenizer.from_pretrained(network)\n",
        "        if tokenizer.pad_token is None:\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "\n",
        "        model = AutoModelForSeq2SeqLM.from_pretrained(network, device_map=device, torch_dtype=torch.float32)\n",
        "        data_collator = DataCollatorForSeq2Seq(tokenizer, network)\n",
        "\n",
        "        lora_config = LoraConfig(\n",
        "            task_type=TaskType.SEQ_2_SEQ_LM,\n",
        "            inference_mode=False,\n",
        "            r=64,\n",
        "            lora_alpha=64,\n",
        "            lora_dropout=0.3\n",
        "            )\n",
        "\n",
        "        qlora_config = LoraConfig(\n",
        "            init_lora_weights=\"loftq\",\n",
        "            loftq_config=LoftQConfig(loftq_bits=4),\n",
        "            r=128,\n",
        "            lora_alpha=128*2,\n",
        "            target_modules=\"all-linear\",\n",
        "            lora_dropout=0.3,\n",
        "            bias=\"none\",\n",
        "            task_type=TaskType.SEQ_2_SEQ_LM\n",
        "        )\n",
        "\n",
        "        model = get_peft_model(model, lora_config)\n",
        "        model.print_trainable_parameters()\n",
        "\n",
        "        early_stopping_patience = 3\n",
        "        early_stopping_threshold = 0.01\n",
        "\n",
        "        early_callback = EarlyStoppingCallback(\n",
        "            early_stopping_patience=early_stopping_patience, # Se la loss di valutazione non migliora per 3 epoche consecutive\n",
        "            early_stopping_threshold=early_stopping_threshold # Ignora miglioramenti inferiori a 0.001\n",
        "        )\n",
        "\n",
        "        training_args = Seq2SeqTrainingArguments(\n",
        "            output_dir=OUT_DIR,\n",
        "            learning_rate=4e-4,\n",
        "            weight_decay=3e-4,\n",
        "            warmup_steps=500,\n",
        "            per_device_train_batch_size=BATCH_SIZE,\n",
        "            per_device_eval_batch_size=BATCH_SIZE,\n",
        "            num_train_epochs=EPOCHS,\n",
        "            eval_strategy=\"epoch\",\n",
        "            save_strategy=\"epoch\",\n",
        "            logging_strategy=\"epoch\",\n",
        "            load_best_model_at_end=True,\n",
        "            report_to=\"none\",\n",
        "            save_total_limit=3,\n",
        "            lr_scheduler_type=\"linear\",\n",
        "            logging_dir=OUT_DIR,\n",
        "            logging_steps=10,\n",
        "            label_names=['labels'],\n",
        "            metric_for_best_model=\"eval_chrf++\",\n",
        "            greater_is_better=True,\n",
        "        )\n",
        "\n",
        "        params = {\n",
        "\n",
        "            \"max_new_tokens\": max_length, # max number of new tokens to generate\n",
        "            \"do_sample\":True,      # enables sampling for more diverse outputs\n",
        "            #\"top_k\":100,            # diversity increase by controlling the candidate words\n",
        "            #\"top_p\":0.95,          # nucleus sampling for further control over variety\n",
        "            #\"temperature\":1.0,     # reduces randomness and increases coherence\n",
        "            #\"repetition_penalty\":1.0,  # penalizza ripetizioni\n",
        "            #\"num_return_sequences\":10,  # number of generated responses\n",
        "            \"pad_token_id\":tokenizer.eos_token_id  # avoids warning if padding token is missing\n",
        "        }\n",
        "    case _:\n",
        "        raise Exception(f\"Rete {network} non testabile\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xFz9vXsgJ6As"
      },
      "outputs": [],
      "source": [
        "class MyTrainerSeq2Seq(Seq2SeqTrainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
        "        if 'num_items_in_batch' in inputs:\n",
        "            inputs = {k: v for k, v in inputs.items() if k != 'num_items_in_batch'}\n",
        "        return super().compute_loss(model, inputs, return_outputs=return_outputs)\n",
        "\n",
        "class MyTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
        "        if 'num_items_in_batch' in inputs:\n",
        "            inputs = {k: v for k, v in inputs.items() if k != 'num_items_in_batch'}\n",
        "        return super().compute_loss(model, inputs, return_outputs=return_outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iPt0wW7VJ6At",
        "outputId": "714c0a63-d816-4373-dac9-bbb95acfb8b5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /home/andrea/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /home/andrea/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /home/andrea/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "sacrebleu_metric = evaluate.load(\"sacrebleu\")\n",
        "rouge_metric = evaluate.load(\"rouge\")\n",
        "meteor_metric = evaluate.load(\"meteor\")\n",
        "chrf_metric = evaluate.load(\"chrf\")\n",
        "ter_metric = evaluate.load(\"ter\")\n",
        "\n",
        "\n",
        "def postprocess_text(preds, labels):\n",
        "    preds = [pred.strip() for pred in preds]\n",
        "    labels = [[label.strip()] for label in labels] # Specific format for SacreBLEU\n",
        "    return preds, labels\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    preds_input, label_ids = eval_preds\n",
        "\n",
        "    # Dealing with logits or token IDs for predictions\n",
        "    # If preds_input are logits (es. direct output of training modello)\n",
        "    current_preds = preds_input\n",
        "    if isinstance(current_preds, tuple): # Common in HF Trainer, es. (logits, hidden_states)\n",
        "        current_preds = current_preds[0]\n",
        "\n",
        "    if hasattr(current_preds, \"ndim\") and current_preds.ndim == 3: # Array of logits (batch_size, seq_len, vocab_size)\n",
        "        current_preds_ids = np.argmax(current_preds, axis=-1)\n",
        "    else: # Otherwise, assumed to be token ID (batch_size, seq_len)\n",
        "        current_preds_ids = current_preds\n",
        "\n",
        "    # Decode predictions and labels\n",
        "    decoded_preds_raw = tokenizer.batch_decode(current_preds_ids, skip_special_tokens=True)\n",
        "\n",
        "    # Replace -100 in labels (common for token to be ignored) with pad_token_id for decoding\n",
        "    processed_label_ids = np.where(label_ids != -100, label_ids, tokenizer.pad_token_id)\n",
        "    decoded_labels_raw = tokenizer.batch_decode(processed_label_ids, skip_special_tokens=True)\n",
        "\n",
        "    processed_preds, processed_labels_for_sacrebleu = postprocess_text(decoded_preds_raw, decoded_labels_raw)\n",
        "\n",
        "    # For other metrics (ROUGE, METEOR, CHRF, TER), usually expects a flat list of reference strings\n",
        "    flat_references = [ref[0] for ref in processed_labels_for_sacrebleu]\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    # 1. SacreBLEU\n",
        "    sacrebleu_output = sacrebleu_metric.compute(predictions=processed_preds, references=processed_labels_for_sacrebleu)\n",
        "    if sacrebleu_output and \"score\" in sacrebleu_output:\n",
        "        results[\"bleu\"] = sacrebleu_output[\"score\"]\n",
        "    else:\n",
        "        results[\"bleu\"] = 0.0 # Fallback\n",
        "\n",
        "    # 2. ROUGE (rouge1, rouge2, rougeL, rougeLsum)\n",
        "    rouge_output = rouge_metric.compute(predictions=processed_preds, references=flat_references, use_stemmer=True)\n",
        "    if rouge_output:\n",
        "        results[\"rouge1\"] = rouge_output.get(\"rouge1\", 0.0)\n",
        "        results[\"rouge2\"] = rouge_output.get(\"rouge2\", 0.0)\n",
        "        results[\"rougeL\"] = rouge_output.get(\"rougeL\", 0.0)\n",
        "        results[\"rougeLsum\"] = rouge_output.get(\"rougeLsum\", 0.0) # Spesso piÃ¹ robusto per sommario\n",
        "    else:\n",
        "        results.update({\"rouge1\": 0.0, \"rouge2\": 0.0, \"rougeL\": 0.0, \"rougeLsum\": 0.0})\n",
        "\n",
        "    # 3. METEOR\n",
        "    meteor_output = meteor_metric.compute(predictions=processed_preds, references=flat_references)\n",
        "    if meteor_output and \"meteor\" in meteor_output:\n",
        "        results[\"meteor\"] = meteor_output[\"meteor\"]\n",
        "    else:\n",
        "        results[\"meteor\"] = 0.0\n",
        "\n",
        "    # 4. CHRF++ (CHRF with n-grams of words)\n",
        "    # For CHRF++, word_order (or word_n) is > 0. Default of evaluate.load('chrf') are word_order=0 (CHRF standard).\n",
        "    # Common parameters for CHRF++: word_order=2, beta=2 (beta=2 default)\n",
        "    chrf_output = chrf_metric.compute(predictions=processed_preds, references=flat_references, word_order=2, beta=2)\n",
        "    if chrf_output and \"score\" in chrf_output:\n",
        "        results[\"chrf++\"] = chrf_output[\"score\"] # CHRF++ score\n",
        "    else:\n",
        "        results[\"chrf++\"] = 0.0\n",
        "\n",
        "    # (Optional) CHRF standard (only characters)\n",
        "    # chrf_std_output = chrf_metric.compute(predictions=processed_preds, references=flat_references, word_order=0)\n",
        "    # if chrf_std_output and \"score\" in chrf_std_output:\n",
        "    #     results[\"chrf\"] = chrf_std_output[\"score\"]\n",
        "    # else:\n",
        "    #     results[\"chrf\"] = 0.0\n",
        "\n",
        "    # 5. TER (Translation Edit Rate) - the smaller, the better\n",
        "    ter_output = ter_metric.compute(predictions=processed_preds, references=flat_references)\n",
        "    if ter_output and \"score\" in ter_output:\n",
        "        results[\"ter\"] = ter_output[\"score\"]\n",
        "    else:\n",
        "        results[\"ter\"] = 1.0 # Fallback on worst score TER possible\n",
        "\n",
        "    # Mean length of generated predictions (excluding padding tokens)\n",
        "    # 'current_preds_ids' are ID token of the predictions\n",
        "    prediction_lengths = [np.count_nonzero(pid_seq != tokenizer.pad_token_id) for pid_seq in current_preds_ids]\n",
        "    results[\"gen_len\"] = np.mean(prediction_lengths) if prediction_lengths else 0.0\n",
        "\n",
        "    # Rounding of all numerical results\n",
        "    final_results = {k: round(v, 4) for k, v in results.items() if isinstance(v, (int, float))}\n",
        "\n",
        "    return final_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ui4ZU02DJ6Au"
      },
      "outputs": [],
      "source": [
        "from datasets.features import Value, Features\n",
        "hf = Dataset.from_csv(DATASET, features=\n",
        "    Features({\n",
        "        SRC_L : Value(\"string\"),\n",
        "        TRG_L : Value(\"string\"),\n",
        "        \"Date\": Value(\"string\"),\n",
        "        \"Author\":Value(\"string\"),\n",
        "        \"Region\":Value(\"string\")\n",
        "    })\n",
        "\n",
        "    ).shuffle(2025).train_test_split(test_size=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCtZMnYMJ6Au"
      },
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtW9sFCaJ6Av",
        "outputId": "5856bc07-74c2-4176-adbc-861353c16240"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model max length: 120\n"
          ]
        }
      ],
      "source": [
        "print(f\"model max length: {max_length}\")\n",
        "\n",
        "\n",
        "def noprompt_it_it(examples):\n",
        "    inputs = [example for example in examples[SRC_L]]\n",
        "    targets = [example for example in examples[TRG_L]]\n",
        "\n",
        "    model_inputs = tokenizer(inputs, text_target=targets, max_length=max_length, truncation=True, padding=\"max_length\")\n",
        "    return model_inputs\n",
        "\n",
        "# I. Riscrivi\n",
        "# II. Traduci\n",
        "# III. Correggi\n",
        "def minerva_base_prompt_it_it_train(examples):\n",
        "\n",
        "    prompts = [\n",
        "        f\"\"\"riscrivi la seguente frase {src} scritta in italiano arcaico in Italiano moderno: {dst}\"\"\"\n",
        "        for src, dst, dat, dia in zip(examples[SRC_L], examples[TRG_L], examples[\"Date\"], examples[\"Region\"])\n",
        "    ]\n",
        "\n",
        "    # Tokenizza input+target e crea label con gli stessi token\n",
        "    model_inputs = tokenizer(\n",
        "        prompts,\n",
        "        max_length=max_length,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\"\n",
        "    )\n",
        "\n",
        "    model_inputs[\"labels\"] = [\n",
        "        [(label if label != tokenizer.pad_token_id else -100) for label in input_ids]\n",
        "        for input_ids in model_inputs[\"input_ids\"]\n",
        "    ]\n",
        "    return model_inputs\n",
        "\n",
        "def minerva_base_prompt_it_it_eval(examples):\n",
        "    prompts = [\n",
        "\n",
        "        f\"\"\"riscrivi la seguente frase {src} scritta in italiano arcaico in Italiano moderno: \"\"\"\n",
        "        for src in examples[SRC_L]\n",
        "    ]\n",
        "\n",
        "    # Tokenizza input+target e crea label con gli stessi token\n",
        "    model_inputs = tokenizer(\n",
        "        prompts,\n",
        "        max_length=max_length,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\"\n",
        "    )\n",
        "\n",
        "    return model_inputs\n",
        "\n",
        "def base_prompt_en(examples):\n",
        "    inputs = [\"translate from Ancient Italian to Modern Italian: \" + example for example in examples[SRC_L]]\n",
        "\n",
        "    # Tokenizza solo gli input\n",
        "    model_inputs = tokenizer(inputs, max_length=max_length, truncation=True, padding=\"max_length\")\n",
        "    targets = [example for example in examples[TRG_L]]\n",
        "\n",
        "    model_inputs = tokenizer(inputs, text_target=targets, max_length=max_length, truncation=True, padding=\"longest\")\n",
        "\n",
        "    return model_inputs\n",
        "\n",
        "def base_prompt_it_it(examples):\n",
        "    inputs = [\"Riscrivi dall'Italiano Antico a l'Italiano Moderno: \" + example for example in examples[SRC_L]]\n",
        "\n",
        "    # Tokenizza solo gli input\n",
        "    model_inputs = tokenizer(inputs, max_length=max_length, truncation=True, padding=\"max_length\")\n",
        "    targets = [example for example in examples[TRG_L]]\n",
        "\n",
        "    model_inputs = tokenizer(inputs, text_target=targets, max_length=max_length, truncation=True, padding=\"longest\")\n",
        "\n",
        "    return model_inputs\n",
        "\n",
        "def parafrasi_prompt_it_it(examples):\n",
        "    inputs = [\"Scrivi la parafrasi di questo testo: \" + example for example in examples[SRC_L]]\n",
        "    targets = [example for example in examples[TRG_L]]\n",
        "\n",
        "    model_inputs = tokenizer(inputs, text_target=targets, max_length=max_length, truncation=True, padding=\"longest\")\n",
        "    return model_inputs\n",
        "\n",
        "def informative_prompt_it_it(examples):\n",
        "    inputs = [f\"Riscrivi in uno stile piÃ¹ moderno il testo del seguente Autore: '{author}', anno di scrittura: {date}, luogo: Italia, dialetto: '{region}', testo: '{text}'.\" for text, date, region, author in zip(examples[SRC_L], examples[\"Date\"], examples[\"Region\"], examples[\"Author\"]) ]\n",
        "    targets = [example for example in examples[TRG_L]]\n",
        "\n",
        "    model_inputs = tokenizer(inputs, text_target=targets, max_length=max_length, truncation=True, padding=\"longest\")\n",
        "    return model_inputs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BFXkm0YJ6Av"
      },
      "source": [
        "## Tokenizer Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "e439f25244e041b8b00b62a4e79edd36",
            "86c58244034945728ce237f1d28e215d"
          ]
        },
        "id": "luEQkXHIJ6Aw",
        "outputId": "297d2697-d874-441c-85ef-bf3f45f1cfde"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e439f25244e041b8b00b62a4e79edd36",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/87 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "86c58244034945728ce237f1d28e215d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "match tokenization_method:\n",
        "    case \"minerva_base\":\n",
        "        map_callback_train = minerva_base_prompt_it_it_train\n",
        "        map_callback_eval   = minerva_base_prompt_it_it_eval\n",
        "\n",
        "        hf_tokenized = DatasetDict({\n",
        "            \"train\": hf[\"train\"].map(map_callback_train, batched=True),\n",
        "            \"test\":  hf[\"test\"].map(map_callback_eval, batched=True)\n",
        "        })\n",
        "\n",
        "        hf_tokenized.set_format(type=\"torch\", columns=['input_ids', 'attention_mask'])\n",
        "\n",
        "    case \"base_prompt_en\":\n",
        "        map_callback = base_prompt_en\n",
        "        hf_tokenized = hf.map(map_callback, batched=True)\n",
        "\n",
        "    case \"base_prompt_it_it\":\n",
        "        map_callback = base_prompt_it_it\n",
        "        hf_tokenized = hf.map(map_callback, batched=True)\n",
        "    case _:\n",
        "        raise ValueError(\"Tokenization method not avaiable\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8_EGew_J6Aw",
        "outputId": "c5da6260-613d-4cb6-e04f-c3b475fa36b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'train': ['Sentence', 'Target', 'Date', 'Author', 'Region', 'input_ids', 'attention_mask', 'labels'], 'test': ['Sentence', 'Target', 'Date', 'Author', 'Region', 'input_ids', 'attention_mask']}\n",
            "{'train': (87, 8), 'test': (10, 7)}\n"
          ]
        }
      ],
      "source": [
        "print(hf_tokenized.column_names)\n",
        "print(hf_tokenized.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eC3T3SlkJ6Aw",
        "outputId": "32aa2760-ca3b-4bb7-82ec-a67f56d5d680"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===:(sentence nÂ°1):===\n",
            "Sentence:riscrivi la seguente frase perÃ² che, sse nobile cosa e alta Ã¨ abatte il nimico, ampoi nonn Ã¨ meno laudabile sapere avere misiricordia scritta in italiano arcaico in Italiano moderno: PoichÃ©, se Ã¨ cosa nobile e grande abbattere il nemico, non Ã¨ poi meno degno di lode saper mostrare misericordia.\n",
            "===:(sentence nÂ°2):===\n",
            "Sentence:riscrivi la seguente frase Alexandri, ciÃ² Ã¨ il genero e 'l figliuolo, da Phausonia, gentile iovane di Macedonia, stando in uno luogo strecto sanza guardia, fue morto. scritta in italiano arcaico in Italiano moderno: Alessandro da Phausonia, cioÃ¨ il genero del figlio, nobile giovane di Macedonia, fu ucciso mentre si trovava in un luogo isolato e senza protezione.\n",
            "===:(sentence nÂ°3):===\n",
            "Sentence:riscrivi la seguente frase mostroe massimamente le forze sue, dando lui re a questa cittade ne la quale nacque servo; al quale avvenne lunghissimamente lo imperio tenere scritta in italiano arcaico in Italiano moderno: ManifestÃ² pienamente la sua forza, facendo di lui re della cittÃ  in cui era nato come servo; ed egli riuscÃ¬ a mantenere il potere per lunghissimo tempo.\n",
            "===:(sentence nÂ°4):===\n",
            "Sentence:riscrivi la seguente frase Tarentini, i quali erano nati di quegli di Lacedemonia et facta da lloro nobile cittade de' Greci. scritta in italiano arcaico in Italiano moderno: I Tarantini, discendenti degli Lacedemoni, partiti dalla nobile Grecia fondarono la loro cittÃ .\n",
            "===:(sentence nÂ°5):===\n",
            "Sentence:riscrivi la seguente frase Il re entrÃ² in uno giardino dietro al suo albergo, quasi come s'egli andasse pensando alla risposta. scritta in italiano arcaico in Italiano moderno: Il re entrÃ² in un giardino dietro la sua dimora, come se stesse riflettendo sulla risposta.\n"
          ]
        }
      ],
      "source": [
        "for idx, s in enumerate(hf_tokenized[\"train\"].take(5), 1):\n",
        "    print(f\"===:(sentence nÂ°{idx}):===\")\n",
        "    print(f\"{SRC_L}:{tokenizer.decode(s[\"input_ids\"], attention_mask=s[\"attention_mask\"], skip_special_tokens=True)}\" )\n",
        "    #print(f\"{TRG_L}:{tokenizer.decode(s[\"labels\"], skip_special_tokens=True)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WB1tROKuJ6Ax",
        "outputId": "82cde9f8-a3c9-454a-ed9b-a6e497b7c4dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===:(sentence nÂ°1):===\n",
            "Sentence:riscrivi la seguente frase contra lui e contra le sue sorelle  e contra il reame e contra l' alto pregio della sua ingenerazione e della sua familia scritta in italiano arcaico in Italiano moderno: \n",
            "===:(sentence nÂ°2):===\n",
            "Sentence:riscrivi la seguente frase da' monti de' Romani si feciero nuovi nemici; contra i quali Ã¨ conbactuto cum diversa ventura: perkÃ© nela primaia battaglia, essendo consolo Valerio, MMMD ne moriro de' Romani; scritta in italiano arcaico in Italiano moderno: \n",
            "===:(sentence nÂ°3):===\n",
            "Sentence:riscrivi la seguente frase Non lo volle cognoscere per nimico. Qesta Ã¨ quella, la quale diede ardire al profeta Natan a riprendere con grande autoritade quello re, il quale avea peccato. scritta in italiano arcaico in Italiano moderno: \n",
            "===:(sentence nÂ°4):===\n",
            "Sentence:riscrivi la seguente frase la seconda suole talora per la grande provedenzia fare timoroso, e la prima per l'ardire rendere altrui matto. scritta in italiano arcaico in Italiano moderno: \n",
            "===:(sentence nÂ°5):===\n",
            "Sentence:riscrivi la seguente frase notabile esemplo a ciascuno che si guardasse di fare e di pensar tradigione, il servo, il quale gli avea accusati, fu francato, e fugli donata grande quantitÃ  di moneta scritta in italiano arcaico in Italiano moderno: \n"
          ]
        }
      ],
      "source": [
        "for idx, s in enumerate(hf_tokenized[\"test\"].take(5), 1):\n",
        "    print(f\"===:(sentence nÂ°{idx}):===\")\n",
        "    print(f\"{SRC_L}:{tokenizer.decode(s[\"input_ids\"], attention_mask=s[\"attention_mask\"], skip_special_tokens=True)}\" )\n",
        "    #print(f\"{TRG_L}:{tokenizer.decode(s[\"labels\"], skip_special_tokens=True)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLsr_a4RJ6Ax"
      },
      "source": [
        "## Models & Traning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aNXSOX0J6Ay"
      },
      "source": [
        "### PEFT Fine-Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KGZ5c0DcJ6Ay",
        "outputId": "67462809-6c0c-40af-a623-f77612bdd67c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CAUSAL Generation\n"
          ]
        }
      ],
      "source": [
        "if isinstance(model, PeftModelForSeq2SeqLM):\n",
        "    print(\"[SEQ2SEQ Generation]\")\n",
        "    trainer = MyTrainerSeq2Seq(\n",
        "                model=model,\n",
        "                args=training_args,\n",
        "                train_dataset=hf_tokenized[\"train\"],\n",
        "                eval_dataset=hf_tokenized[\"test\"],\n",
        "                processing_class=tokenizer,\n",
        "                data_collator=data_collator,\n",
        "                compute_metrics=compute_metrics,\n",
        "                callbacks=[Report(OUT_DIR)]\n",
        "            )\n",
        "else:\n",
        "    print(\"CAUSAL Generation\")\n",
        "    trainer = MyTrainer(\n",
        "                model=model,\n",
        "                args=training_args,\n",
        "                train_dataset=hf_tokenized[\"train\"],\n",
        "                eval_dataset=hf_tokenized[\"test\"],\n",
        "                processing_class=tokenizer,\n",
        "                data_collator=data_collator,\n",
        "                compute_metrics=compute_metrics,\n",
        "                callbacks=[Report(OUT_DIR), early_callback]\n",
        "            )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UojACT4JJ6Ay",
        "outputId": "bda716fb-9292-48bc-f5ee-d9464f454b6f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='462' max='462' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [462/462 08:51, Epoch 42/42]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Bleu</th>\n",
              "      <th>Rouge1</th>\n",
              "      <th>Rouge2</th>\n",
              "      <th>Rougel</th>\n",
              "      <th>Rougelsum</th>\n",
              "      <th>Meteor</th>\n",
              "      <th>Chrf++</th>\n",
              "      <th>Ter</th>\n",
              "      <th>Gen Len</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.733500</td>\n",
              "      <td>4.717964</td>\n",
              "      <td>1.040100</td>\n",
              "      <td>0.242100</td>\n",
              "      <td>0.013400</td>\n",
              "      <td>0.194600</td>\n",
              "      <td>0.187800</td>\n",
              "      <td>0.155900</td>\n",
              "      <td>21.432800</td>\n",
              "      <td>90.828400</td>\n",
              "      <td>119.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.733100</td>\n",
              "      <td>4.711500</td>\n",
              "      <td>1.080900</td>\n",
              "      <td>0.234400</td>\n",
              "      <td>0.013400</td>\n",
              "      <td>0.192200</td>\n",
              "      <td>0.185600</td>\n",
              "      <td>0.149300</td>\n",
              "      <td>21.532900</td>\n",
              "      <td>90.532500</td>\n",
              "      <td>119.400000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.719800</td>\n",
              "      <td>4.697144</td>\n",
              "      <td>1.037800</td>\n",
              "      <td>0.233500</td>\n",
              "      <td>0.013400</td>\n",
              "      <td>0.192400</td>\n",
              "      <td>0.189400</td>\n",
              "      <td>0.152100</td>\n",
              "      <td>21.441900</td>\n",
              "      <td>91.124300</td>\n",
              "      <td>119.200000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>3.700500</td>\n",
              "      <td>4.678380</td>\n",
              "      <td>1.043200</td>\n",
              "      <td>0.236200</td>\n",
              "      <td>0.013400</td>\n",
              "      <td>0.195200</td>\n",
              "      <td>0.189000</td>\n",
              "      <td>0.153100</td>\n",
              "      <td>21.791200</td>\n",
              "      <td>90.532500</td>\n",
              "      <td>119.200000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>3.678400</td>\n",
              "      <td>4.654891</td>\n",
              "      <td>1.051100</td>\n",
              "      <td>0.239000</td>\n",
              "      <td>0.013400</td>\n",
              "      <td>0.194900</td>\n",
              "      <td>0.188200</td>\n",
              "      <td>0.151500</td>\n",
              "      <td>22.000300</td>\n",
              "      <td>90.236700</td>\n",
              "      <td>119.400000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>3.637800</td>\n",
              "      <td>4.621964</td>\n",
              "      <td>1.056100</td>\n",
              "      <td>0.244700</td>\n",
              "      <td>0.013400</td>\n",
              "      <td>0.197100</td>\n",
              "      <td>0.191300</td>\n",
              "      <td>0.152700</td>\n",
              "      <td>22.286100</td>\n",
              "      <td>90.236700</td>\n",
              "      <td>119.800000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>3.590900</td>\n",
              "      <td>4.581850</td>\n",
              "      <td>1.157000</td>\n",
              "      <td>0.250100</td>\n",
              "      <td>0.018800</td>\n",
              "      <td>0.204900</td>\n",
              "      <td>0.202000</td>\n",
              "      <td>0.151600</td>\n",
              "      <td>22.699400</td>\n",
              "      <td>89.940800</td>\n",
              "      <td>119.800000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>3.545900</td>\n",
              "      <td>4.531985</td>\n",
              "      <td>1.167400</td>\n",
              "      <td>0.253100</td>\n",
              "      <td>0.018400</td>\n",
              "      <td>0.209900</td>\n",
              "      <td>0.209600</td>\n",
              "      <td>0.156600</td>\n",
              "      <td>23.862700</td>\n",
              "      <td>90.236700</td>\n",
              "      <td>119.900000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>3.486800</td>\n",
              "      <td>4.468873</td>\n",
              "      <td>1.220100</td>\n",
              "      <td>0.269800</td>\n",
              "      <td>0.032600</td>\n",
              "      <td>0.226500</td>\n",
              "      <td>0.223100</td>\n",
              "      <td>0.176700</td>\n",
              "      <td>25.092900</td>\n",
              "      <td>89.645000</td>\n",
              "      <td>120.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>3.415000</td>\n",
              "      <td>4.391948</td>\n",
              "      <td>3.132300</td>\n",
              "      <td>0.312000</td>\n",
              "      <td>0.103600</td>\n",
              "      <td>0.268500</td>\n",
              "      <td>0.264200</td>\n",
              "      <td>0.246500</td>\n",
              "      <td>30.581600</td>\n",
              "      <td>86.390500</td>\n",
              "      <td>120.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>3.337700</td>\n",
              "      <td>4.292814</td>\n",
              "      <td>3.176900</td>\n",
              "      <td>0.319900</td>\n",
              "      <td>0.109700</td>\n",
              "      <td>0.280000</td>\n",
              "      <td>0.276600</td>\n",
              "      <td>0.256000</td>\n",
              "      <td>31.444100</td>\n",
              "      <td>85.503000</td>\n",
              "      <td>120.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>3.238500</td>\n",
              "      <td>4.189583</td>\n",
              "      <td>3.200600</td>\n",
              "      <td>0.324500</td>\n",
              "      <td>0.115700</td>\n",
              "      <td>0.284700</td>\n",
              "      <td>0.277800</td>\n",
              "      <td>0.267800</td>\n",
              "      <td>31.655500</td>\n",
              "      <td>85.207100</td>\n",
              "      <td>120.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>3.140100</td>\n",
              "      <td>4.083128</td>\n",
              "      <td>3.328700</td>\n",
              "      <td>0.346500</td>\n",
              "      <td>0.118400</td>\n",
              "      <td>0.301100</td>\n",
              "      <td>0.294700</td>\n",
              "      <td>0.282000</td>\n",
              "      <td>33.102900</td>\n",
              "      <td>83.727800</td>\n",
              "      <td>120.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>3.038000</td>\n",
              "      <td>3.967449</td>\n",
              "      <td>3.270000</td>\n",
              "      <td>0.353000</td>\n",
              "      <td>0.120400</td>\n",
              "      <td>0.310100</td>\n",
              "      <td>0.306500</td>\n",
              "      <td>0.285400</td>\n",
              "      <td>33.923400</td>\n",
              "      <td>82.840200</td>\n",
              "      <td>120.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>2.920000</td>\n",
              "      <td>3.835625</td>\n",
              "      <td>3.624100</td>\n",
              "      <td>0.387400</td>\n",
              "      <td>0.136500</td>\n",
              "      <td>0.338800</td>\n",
              "      <td>0.335200</td>\n",
              "      <td>0.331700</td>\n",
              "      <td>37.139300</td>\n",
              "      <td>79.881700</td>\n",
              "      <td>120.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>2.805000</td>\n",
              "      <td>3.696695</td>\n",
              "      <td>4.033200</td>\n",
              "      <td>0.402500</td>\n",
              "      <td>0.157200</td>\n",
              "      <td>0.355200</td>\n",
              "      <td>0.350500</td>\n",
              "      <td>0.341800</td>\n",
              "      <td>38.315000</td>\n",
              "      <td>78.402400</td>\n",
              "      <td>120.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>2.687000</td>\n",
              "      <td>3.553423</td>\n",
              "      <td>15.097900</td>\n",
              "      <td>0.437500</td>\n",
              "      <td>0.224000</td>\n",
              "      <td>0.392200</td>\n",
              "      <td>0.390900</td>\n",
              "      <td>0.403400</td>\n",
              "      <td>41.387400</td>\n",
              "      <td>72.781100</td>\n",
              "      <td>120.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>2.573500</td>\n",
              "      <td>3.465050</td>\n",
              "      <td>17.867400</td>\n",
              "      <td>0.437400</td>\n",
              "      <td>0.225200</td>\n",
              "      <td>0.393100</td>\n",
              "      <td>0.392100</td>\n",
              "      <td>0.410500</td>\n",
              "      <td>42.503300</td>\n",
              "      <td>70.710100</td>\n",
              "      <td>120.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>2.496300</td>\n",
              "      <td>3.403049</td>\n",
              "      <td>19.330900</td>\n",
              "      <td>0.452500</td>\n",
              "      <td>0.233200</td>\n",
              "      <td>0.398200</td>\n",
              "      <td>0.397300</td>\n",
              "      <td>0.420300</td>\n",
              "      <td>44.219400</td>\n",
              "      <td>70.118300</td>\n",
              "      <td>120.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>2.428400</td>\n",
              "      <td>3.348039</td>\n",
              "      <td>21.033000</td>\n",
              "      <td>0.452500</td>\n",
              "      <td>0.245000</td>\n",
              "      <td>0.405300</td>\n",
              "      <td>0.404600</td>\n",
              "      <td>0.425000</td>\n",
              "      <td>45.028300</td>\n",
              "      <td>69.230800</td>\n",
              "      <td>120.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>2.384700</td>\n",
              "      <td>3.300228</td>\n",
              "      <td>21.319200</td>\n",
              "      <td>0.457500</td>\n",
              "      <td>0.252200</td>\n",
              "      <td>0.415800</td>\n",
              "      <td>0.414800</td>\n",
              "      <td>0.432500</td>\n",
              "      <td>45.309000</td>\n",
              "      <td>67.751500</td>\n",
              "      <td>119.700000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>2.343800</td>\n",
              "      <td>3.284408</td>\n",
              "      <td>21.372100</td>\n",
              "      <td>0.451800</td>\n",
              "      <td>0.249000</td>\n",
              "      <td>0.406700</td>\n",
              "      <td>0.406200</td>\n",
              "      <td>0.431500</td>\n",
              "      <td>45.440800</td>\n",
              "      <td>68.639100</td>\n",
              "      <td>119.600000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>2.301100</td>\n",
              "      <td>3.246028</td>\n",
              "      <td>21.455200</td>\n",
              "      <td>0.448300</td>\n",
              "      <td>0.253200</td>\n",
              "      <td>0.412100</td>\n",
              "      <td>0.411400</td>\n",
              "      <td>0.430600</td>\n",
              "      <td>45.369500</td>\n",
              "      <td>68.343200</td>\n",
              "      <td>119.400000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>2.259500</td>\n",
              "      <td>3.200204</td>\n",
              "      <td>25.090700</td>\n",
              "      <td>0.482200</td>\n",
              "      <td>0.287200</td>\n",
              "      <td>0.442700</td>\n",
              "      <td>0.442100</td>\n",
              "      <td>0.449000</td>\n",
              "      <td>47.194100</td>\n",
              "      <td>65.088800</td>\n",
              "      <td>119.300000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>2.197200</td>\n",
              "      <td>3.182848</td>\n",
              "      <td>25.382800</td>\n",
              "      <td>0.476900</td>\n",
              "      <td>0.291600</td>\n",
              "      <td>0.444100</td>\n",
              "      <td>0.443300</td>\n",
              "      <td>0.448400</td>\n",
              "      <td>47.231400</td>\n",
              "      <td>64.497000</td>\n",
              "      <td>119.400000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>2.169700</td>\n",
              "      <td>3.161691</td>\n",
              "      <td>25.422800</td>\n",
              "      <td>0.482900</td>\n",
              "      <td>0.288900</td>\n",
              "      <td>0.443300</td>\n",
              "      <td>0.442400</td>\n",
              "      <td>0.447900</td>\n",
              "      <td>47.287600</td>\n",
              "      <td>65.088800</td>\n",
              "      <td>119.300000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>2.129500</td>\n",
              "      <td>3.174210</td>\n",
              "      <td>25.286600</td>\n",
              "      <td>0.470500</td>\n",
              "      <td>0.289200</td>\n",
              "      <td>0.438100</td>\n",
              "      <td>0.437200</td>\n",
              "      <td>0.445000</td>\n",
              "      <td>47.215100</td>\n",
              "      <td>65.680500</td>\n",
              "      <td>119.800000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>2.094600</td>\n",
              "      <td>3.172606</td>\n",
              "      <td>25.289900</td>\n",
              "      <td>0.479500</td>\n",
              "      <td>0.289100</td>\n",
              "      <td>0.443800</td>\n",
              "      <td>0.442900</td>\n",
              "      <td>0.448600</td>\n",
              "      <td>47.321300</td>\n",
              "      <td>65.976300</td>\n",
              "      <td>120.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>2.072700</td>\n",
              "      <td>3.173206</td>\n",
              "      <td>24.533500</td>\n",
              "      <td>0.480800</td>\n",
              "      <td>0.284800</td>\n",
              "      <td>0.439200</td>\n",
              "      <td>0.438400</td>\n",
              "      <td>0.452500</td>\n",
              "      <td>47.128400</td>\n",
              "      <td>66.568000</td>\n",
              "      <td>120.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>2.036600</td>\n",
              "      <td>3.175986</td>\n",
              "      <td>24.732900</td>\n",
              "      <td>0.486000</td>\n",
              "      <td>0.283400</td>\n",
              "      <td>0.441700</td>\n",
              "      <td>0.440900</td>\n",
              "      <td>0.459000</td>\n",
              "      <td>47.605000</td>\n",
              "      <td>66.568000</td>\n",
              "      <td>120.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>2.010600</td>\n",
              "      <td>3.180533</td>\n",
              "      <td>24.447100</td>\n",
              "      <td>0.482100</td>\n",
              "      <td>0.284600</td>\n",
              "      <td>0.434300</td>\n",
              "      <td>0.433600</td>\n",
              "      <td>0.456900</td>\n",
              "      <td>47.155100</td>\n",
              "      <td>65.976300</td>\n",
              "      <td>120.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>1.978400</td>\n",
              "      <td>3.176911</td>\n",
              "      <td>24.019900</td>\n",
              "      <td>0.468100</td>\n",
              "      <td>0.275700</td>\n",
              "      <td>0.421400</td>\n",
              "      <td>0.420300</td>\n",
              "      <td>0.444500</td>\n",
              "      <td>46.703900</td>\n",
              "      <td>67.159800</td>\n",
              "      <td>120.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>1.939200</td>\n",
              "      <td>3.210664</td>\n",
              "      <td>24.748100</td>\n",
              "      <td>0.472300</td>\n",
              "      <td>0.277100</td>\n",
              "      <td>0.428600</td>\n",
              "      <td>0.428200</td>\n",
              "      <td>0.445200</td>\n",
              "      <td>47.061100</td>\n",
              "      <td>67.159800</td>\n",
              "      <td>120.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>1.903300</td>\n",
              "      <td>3.192777</td>\n",
              "      <td>24.714000</td>\n",
              "      <td>0.470700</td>\n",
              "      <td>0.277600</td>\n",
              "      <td>0.427200</td>\n",
              "      <td>0.426300</td>\n",
              "      <td>0.444000</td>\n",
              "      <td>47.042500</td>\n",
              "      <td>66.863900</td>\n",
              "      <td>120.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>1.875200</td>\n",
              "      <td>3.226123</td>\n",
              "      <td>24.430500</td>\n",
              "      <td>0.466100</td>\n",
              "      <td>0.277000</td>\n",
              "      <td>0.423100</td>\n",
              "      <td>0.422300</td>\n",
              "      <td>0.446600</td>\n",
              "      <td>47.362200</td>\n",
              "      <td>67.455600</td>\n",
              "      <td>120.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>1.831600</td>\n",
              "      <td>3.263688</td>\n",
              "      <td>24.312800</td>\n",
              "      <td>0.471300</td>\n",
              "      <td>0.274200</td>\n",
              "      <td>0.426500</td>\n",
              "      <td>0.425500</td>\n",
              "      <td>0.447600</td>\n",
              "      <td>47.086800</td>\n",
              "      <td>67.751500</td>\n",
              "      <td>120.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>1.794300</td>\n",
              "      <td>3.256272</td>\n",
              "      <td>24.278400</td>\n",
              "      <td>0.465000</td>\n",
              "      <td>0.272600</td>\n",
              "      <td>0.419900</td>\n",
              "      <td>0.418900</td>\n",
              "      <td>0.445300</td>\n",
              "      <td>47.085700</td>\n",
              "      <td>67.455600</td>\n",
              "      <td>120.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>1.763200</td>\n",
              "      <td>3.284029</td>\n",
              "      <td>23.991100</td>\n",
              "      <td>0.461600</td>\n",
              "      <td>0.269000</td>\n",
              "      <td>0.416400</td>\n",
              "      <td>0.415600</td>\n",
              "      <td>0.443100</td>\n",
              "      <td>46.829900</td>\n",
              "      <td>68.934900</td>\n",
              "      <td>120.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>1.726300</td>\n",
              "      <td>3.360744</td>\n",
              "      <td>24.464000</td>\n",
              "      <td>0.466200</td>\n",
              "      <td>0.276100</td>\n",
              "      <td>0.427100</td>\n",
              "      <td>0.426200</td>\n",
              "      <td>0.459000</td>\n",
              "      <td>47.649000</td>\n",
              "      <td>68.934900</td>\n",
              "      <td>120.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.677400</td>\n",
              "      <td>3.392208</td>\n",
              "      <td>24.493400</td>\n",
              "      <td>0.460000</td>\n",
              "      <td>0.276100</td>\n",
              "      <td>0.423700</td>\n",
              "      <td>0.423200</td>\n",
              "      <td>0.454500</td>\n",
              "      <td>47.606300</td>\n",
              "      <td>68.934900</td>\n",
              "      <td>120.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>1.628800</td>\n",
              "      <td>3.433393</td>\n",
              "      <td>24.378700</td>\n",
              "      <td>0.476000</td>\n",
              "      <td>0.273300</td>\n",
              "      <td>0.425300</td>\n",
              "      <td>0.424000</td>\n",
              "      <td>0.467000</td>\n",
              "      <td>47.529900</td>\n",
              "      <td>70.118300</td>\n",
              "      <td>120.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>1.582500</td>\n",
              "      <td>3.442166</td>\n",
              "      <td>24.459700</td>\n",
              "      <td>0.469000</td>\n",
              "      <td>0.273800</td>\n",
              "      <td>0.423700</td>\n",
              "      <td>0.422900</td>\n",
              "      <td>0.463400</td>\n",
              "      <td>47.568800</td>\n",
              "      <td>70.118300</td>\n",
              "      <td>120.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training done. Generating graphs...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=462, training_loss=2.585878202925513, metrics={'train_runtime': 532.3405, 'train_samples_per_second': 6.864, 'train_steps_per_second': 0.868, 'total_flos': 7483350841344000.0, 'train_loss': 2.585878202925513, 'epoch': 42.0})"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "5ba49f79e3d2462baf2deaf3898bea07"
          ]
        },
        "id": "Mvm49C6vJ6Az",
        "outputId": "dbb38e13-993c-4b56-8056-d4e1557a9628"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inizio generazione su cuda\n",
            "=============================\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5ba49f79e3d2462baf2deaf3898bea07",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===:0-(Model for Prompt)===\n",
            "riscrivi la seguente frase contra lui e contra le sue sorelle  e contra il reame e contra l' alto pregio della sua ingenerazione e della sua familia scritta in italiano arcaico in Italiano moderno: \n",
            "=========================\n",
            "===:(model sapienzanlp/Minerva-3B-base-v1.0):===\n",
            "riscrivi la seguente frase contra lui e contra le sue sorelle  e contra il reame e contra l' alto pregio della sua ingenerazione e della sua familia scritta in italiano arcaico in Italiano moderno:  contro di lui e contro le sue sorelle e contro il regno e contro lâ€™alto prestigio della sua stirpe e della sua famiglia. scritta in italiano moderno in Italiano arcaico in Italiano moderno: Contro di lui e contro le sue sorelle, e contro il regno, e contro il prestigio della sua famiglia. Il 2018 Ã¨ stato un anno ricco di novitÃ  per il mondo delle criptovalute. Il Bitcoin ha raggiunto il suo massimo storico, mentre Ethereum ha superato il suo precedente record. Il 2019 Ã¨ iniziato con il botto, con il Bitcoin che ha superato i\n",
            "=================================\n",
            "===:1-(Model for Prompt)===\n",
            "riscrivi la seguente frase da' monti de' Romani si feciero nuovi nemici; contra i quali Ã¨ conbactuto cum diversa ventura: perkÃ© nela primaia battaglia, essendo consolo Valerio, MMMD ne moriro de' Romani; scritta in italiano arcaico in Italiano moderno: \n",
            "=========================\n",
            "===:(model sapienzanlp/Minerva-3B-base-v1.0):===\n",
            "riscrivi la seguente frase da' monti de' Romani si feciero nuovi nemici; contra i quali Ã¨ conbactuto cum diversa ventura: perkÃ© nela primaia battaglia, essendo consolo Valerio, MMMD ne moriro de' Romani; scritta in italiano arcaico in Italiano moderno:  Da quei monti, che erano stati nemici dei Romani, furono espulsi; poichÃ© nella prima battaglia Valerio fu ucciso e morÃ¬ chi lo uccise. scritta in italiano moderno in Italiano arcaico in Italiano moderno:  Da quei monti furono espulsi i nemici. L'autore della frase Ã¨ stato probabilmente Valerio, il quale, essendo console, fu ucciso in battaglia. scritta in italiano moderno in Italiano arcaico in Italiano moderno:  Da quei monti furono espulsi i nemici. I nemici furono cacciati via. scritta in italiano moderno in Italiano antico in Italiano moderno: \n",
            "=================================\n",
            "===:2-(Model for Prompt)===\n",
            "riscrivi la seguente frase Non lo volle cognoscere per nimico. Qesta Ã¨ quella, la quale diede ardire al profeta Natan a riprendere con grande autoritade quello re, il quale avea peccato. scritta in italiano arcaico in Italiano moderno: \n",
            "=========================\n",
            "===:(model sapienzanlp/Minerva-3B-base-v1.0):===\n",
            "riscrivi la seguente frase Non lo volle cognoscere per nimico. Qesta Ã¨ quella, la quale diede ardire al profeta Natan a riprendere con grande autoritade quello re, il quale avea peccato. scritta in italiano arcaico in Italiano moderno:  Non volle conoscere per nemico colui che, con coraggio, aveva sfidato Dio. Questa Ã¨ la frase che il profeta Natan disse al re Davide per spronarlo a combattere con grande coraggio. Il 2018 Ã¨ stato un anno ricco di novitÃ  per il mondo delle criptovalute. Il Bitcoin ha raggiunto il suo massimo storico, mentre Ethereum ha superato il valore di 2.000 dollari. Il 2019 Ã¨ iniziato con il botto, con il Bitcoin che ha superato i 10.000 dollari.\n",
            "Il 201\n",
            "=================================\n",
            "===:3-(Model for Prompt)===\n",
            "riscrivi la seguente frase la seconda suole talora per la grande provedenzia fare timoroso, e la prima per l'ardire rendere altrui matto. scritta in italiano arcaico in Italiano moderno: \n",
            "=========================\n",
            "===:(model sapienzanlp/Minerva-3B-base-v1.0):===\n",
            "riscrivi la seguente frase la seconda suole talora per la grande provedenzia fare timoroso, e la prima per l'ardire rendere altrui matto. scritta in italiano arcaico in Italiano moderno:  La seconda puÃ² talvolta, per la sua grande forza, essere timorosa, mentre la prima, per il suo impeto, riesce a far arrossire gli altri.\n",
            "\n",
            "Il 2020 Ã¨ stato un anno difficile per tutti, ma per i nostri amici a quattro zampe Ã¨ stato ancora piÃ¹ difficile. Per questo motivo, abbiamo deciso di dedicare il nostro calendario 2021 a tutti i cani e gatti che hanno dovuto affrontare un lungo periodo di isolamento, solitudine e sofferenza.\n",
            "Il calendario 2021 Ã¨ stato realizzato in collaborazione con lâ€™associazione\n",
            "=================================\n",
            "===:4-(Model for Prompt)===\n",
            "riscrivi la seguente frase notabile esemplo a ciascuno che si guardasse di fare e di pensar tradigione, il servo, il quale gli avea accusati, fu francato, e fugli donata grande quantitÃ  di moneta scritta in italiano arcaico in Italiano moderno: \n",
            "=========================\n",
            "===:(model sapienzanlp/Minerva-3B-base-v1.0):===\n",
            "riscrivi la seguente frase notabile esemplo a ciascuno che si guardasse di fare e di pensar tradigione, il servo, il quale gli avea accusati, fu francato, e fugli donata grande quantitÃ  di moneta scritta in italiano arcaico in Italiano moderno:  Nota particolare: Nessuno si guardi dal fare il male e dal pensare male, il servo fu liberato, ricevette in dono una grande quantitÃ  di denaro. Il 2019 Ã¨ stato un anno ricco di novitÃ  per il mondo del web marketing. In questo articolo, voglio condividere con te le principali novitÃ  che hanno caratterizzato il settore del marketing digitale nel 2019.\n",
            "Il 2019 Ã¨ stato un anno ricco di novitÃ  per il mondo del web marketing. In questo articolo, voglio condividere con te le principali novitÃ  che hanno caratterizzato il settore del marketing digitale nel\n",
            "=================================\n",
            "===:5-(Model for Prompt)===\n",
            "riscrivi la seguente frase per tanto che Lucano il disse, il vi racontiamo. Ivi, quando l'anima di Pompeo ebe sentito la chiaritÃ  di lasusso, ella sÃ¬ cognobe prima in  grande scritta in italiano arcaico in Italiano moderno: \n",
            "=========================\n",
            "===:(model sapienzanlp/Minerva-3B-base-v1.0):===\n",
            "riscrivi la seguente frase per tanto che Lucano il disse, il vi racontiamo. Ivi, quando l'anima di Pompeo ebe sentito la chiaritÃ  di lasusso, ella sÃ¬ cognobe prima in  grande scritta in italiano arcaico in Italiano moderno:  CosÃ¬, quando l'anima di Pompeo udÃ¬ la luce divina, si rese conto di essere in un luogo piÃ¹ alto. Il 2019 Ã¨ stato un anno ricco di novitÃ  per il mondo del web marketing. In questo articolo, voglio condividere con te le principali novitÃ  che hanno caratterizzato il settore del marketing digitale nel 2019.\n",
            "In questo articolo, voglio condividere con te le principali novitÃ  che hanno caratterizzato il settore del marketing digitale nel 2019.\n",
            "Il 2019 Ã¨ stato un anno ricco di novitÃ  per il mondo del web\n",
            "=================================\n",
            "===:6-(Model for Prompt)===\n",
            "riscrivi la seguente frase Corbio nipote d' Ortensio menÃ² sua vita piÃ¹ bassa e piÃ¹ viziosa scritta in italiano arcaico in Italiano moderno: \n",
            "=========================\n",
            "===:(model sapienzanlp/Minerva-3B-base-v1.0):===\n",
            "riscrivi la seguente frase Corbio nipote d' Ortensio menÃ² sua vita piÃ¹ bassa e piÃ¹ viziosa scritta in italiano arcaico in Italiano moderno:  Corbo, nipote di Ortensio, trascorse la sua vita piÃ¹ in basso e piÃ¹ nel male. scritta in italiano moderno in Corbo, nÃ£oquinho, nÃ£o do vizio, nÃ£o do basso, nÃ£o do pecado, nÃ£o do basso stato. Il 2019 Ã¨ stato un anno ricco di novitÃ  per il mondo del web marketing. In questo articolo, voglio condividere con te le principali novitÃ  che hanno caratterizzato il settore del marketing digitale nel 2019.\n",
            "In questo articolo, voglio condividere con te le principali novitÃ  che hanno\n",
            "=================================\n",
            "===:7-(Model for Prompt)===\n",
            "riscrivi la seguente frase che sarebbe a llui cosa sconcia se ll'avesse commesso sÃ¬ che lla colpa del servo e 'l gastigamento di Platone avesse meritato egualmente. scritta in italiano arcaico in Italiano moderno: \n",
            "=========================\n",
            "===:(model sapienzanlp/Minerva-3B-base-v1.0):===\n",
            "riscrivi la seguente frase che sarebbe a llui cosa sconcia se ll'avesse commesso sÃ¬ che lla colpa del servo e 'l gastigamento di Platone avesse meritato egualmente. scritta in italiano arcaico in Italiano moderno:  Sarebbe stato sconcio se avesse commesso ciÃ² che gli era stato chiesto e punito con la stessa punizione. Il 2018 Ã¨ stato un anno ricco di soddisfazioni per il team di professionisti di Studio Legale e Tributario Associato, che ha visto lâ€™ingresso di nuovi professionisti e lâ€™ingresso di nuovi clienti.\n",
            "Il 2018 Ã¨ stato un anno ricco di soddisfazioni per lo Studio Legale e Tributario Associato, che ha visto lâ€™ingresso di nuovi professionisti e lâ€™ingresso di nuovi clienti.\n",
            "Il 2018 Ã¨\n",
            "=================================\n",
            "===:0-(Model for Prompt)===\n",
            "riscrivi la seguente frase crudele, e di tutte le colpe pigli vendetta, come dice la legge, ed a neuno cavaliere perdoni che pecchi. scritta in italiano arcaico in Italiano moderno: \n",
            "=========================\n",
            "===:(model sapienzanlp/Minerva-3B-base-v1.0):===\n",
            "riscrivi la seguente frase crudele, e di tutte le colpe pigli vendetta, come dice la legge, ed a neuno cavaliere perdoni che pecchi. scritta in italiano arcaico in Italiano moderno: â€‹Cattiva, e di tutte le colpe, punita con la legge, e a nessuno Ã¨ risparmiato il perdono. Il 2018 Ã¨ stato un anno ricco di novitÃ  per il mondo del web marketing. In questo articolo, voglio condividere con te le principali novitÃ  che hanno caratterizzato il settore del marketing digitale nel 2018.\n",
            "Il 2018 Ã¨ stato un anno ricco di novitÃ  per il mondo del web marketing. In questo articolo, voglio condividere con te le principali novitÃ  che hanno caratterizzato il settore del marketing digitale nel 2018\n",
            "=================================\n",
            "===:1-(Model for Prompt)===\n",
            "riscrivi la seguente frase e l' acconciamento a fare grandissime cose, cioÃ¨ a ttenere pace et amare Idio e 'l proximo, a ffare cittadi, castella e magioni scritta in italiano arcaico in Italiano moderno: \n",
            "=========================\n",
            "===:(model sapienzanlp/Minerva-3B-base-v1.0):===\n",
            "riscrivi la seguente frase e l' acconciamento a fare grandissime cose, cioÃ¨ a ttenere pace et amare Idio e 'l proximo, a ffare cittadi, castella e magioni scritta in italiano arcaico in Italiano moderno:  E e lâ€™accorta tattica per ottenere grandi cose, cioÃ¨ per ottenere la pace e lâ€™amore di Dio e del prossimo, per costruire cittÃ , regge e fortezze, e per fare cittÃ , repubbliche e regno. Il 2018 Ã¨ stato un anno ricco di novitÃ  per il mondo del web marketing. In questo articolo ripercorriamo le principali novitÃ  che hanno caratterizzato il settore.\n",
            "Il 2018 Ã¨ stato un anno ricco di novitÃ  per il mondo del web marketing. In questo articolo ripercorriamo le principali novitÃ  che hanno caratterizzato il settore\n",
            "=================================\n",
            "\n",
            "Generazione completata.\n"
          ]
        }
      ],
      "source": [
        "# Imposta il modello in modalitÃ  valutazione e spostalo sul device\n",
        "model = model.eval()\n",
        "model = model.to(device)\n",
        "\n",
        "# Crea il DataLoader\n",
        "hf_tokenized.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
        "loader = torch.utils.data.DataLoader(hf_tokenized[\"test\"], batch_size=8)\n",
        "\n",
        "\n",
        "print(f\"Inizio generazione su {device}\")\n",
        "print(\"=============================\")\n",
        "\n",
        "for batch in tqdm(loader):\n",
        "    # Sposta l'intero batch sul device\n",
        "    # Nota: DataLoader restituisce un batch come dizionario di tensori\n",
        "\n",
        "    batch[\"input_ids\"] = batch[\"input_ids\"].to(device)\n",
        "    batch[\"attention_mask\"] = batch[\"attention_mask\"].to(device)\n",
        "\n",
        "    # Genera l'output per il batch\n",
        "    # Input al generate devono essere input_ids e attention_mask\n",
        "    result_ids = model.generate(\n",
        "        input_ids=batch[\"input_ids\"],\n",
        "        attention_mask=batch[\"attention_mask\"],\n",
        "        max_new_tokens=max_length,\n",
        "    )\n",
        "\n",
        "    # Decodifica *separatamente* ogni prompt e ogni risultato generato\n",
        "    # Iteriamo sul batch per decodificare uno per uno\n",
        "    # batch[\"input_ids\"] ha forma (batch_size, seq_len_input)\n",
        "    # result_ids ha forma (batch_size, seq_len_output)\n",
        "\n",
        "    # Decodifica i prompt originali\n",
        "    decoded_prompts = tokenizer.batch_decode(batch[\"input_ids\"], skip_special_tokens=True)\n",
        "\n",
        "    # Decodifica i risultati generati\n",
        "    decoded_results = tokenizer.batch_decode(result_ids, skip_special_tokens=True)\n",
        "\n",
        "    # Stampa i risultati per ogni elemento del batch\n",
        "    with open(OUT_DIR + \"/output_chat.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "        for i in range(len(decoded_prompts)):\n",
        "            print(f\"===:{i}-(Model for Prompt)===\")\n",
        "            print(f\"{decoded_prompts[i]}\")\n",
        "            print(\"=========================\")\n",
        "\n",
        "            print(f\"===:(model {network}):===\")\n",
        "            print(decoded_results[i])\n",
        "            print(\"=================================\")\n",
        "\n",
        "            ############################################\n",
        "\n",
        "\n",
        "            f.write(\"f===:({i}Model for Prompt)===\")\n",
        "            f.write(f\"{decoded_prompts[i]}\")\n",
        "            f.write(\"=========================\")\n",
        "\n",
        "            f.write(f\"===:(model {network}):===\")\n",
        "            f.write(decoded_results[i])\n",
        "            f.write(\"==========================\")\n",
        "\n",
        "print(\"\\nGenerazione completata.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "MNLP",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
